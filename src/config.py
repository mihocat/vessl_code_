#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Configuration module for the RAG system
ì‹œìŠ¤í…œ ì„¤ì • ëª¨ë“ˆ
"""

import os
from dataclasses import dataclass
from typing import List, Optional


@dataclass
class LLMConfig:
    """LLM ì„¤ì •"""
    base_url: str = "http://localhost:8000"
    model_name: str = "test_model"
    # model_name: str = "yanolja/EEVE-Korean-Instruct-10.8B-v1.0"
    max_tokens: int = 800
    temperature: float = 0.1
    top_p: float = 0.85
    presence_penalty: float = 0.05
    frequency_penalty: float = 0.05
    repetition_penalty: float = 1.2  # ë°˜ë³µ ë°©ì§€ë¥¼ ìœ„í•œ penalty ì¶”ê°€
    timeout: int = 45
    health_check_timeout: int = 2


@dataclass
class RAGConfig:
    """RAG ì‹œìŠ¤í…œ ì„¤ì •"""
    embedding_model_name: str = "jinaai/jina-embeddings-v3"
    collection_name: str = "qa_collection_v3"
    chroma_db_path: str = "./chroma_db"
    max_documents: int = 10000
    batch_size: int = 500
    
    # ê²€ìƒ‰ ì„¤ì •
    search_k: int = 10
    rerank_k: int = 5
    
    # ì ìˆ˜ ì„ê³„ê°’ (í˜„ì‹¤ì ì¸ ê°’ìœ¼ë¡œ ì¡°ì •)
    high_confidence_threshold: float = 0.75  # ë†’ì€ ì‹ ë¢°ë„
    medium_confidence_threshold: float = 0.5   # ì¤‘ê°„ ì‹ ë¢°ë„
    low_confidence_threshold: float = 0.25    # ë‚®ì€ ì‹ ë¢°ë„
    
    # ì ìˆ˜ ì¡°ì • ê°€ì¤‘ì¹˜
    keyword_match_bonus: float = 0.1
    substring_match_bonus: float = 0.05
    category_bonus: float = 0.2
    
    # Intelligent RAG ì„¤ì •
    use_intelligent_rag: bool = False
    intelligent_rag_mode: str = "adaptive"  # adaptive, always, never
    intelligent_features: dict = None  # ëŸ°íƒ€ì„ì— ì´ˆê¸°í™”
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì²˜ë¦¬"""
        if self.intelligent_features is None:
            self.intelligent_features = {
                "intent_detection": True,
                "knowledge_graph": False,
                "adaptive_response": True,
                "complexity_threshold": 0.25  # í…ŒìŠ¤íŠ¸ ê²°ê³¼ì— ë”°ë¼ ì¡°ì •
            }


@dataclass
class DatasetConfig:
    """ë°ì´í„°ì…‹ ì„¤ì •"""
    paths: List[str] = None
    file_extensions: List[str] = None
    
    def __post_init__(self):
        if self.paths is None:
            self.paths = [
                "/dataset",
                "./dataset",
                "./2_documents",
                "/data",
                "./data"
            ]
        if self.file_extensions is None:
            self.file_extensions = ['*.txt', '*.json', '*.jsonl', '*.csv', '*.tsv']


@dataclass
class WebSearchConfig:
    """ì›¹ ê²€ìƒ‰ ì„¤ì •"""
    max_results: int = 3
    context_limit: int = 500
    db_context_char_limit: int = 150
    web_context_char_limit: int = 100


@dataclass
class OpenAIConfig:
    """OpenAI API ì„¤ì • - ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ë¶„ì„ ì „ìš©"""
    api_key: Optional[str] = None
    # GPT-5 í†µí•© ëª¨ë¸: ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ë¶„ì„ ì „ìš© (ìµœì¢… ë‹µë³€ ìƒì„± ê¸ˆì§€)
    unified_model: str = "gpt-5"  # ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ë¶„ì„ ì „ìš©
    max_tokens: int = 500  # ë” ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•´ ì¦ê°€
    temperature: float = 0.3  # ì•½ê°„ ë” ì°½ì˜ì ì¸ ë¶„ì„
    # ì‚¬ìš© ì œí•œ ì„¤ì •
    use_for_analysis_only: bool = True  # ë¶„ì„ ì „ìš©, ë‹µë³€ ìƒì„± ê¸ˆì§€
    use_for_final_response: bool = False  # ìµœì¢… ë‹µë³€ ìƒì„± ê¸ˆì§€
    max_calls_per_query: int = 1  # ì§ˆì˜ë‹¹ 1íšŒë§Œ í˜¸ì¶œ
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì²˜ë¦¬"""
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
        if not self.api_key:
            self.api_key = os.getenv("OPENAI_API_KEY")


@dataclass
class NextGenConfig:
    """ì°¨ì„¸ëŒ€ ì‹œìŠ¤í…œ ì„¤ì •"""
    # Multi-Agent ì„¤ì •
    enable_multi_agent: bool = True
    agent_timeout: int = 30
    max_parallel_agents: int = 4
    
    # Intent Analysis ì„¤ì •
    enable_intent_analysis: bool = True
    intent_confidence_threshold: float = 0.8
    
    # Memory System ì„¤ì •
    enable_memory_system: bool = True
    memory_retention_days: int = 30
    max_conversation_history: int = 100
    
    # Adaptive Response ì„¤ì •
    enable_adaptive_response: bool = True
    response_personalization: bool = True
    
    # Performance ì„¤ì •
    enable_caching: bool = True
    cache_ttl_seconds: int = 3600
    max_response_time: int = 15


@dataclass
class AppConfig:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •"""
    server_name: str = "0.0.0.0"
    server_port: int = 7860
    share: bool = False
    
    # UI ì„¤ì •
    title: str = "ğŸš€ Next-Generation AI Chatbot"
    description: str = """
    # ğŸš€ Next-Generation AI Chatbot
    
    **Advanced Multi-Agent System with Vision, RAG, and Reasoning**
    
    âœ¨ **Features:**
    - ğŸ¯ Intelligent Intent Analysis
    - ğŸ¤– Multi-Agent Collaboration  
    - ğŸ‘ï¸ Advanced Vision Processing
    - ğŸ§  Adaptive Memory System
    - ğŸ”„ Chain-of-Thought Reasoning
    """
    
    # ì´ë¯¸ì§€ ë¶„ì„ ì„¤ì •
    use_simple_analyzer: bool = False  # OpenAI Vision API ìš°ì„  ì‚¬ìš©
    force_ocr_engines: bool = False  # OCR ì—”ì§„ ì„ íƒì  ì‚¬ìš©
    
    # ì˜ˆì œ ì§ˆë¬¸
    example_questions: List[str] = None
    
    def __post_init__(self):
        if self.example_questions is None:
            self.example_questions = [
                "ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "ë°ì´í„° ì²˜ë¦¬ ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì´ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "ìµœì‹  AI ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
            ]


class Config:
    """ì „ì²´ ì„¤ì • ê´€ë¦¬"""
    def __init__(self):
        # API í‚¤ ì„¤ì • (Storage ìš°ì„ )
        self._setup_api_keys()
        
        self.llm = LLMConfig()
        self.rag = RAGConfig()
        self.dataset = DatasetConfig()
        self.web_search = WebSearchConfig()
        self.openai = OpenAIConfig()
        self.app = AppConfig()
        self.next_gen = NextGenConfig()
        
        # í™˜ê²½ ë³€ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ
        self._load_from_env()
    
    def _setup_api_keys(self):
        """API í‚¤ ì´ˆê¸° ì„¤ì •"""
        try:
            from api_key_loader import setup_api_keys
            setup_api_keys()
        except Exception as e:
            print(f"Warning: Failed to load API keys from storage: {e}")
    
    def _load_from_env(self):
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ"""
        # LLM ì„¤ì •
        if os.getenv("VLLM_API_URL"):
            self.llm.base_url = os.getenv("VLLM_API_URL")
        elif os.getenv("LLM_BASE_URL"):
            self.llm.base_url = os.getenv("LLM_BASE_URL")
        if os.getenv("LLM_MODEL_NAME"):
            self.llm.model_name = os.getenv("LLM_MODEL_NAME")
        
        # RAG ì„¤ì •
        if os.getenv("EMBEDDING_MODEL"):
            self.rag.embedding_model_name = os.getenv("EMBEDDING_MODEL")
        if os.getenv("CHROMA_DB_PATH"):
            self.rag.chroma_db_path = os.getenv("CHROMA_DB_PATH")
        
        # Intelligent RAG ì„¤ì •
        if os.getenv("USE_INTELLIGENT_RAG"):
            self.rag.use_intelligent_rag = os.getenv("USE_INTELLIGENT_RAG").lower() == "true"
        if os.getenv("INTELLIGENT_RAG_MODE"):
            self.rag.intelligent_rag_mode = os.getenv("INTELLIGENT_RAG_MODE")
        
        # ë°ì´í„°ì…‹ ê²½ë¡œ
        if os.getenv("DATASET_PATH"):
            self.dataset.paths = [os.getenv("DATASET_PATH")]
        
        # ì„œë²„ ì„¤ì •
        if os.getenv("SERVER_PORT"):
            self.app.server_port = int(os.getenv("SERVER_PORT"))
        
        # OpenAI ì„¤ì •
        if os.getenv("OPENAI_API_KEY"):
            self.openai.api_key = os.getenv("OPENAI_API_KEY")
        if os.getenv("OPENAI_UNIFIED_MODEL"):
            self.openai.unified_model = os.getenv("OPENAI_UNIFIED_MODEL")
        
        # ê°•ì œ ì„¤ì •: OpenAIëŠ” ë¶„ì„ ì „ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©
        self.openai.use_for_analysis_only = True
        self.openai.use_for_final_response = False


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
config = Config()
