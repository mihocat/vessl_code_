#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© íŒŒì´í”„ë¼ì¸ Gradio ì•± v3
OpenAI ë¶„ì„(1íšŒ) â†’ RAG â†’ íŒŒì¸íŠœë‹ LLM íŒŒì´í”„ë¼ì¸
"""

import sys
import os
import time
import logging
from datetime import datetime
from typing import List, Tuple, Optional, Dict, Any
from PIL import Image
import gradio as gr

from config import Config
from integrated_pipeline import IntegratedPipeline

# ìƒì„¸í•œ ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ì•± ì‹œì‘ ì‹œê°„ ê¸°ë¡
APP_START_TIME = datetime.now()


class IntegratedChatService:
    """í†µí•© íŒŒì´í”„ë¼ì¸ ì±„íŒ… ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: Config):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        init_start_time = time.time()
        logger.info("ğŸš€ [INIT] IntegratedChatService ì´ˆê¸°í™” ì‹œì‘")
        
        logger.info("ğŸ”§ [INIT-1] ì„¤ì • ê°ì²´ ì €ì¥ ì¤‘...")
        self.config = config
        
        logger.info("ğŸ”§ [INIT-2] í†µí•© íŒŒì´í”„ë¼ì¸ ìƒì„± ì¤‘...")
        pipeline_start = time.time()
        self.pipeline = IntegratedPipeline(config)
        pipeline_time = time.time() - pipeline_start
        logger.info(f"âœ… [INIT-2] í†µí•© íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ ({pipeline_time:.2f}ì´ˆ)")
        
        logger.info("ğŸ”§ [INIT-3] ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™” ì¤‘...")
        self.conversation_history = []
        
        init_total_time = time.time() - init_start_time
        logger.info(f"âœ… [INIT] IntegratedChatService ì´ˆê¸°í™” ì™„ë£Œ (ì´ {init_total_time:.2f}ì´ˆ)")
    
    def process_query(
        self, 
        question: str, 
        history: List[Tuple[str, str]],
        image: Optional[Image.Image] = None
    ) -> Tuple[str, List[Tuple[str, str]]]:
        """
        ì§ˆì˜ ì²˜ë¦¬ - ì „ì²´ í”Œë¡œìš° ë¡œê¹… í¬í•¨
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            history: ëŒ€í™” ì´ë ¥
            image: ì„ íƒì  ì´ë¯¸ì§€
            
        Returns:
            (ì‘ë‹µ ë©”ì‹œì§€, ì—…ë°ì´íŠ¸ëœ ëŒ€í™” ì´ë ¥)
        """
        query_start_time = time.time()
        query_id = int(time.time() * 1000) % 100000  # 5ìë¦¬ ì¿¼ë¦¬ ID
        
        logger.info(f"\nğŸ¯ ====== [QUERY-{query_id}] ì‚¬ìš©ì ì§ˆì˜ ì²˜ë¦¬ ì‹œì‘ ======")
        logger.info(f"ğŸ“ [QUERY-{query_id}] ì§ˆë¬¸: {question[:100]}{'...' if len(question) > 100 else ''}")
        logger.info(f"ğŸ–¼ï¸ [QUERY-{query_id}] ì´ë¯¸ì§€: {'ìˆìŒ' if image else 'ì—†ìŒ'}")
        logger.info(f"ğŸ“š [QUERY-{query_id}] ëŒ€í™” ì´ë ¥: {len(history)}ê°œ")
        
        if not question.strip():
            logger.warning(f"âš ï¸ [QUERY-{query_id}] ë¹ˆ ì§ˆë¬¸ ì…ë ¥ë¨")
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", history
        
        logger.info(f"ğŸ”„ [QUERY-{query_id}] í†µí•© íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹œì‘")
        pipeline_start = time.time()
        
        # SKIP_VLLM í™˜ê²½ë³€ìˆ˜ í™•ì¸
        skip_vllm = os.getenv("SKIP_VLLM", "false").lower() == "true"
        use_llm_flag = not skip_vllm
        
        # íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ - ê²½ë¡œë³„ ìƒì„¸ ë¡œê¹…
        logger.info(f"ğŸ“‹ [QUERY-{query_id}] íŒŒì´í”„ë¼ì¸ ì…ë ¥ íŒŒë¼ë¯¸í„°:")
        logger.info(f"   - use_rag: True")
        logger.info(f"   - use_llm: {use_llm_flag}")
        logger.info(f"   - SKIP_VLLM: {skip_vllm}")
        logger.info(f"   - ì§ˆë¬¸ ê¸¸ì´: {len(question)}ì")
        logger.info(f"   - ì´ë¯¸ì§€: {'í¬í•¨' if image else 'ì—†ìŒ'}")
        
        # íŒŒì´í”„ë¼ì¸ ê²½ë¡œ í™•ì¸ ë° ë¡œê¹…
        if image is not None:
            logger.info(f"ğŸ–¼ï¸ [QUERY-{query_id}] ì„ íƒëœ íŒŒì´í”„ë¼ì¸: OpenAI â†’ RAG â†’ LLM (ì´ë¯¸ì§€ í¬í•¨)")
        else:
            logger.info(f"ğŸ“ [QUERY-{query_id}] ì„ íƒëœ íŒŒì´í”„ë¼ì¸: RAG â†’ LLM (í…ìŠ¤íŠ¸ ì „ìš©)")
        
        result = self.pipeline.process_query(
            question=question,
            image=image,
            use_rag=True,
            use_llm=use_llm_flag
        )
        
        # íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
        logger.info(f"ğŸ“Š [QUERY-{query_id}] íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ë¶„ì„:")
        if result.success:
            logger.info(f"   âœ… ì²˜ë¦¬ ì„±ê³µ")
            logger.info(f"   ğŸ“ ìµœì¢… ë‹µë³€ ê¸¸ì´: {len(result.final_answer)}ì")
            
            # OpenAI ë¶„ì„ ê²°ê³¼ (ì´ë¯¸ì§€ í¬í•¨ ì§ˆì˜ì¸ ê²½ìš°ì—ë§Œ)
            if result.analysis_result:
                analysis = result.analysis_result
                logger.info(f"   ğŸ” OpenAI ë¶„ì„: ì„±ê³µ")
                if analysis.get('token_usage'):
                    tokens = analysis['token_usage']
                    logger.info(f"     - í† í°: {tokens.get('total_tokens', 0)}ê°œ")
                if analysis.get('cost'):
                    logger.info(f"     - ë¹„ìš©: ${analysis['cost']:.4f}")
            else:
                # í…ìŠ¤íŠ¸ ì „ìš© ì§ˆì˜ì¸ ê²½ìš°
                logger.info(f"   ğŸ“ OpenAI ë¶„ì„: ê±´ë„ˆëœ€ (í…ìŠ¤íŠ¸ ì „ìš© ì§ˆì˜)")
            
            # RAG ê²€ìƒ‰ ê²°ê³¼
            if result.rag_results:
                logger.info(f"   ğŸ“š RAG ê²€ìƒ‰: {len(result.rag_results)}ê°œ ë¬¸ì„œ")
            
            # ì²˜ë¦¬ ì‹œê°„ ë¶„ì„
            if result.processing_times:
                times = result.processing_times
                logger.info(f"   â±ï¸ ë‹¨ê³„ë³„ ì‹œê°„:")
                for step, duration in times.items():
                    if step != 'total':
                        logger.info(f"     - {step}: {duration:.2f}ì´ˆ")
        else:
            logger.error(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {result.error_message}")
        
        pipeline_time = time.time() - pipeline_start
        logger.info(f"{'âœ…' if result.success else 'âŒ'} [QUERY-{query_id}] íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ ({pipeline_time:.2f}ì´ˆ)")
        
        if result.success:
            logger.info(f"âœ… [QUERY-{query_id}] íŒŒì´í”„ë¼ì¸ ì„±ê³µ - ì‘ë‹µ í¬ë§·íŒ… ì‹œì‘")
            format_start = time.time()
            response = self._format_response(result, question, image)
            format_time = time.time() - format_start
            logger.info(f"ğŸ“ [QUERY-{query_id}] ì‘ë‹µ í¬ë§·íŒ… ì™„ë£Œ ({format_time:.3f}ì´ˆ)")
            logger.info(f"ğŸ“„ [QUERY-{query_id}] ìµœì¢… ì‘ë‹µ ìš”ì•½:")
            logger.info(f"   - ì‘ë‹µ ì´ ê¸¸ì´: {len(response)}ì")
            logger.info(f"   - ë©”ì¸ ë‹µë³€: {result.final_answer[:100]}{'...' if len(result.final_answer) > 100 else ''}")
        else:
            logger.error(f"âŒ [QUERY-{query_id}] íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {result.error_message}")
            response = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {result.error_message}"
        
        # ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸
        logger.info(f"ğŸ“š [QUERY-{query_id}] ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸ ì¤‘...")
        history.append((question, response))
        self.conversation_history.append({
            'query_id': query_id,
            'question': question,
            'response': response,
            'has_image': image is not None,
            'timestamp': time.time(),
            'pipeline_result': result,
            'processing_time': time.time() - query_start_time
        })
        
        total_time = time.time() - query_start_time
        logger.info(f"ğŸ [QUERY-{query_id}] ì „ì²´ ì§ˆì˜ ì²˜ë¦¬ ì™„ë£Œ (ì´ {total_time:.2f}ì´ˆ)")
        logger.info(f"ğŸ“ˆ [QUERY-{query_id}] ì„±ëŠ¥ ìš”ì•½:")
        logger.info(f"   - íŒŒì´í”„ë¼ì¸: {pipeline_time:.2f}ì´ˆ ({pipeline_time/total_time*100:.1f}%)")
        if result.success and result.processing_times:
            times = result.processing_times
            # OpenAI ë¶„ì„ ì‹œê°„ (ì´ë¯¸ì§€ í¬í•¨ ì§ˆì˜ì¸ ê²½ìš°ì—ë§Œ)
            if 'openai_analysis' in times:
                openai_time = times['openai_analysis']
                logger.info(f"   - OpenAI ë¶„ì„: {openai_time:.2f}ì´ˆ ({openai_time/total_time*100:.1f}%)")
            # RAG ê²€ìƒ‰ ì‹œê°„
            if 'rag_search' in times:
                rag_time = times['rag_search']
                logger.info(f"   - RAG ê²€ìƒ‰: {rag_time:.2f}ì´ˆ ({rag_time/total_time*100:.1f}%)")
            # LLM ìƒì„± ì‹œê°„
            if 'llm_generation' in times:
                llm_time = times['llm_generation']
                logger.info(f"   - LLM ìƒì„±: {llm_time:.2f}ì´ˆ ({llm_time/total_time*100:.1f}%)")
        logger.info(f"ğŸ¯ ====== [QUERY-{query_id}] ì²˜ë¦¬ ì¢…ë£Œ ======\n")
        
        return response, history
    
    def _format_response(self, result, question: str, image: Optional[Image.Image]) -> str:
        """ì‘ë‹µ í¬ë§·íŒ…"""
        response_parts = []
        
        # ë©”ì¸ ë‹µë³€
        response_parts.append(result.final_answer)
        
        # ì²˜ë¦¬ ì •ë³´ ì¶”ê°€
        if result.analysis_result or result.processing_times:
            response_parts.append("\n" + "="*50)
            response_parts.append("ğŸ“Š **ì²˜ë¦¬ ì •ë³´**")
            
            # OpenAI ë¶„ì„ ê²°ê³¼
            if result.analysis_result:
                analysis = result.analysis_result
                if analysis.get('formulas'):
                    response_parts.append(f"ğŸ“ ê°ì§€ëœ ìˆ˜ì‹: {len(analysis['formulas'])}ê°œ")
                if analysis.get('key_concepts'):
                    response_parts.append(f"ğŸ”‘ í•µì‹¬ ê°œë…: {', '.join(analysis['key_concepts'][:3])}")
                if analysis.get('token_usage'):
                    tokens = analysis['token_usage']
                    response_parts.append(f"ğŸª™ í† í° ì‚¬ìš©: {tokens['total_tokens']}ê°œ (ì…ë ¥: {tokens['prompt_tokens']}, ì¶œë ¥: {tokens['completion_tokens']})")
                if analysis.get('cost'):
                    response_parts.append(f"ğŸ’° OpenAI ë¹„ìš©: ${analysis['cost']:.4f}")
            
            # RAG ê²€ìƒ‰ ê²°ê³¼
            if result.rag_results:
                response_parts.append(f"ğŸ“š RAG ê²€ìƒ‰: {len(result.rag_results)}ê°œ ë¬¸ì„œ í™œìš©")
            
            # ì²˜ë¦¬ ì‹œê°„
            if result.processing_times:
                times = result.processing_times
                response_parts.append(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {times.get('total', 0):.2f}ì´ˆ")
                if 'openai_analysis' in times:
                    response_parts.append(f"  - OpenAI ë¶„ì„: {times['openai_analysis']:.2f}ì´ˆ")
                if 'rag_search' in times:
                    response_parts.append(f"  - RAG ê²€ìƒ‰: {times['rag_search']:.2f}ì´ˆ")
                if 'llm_generation' in times:
                    response_parts.append(f"  - LLM ìƒì„±: {times['llm_generation']:.2f}ì´ˆ")
            
            # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„
            if result.pipeline_steps:
                response_parts.append(f"ğŸ”„ íŒŒì´í”„ë¼ì¸: {' â†’ '.join(result.pipeline_steps)}")
        
        return "\n".join(response_parts)
    
    def get_system_status(self) -> str:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        try:
            # íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸
            health_status = self.pipeline.health_check()
            stats = self.pipeline.get_statistics()
            
            status_parts = [
                "ğŸ¥ **ì‹œìŠ¤í…œ ìƒíƒœ**",
                f"OpenAI í”„ë¡œì„¸ì„œ: {'âœ…' if health_status['openai_processor'] else 'âŒ'}",
                f"RAG ì‹œìŠ¤í…œ: {'âœ…' if health_status['rag_system'] else 'âŒ'}",
                f"íŒŒì¸íŠœë‹ LLM: {'âœ…' if health_status['llm_client'] else 'âŒ'}",
                "",
                "ğŸ“ˆ **ì²˜ë¦¬ í†µê³„**",
                f"ì´ ì§ˆì˜ ìˆ˜: {stats['total_queries']}",
                f"ì„±ê³µë¥ : {stats['success_rate']:.1%}",
                f"í‰ê·  ë¹„ìš©: ${stats['average_cost_per_query']:.4f}",
                f"OpenAI í˜¸ì¶œ íš¨ìœ¨: {stats['openai_call_efficiency']}",
                f"ì´ ë¹„ìš©: ${stats['total_cost']:.4f}"
            ]
            
            return "\n".join(status_parts)
            
        except Exception as e:
            return f"ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


def create_gradio_interface(service: IntegratedChatService) -> gr.Interface:
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    def chat_function(question: str, history: List[Tuple[str, str]], image: Optional[Image.Image]):
        """ì±„íŒ… í•¨ìˆ˜"""
        return service.process_query(question, history, image)
    
    def status_function():
        """ìƒíƒœ í™•ì¸ í•¨ìˆ˜"""
        return service.get_system_status()
    
    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(
        title="ğŸš€ í†µí•© AI íŒŒì´í”„ë¼ì¸",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1200px !important;
            margin: auto;
        }
        .chat-container {
            height: 600px;
        }
        """
    ) as iface:
        gr.Markdown("""
        # ğŸš€ í†µí•© AI íŒŒì´í”„ë¼ì¸ ì±„íŒ…ë´‡
        
        **ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜:**
        1. ğŸ” **OpenAI GPT-4.1** - ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ë¶„ì„ (1íšŒ í˜¸ì¶œ)
        2. ğŸ“š **RAG ê²€ìƒ‰** - ChromaDB ì „ë¬¸ ë¬¸ì„œ í™œìš©
        3. ğŸ¤– **íŒŒì¸íŠœë‹ LLM** - KoLlama í•œêµ­ì–´ ì „ë¬¸ ëª¨ë¸
        
        **íŠ¹ì§•:**
        - OpenAI APIëŠ” ë¶„ì„ ì „ìš© (ìµœì¢… ë‹µë³€ ê¸ˆì§€)
        - ì§ˆì˜ë‹¹ 1íšŒë§Œ OpenAI í˜¸ì¶œ
        - ìµœì¢… ë‹µë³€ì€ íŒŒì¸íŠœë‹ëœ í•œêµ­ì–´ LLMë§Œ ë‹´ë‹¹
        """)
        
        with gr.Row():
            with gr.Column(scale=3):
                # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
                chatbot = gr.Chatbot(
                    label="ëŒ€í™”",
                    height=600,
                    show_label=True,
                    container=True,
                    bubble_full_width=False
                )
                
                with gr.Row():
                    question_input = gr.Textbox(
                        label="ì§ˆë¬¸",
                        placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
                        lines=2,
                        scale=4
                    )
                    image_input = gr.Image(
                        label="ì´ë¯¸ì§€ (ì„ íƒì‚¬í•­)",
                        type="pil",
                        scale=1
                    )
                
                with gr.Row():
                    submit_btn = gr.Button("ì „ì†¡", variant="primary", scale=1)
                    clear_btn = gr.Button("ëŒ€í™” ì´ˆê¸°í™”", scale=1)
            
            with gr.Column(scale=1):
                # ì‹œìŠ¤í…œ ìƒíƒœ íŒ¨ë„
                status_output = gr.Textbox(
                    label="ì‹œìŠ¤í…œ ìƒíƒœ",
                    lines=20,
                    interactive=False
                )
                status_btn = gr.Button("ìƒíƒœ ìƒˆë¡œê³ ì¹¨")
                
                # ì˜ˆì œ ì§ˆë¬¸ë“¤
                gr.Markdown("### ğŸ’¡ ì˜ˆì œ ì§ˆë¬¸")
                example_questions = [
                    "Pr'ì„ êµ¬í•  ë•Œ ì™œ Prì—ì„œ Qcë¥¼ ë¹¼ëŠ” ê±´ê°€ìš”?",
                    "ë¯¸ë¶„í•  ë•Œ d/daë¥¼ ì ìš©í•˜ë©´ ì™œ sëŠ” 1ì´ê³  aëŠ” 0ì´ ë˜ëŠ” ê±´ê°€ìš”?",
                    "ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ ë‹¨ìœ„ë²¡í„°ë¥¼ êµ¬í•˜ëŠ” ë°©ì‹ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                    "ë¼í”Œë¼ìŠ¤ ë³€í™˜ì˜ ì •ì˜ì™€ ì„±ì§ˆì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
                ]
                
                for i, example in enumerate(example_questions, 1):
                    gr.Button(f"{i}. {example[:30]}...", size="sm").click(
                        lambda x=example: (x, []),
                        outputs=[question_input, chatbot]
                    )
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        submit_btn.click(
            chat_function,
            inputs=[question_input, chatbot, image_input],
            outputs=[question_input, chatbot]
        ).then(
            lambda: ("", None),
            outputs=[question_input, image_input]
        )
        
        question_input.submit(
            chat_function,
            inputs=[question_input, chatbot, image_input],
            outputs=[question_input, chatbot]
        ).then(
            lambda: ("", None),
            outputs=[question_input, image_input]
        )
        
        clear_btn.click(
            lambda: ([], "", None),
            outputs=[chatbot, question_input, image_input]
        )
        
        status_btn.click(
            status_function,
            outputs=status_output
        )
        
        # ì´ˆê¸° ìƒíƒœ ë¡œë“œ
        iface.load(
            status_function,
            outputs=status_output
        )
    
    return iface


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ì•± ë¡œë”© ë‹¨ê³„ë³„ ìƒì„¸ ë¡œê¹…"""
    app_start_time = time.time()
    logger.info("\nğŸ‰ ========================================")
    logger.info("ğŸ‰    í†µí•© AI íŒŒì´í”„ë¼ì¸ ì•± ì‹œì‘")
    logger.info("ğŸ‰ ========================================")
    
    try:
        # Step 1: ì„¤ì • ë¡œë“œ
        logger.info("ğŸ”§ [STEP-1] ì‹œìŠ¤í…œ ì„¤ì • ë¡œë“œ ì‹œì‘...")
        config_start = time.time()
        config = Config()
        config_time = time.time() - config_start
        logger.info(f"âœ… [STEP-1] ì‹œìŠ¤í…œ ì„¤ì • ë¡œë“œ ì™„ë£Œ ({config_time:.2f}ì´ˆ)")
        logger.info(f"ğŸ“‹ [STEP-1] ì„¤ì • ìš”ì•½:")
        logger.info(f"   - OpenAI ëª¨ë¸: {getattr(config.openai, 'unified_model', 'N/A')}")
        logger.info(f"   - ì„œë²„ ì£¼ì†Œ: {config.app.server_name}:{config.app.server_port}")
        logger.info(f"   - RAG í™œì„±í™”: {hasattr(config, 'rag')}")
        
        # Step 2: í†µí•© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        logger.info("ğŸ”§ [STEP-2] IntegratedChatService ì´ˆê¸°í™” ì‹œì‘...")
        service_start = time.time()
        service = IntegratedChatService(config)
        service_time = time.time() - service_start
        logger.info(f"âœ… [STEP-2] IntegratedChatService ì´ˆê¸°í™” ì™„ë£Œ ({service_time:.2f}ì´ˆ)")
        
        # Step 3: Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        logger.info("ğŸ”§ [STEP-3] Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì‹œì‘...")
        iface_start = time.time()
        iface = create_gradio_interface(service)
        iface_time = time.time() - iface_start
        logger.info(f"âœ… [STEP-3] Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ ({iface_time:.2f}ì´ˆ)")
        
        # Step 4: ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        logger.info("ğŸ”§ [STEP-4] ì‹œìŠ¤í…œ ìƒíƒœ ìµœì¢… í™•ì¸...")
        try:
            status_check = service.get_system_status()
            logger.info("âœ… [STEP-4] ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì™„ë£Œ")
        except Exception as status_e:
            logger.warning(f"âš ï¸ [STEP-4] ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {status_e}")
        
        # ì „ì²´ ì´ˆê¸°í™” ì‹œê°„ ê³„ì‚°
        total_init_time = time.time() - app_start_time
        logger.info(f"\nğŸŠ ì „ì²´ ì•± ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ˆê¸°í™” ì‹œê°„ ë¶„ì„:")
        logger.info(f"   - ì„¤ì • ë¡œë“œ: {config_time:.2f}ì´ˆ")
        logger.info(f"   - ì„œë¹„ìŠ¤ ì´ˆê¸°í™”: {service_time:.2f}ì´ˆ")
        logger.info(f"   - ì¸í„°í˜ì´ìŠ¤ ìƒì„±: {iface_time:.2f}ì´ˆ")
        logger.info(f"   - ì „ì²´ ì†Œìš”ì‹œê°„: {total_init_time:.2f}ì´ˆ")
        
        # Step 5: ì„œë²„ ì‹œì‘
        logger.info(f"\nğŸš€ [LAUNCH] ì›¹ ì„œë²„ ì‹œì‘ ì¤‘...")
        logger.info(f"ğŸŒ [LAUNCH] ì„œë²„ ì£¼ì†Œ: http://{config.app.server_name}:{config.app.server_port}")
        logger.info(f"ğŸ”— [LAUNCH] ê³µìœ  ë§í¬: {'í™œì„±í™”' if config.app.share else 'ë¹„í™œì„±í™”'}")
        logger.info(f"ğŸ¯ [LAUNCH] ì„œë²„ ì‹œì‘ í›„ ì§ˆì˜ ì²˜ë¦¬ ë¡œê¹…ì´ ì‹œì‘ë©ë‹ˆë‹¤...")
        
        # Gradio ì„œë²„ ëŸ°ì¹˜
        iface.launch(
            server_name=config.app.server_name,
            server_port=config.app.server_port,
            share=config.app.share,
            show_error=True
        )
        
    except Exception as e:
        logger.error(f"âŒ [ERROR] ì•± ì‹œì‘ ì‹¤íŒ¨: {e}")
        logger.error(f"âŒ [ERROR] ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        logger.error(f"âŒ [ERROR] ì˜¤ë¥˜ ìƒì„¸: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()