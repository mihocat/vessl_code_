#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© íŒŒì´í”„ë¼ì¸ Gradio ì•± v3
OpenAI ë¶„ì„(1íšŒ) â†’ RAG â†’ íŒŒì¸íŠœë‹ LLM íŒŒì´í”„ë¼ì¸
"""

import sys
import time
import logging
from typing import List, Tuple, Optional, Dict, Any
from PIL import Image
import gradio as gr

from config import Config
from integrated_pipeline import IntegratedPipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class IntegratedChatService:
    """í†µí•© íŒŒì´í”„ë¼ì¸ ì±„íŒ… ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: Config):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.config = config
        self.pipeline = IntegratedPipeline(config)
        
        # ëŒ€í™” ì´ë ¥
        self.conversation_history = []
        
        logger.info("IntegratedChatService initialized")
    
    def process_query(
        self, 
        question: str, 
        history: List[Tuple[str, str]],
        image: Optional[Image.Image] = None
    ) -> Tuple[str, List[Tuple[str, str]]]:
        """
        ì§ˆì˜ ì²˜ë¦¬
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            history: ëŒ€í™” ì´ë ¥
            image: ì„ íƒì  ì´ë¯¸ì§€
            
        Returns:
            (ì‘ë‹µ ë©”ì‹œì§€, ì—…ë°ì´íŠ¸ëœ ëŒ€í™” ì´ë ¥)
        """
        if not question.strip():
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", history
        
        # íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬
        result = self.pipeline.process_query(
            question=question,
            image=image,
            use_rag=True,
            use_llm=True
        )
        
        if result.success:
            # ì„±ê³µì ì¸ ì²˜ë¦¬
            response = self._format_response(result, question, image)
        else:
            # ì‹¤íŒ¨ ì²˜ë¦¬
            response = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {result.error_message}"
        
        # ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸
        history.append((question, response))
        self.conversation_history.append({
            'question': question,
            'response': response,
            'has_image': image is not None,
            'timestamp': time.time(),
            'pipeline_result': result
        })
        
        return response, history
    
    def _format_response(self, result, question: str, image: Optional[Image.Image]) -> str:
        """ì‘ë‹µ í¬ë§·íŒ…"""
        response_parts = []
        
        # ë©”ì¸ ë‹µë³€
        response_parts.append(result.final_answer)
        
        # ì²˜ë¦¬ ì •ë³´ ì¶”ê°€
        if result.analysis_result or result.processing_times:
            response_parts.append("\n" + "="*50)
            response_parts.append("ğŸ“Š **ì²˜ë¦¬ ì •ë³´**")
            
            # OpenAI ë¶„ì„ ê²°ê³¼
            if result.analysis_result:
                analysis = result.analysis_result
                if analysis.get('formulas'):
                    response_parts.append(f"ğŸ“ ê°ì§€ëœ ìˆ˜ì‹: {len(analysis['formulas'])}ê°œ")
                if analysis.get('key_concepts'):
                    response_parts.append(f"ğŸ”‘ í•µì‹¬ ê°œë…: {', '.join(analysis['key_concepts'][:3])}")
                if analysis.get('token_usage'):
                    tokens = analysis['token_usage']
                    response_parts.append(f"ğŸª™ í† í° ì‚¬ìš©: {tokens['total_tokens']}ê°œ (ì…ë ¥: {tokens['prompt_tokens']}, ì¶œë ¥: {tokens['completion_tokens']})")
                if analysis.get('cost'):
                    response_parts.append(f"ğŸ’° OpenAI ë¹„ìš©: ${analysis['cost']:.4f}")
            
            # RAG ê²€ìƒ‰ ê²°ê³¼
            if result.rag_results:
                response_parts.append(f"ğŸ“š RAG ê²€ìƒ‰: {len(result.rag_results)}ê°œ ë¬¸ì„œ í™œìš©")
            
            # ì²˜ë¦¬ ì‹œê°„
            if result.processing_times:
                times = result.processing_times
                response_parts.append(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {times.get('total', 0):.2f}ì´ˆ")
                if 'openai_analysis' in times:
                    response_parts.append(f"  - OpenAI ë¶„ì„: {times['openai_analysis']:.2f}ì´ˆ")
                if 'rag_search' in times:
                    response_parts.append(f"  - RAG ê²€ìƒ‰: {times['rag_search']:.2f}ì´ˆ")
                if 'llm_generation' in times:
                    response_parts.append(f"  - LLM ìƒì„±: {times['llm_generation']:.2f}ì´ˆ")
            
            # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„
            if result.pipeline_steps:
                response_parts.append(f"ğŸ”„ íŒŒì´í”„ë¼ì¸: {' â†’ '.join(result.pipeline_steps)}")
        
        return "\n".join(response_parts)
    
    def get_system_status(self) -> str:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        try:
            # íŒŒì´í”„ë¼ì¸ ìƒíƒœ í™•ì¸
            health_status = self.pipeline.health_check()
            stats = self.pipeline.get_statistics()
            
            status_parts = [
                "ğŸ¥ **ì‹œìŠ¤í…œ ìƒíƒœ**",
                f"OpenAI í”„ë¡œì„¸ì„œ: {'âœ…' if health_status['openai_processor'] else 'âŒ'}",
                f"RAG ì‹œìŠ¤í…œ: {'âœ…' if health_status['rag_system'] else 'âŒ'}",
                f"íŒŒì¸íŠœë‹ LLM: {'âœ…' if health_status['llm_client'] else 'âŒ'}",
                "",
                "ğŸ“ˆ **ì²˜ë¦¬ í†µê³„**",
                f"ì´ ì§ˆì˜ ìˆ˜: {stats['total_queries']}",
                f"ì„±ê³µë¥ : {stats['success_rate']:.1%}",
                f"í‰ê·  ë¹„ìš©: ${stats['average_cost_per_query']:.4f}",
                f"OpenAI í˜¸ì¶œ íš¨ìœ¨: {stats['openai_call_efficiency']}",
                f"ì´ ë¹„ìš©: ${stats['total_cost']:.4f}"
            ]
            
            return "\n".join(status_parts)
            
        except Exception as e:
            return f"ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


def create_gradio_interface(service: IntegratedChatService) -> gr.Interface:
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    def chat_function(question: str, history: List[Tuple[str, str]], image: Optional[Image.Image]):
        """ì±„íŒ… í•¨ìˆ˜"""
        return service.process_query(question, history, image)
    
    def status_function():
        """ìƒíƒœ í™•ì¸ í•¨ìˆ˜"""
        return service.get_system_status()
    
    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(
        title="ğŸš€ í†µí•© AI íŒŒì´í”„ë¼ì¸",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1200px !important;
            margin: auto;
        }
        .chat-container {
            height: 600px;
        }
        """
    ) as iface:
        gr.Markdown("""
        # ğŸš€ í†µí•© AI íŒŒì´í”„ë¼ì¸ ì±„íŒ…ë´‡
        
        **ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜:**
        1. ğŸ” **OpenAI GPT-4.1** - ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ë¶„ì„ (1íšŒ í˜¸ì¶œ)
        2. ğŸ“š **RAG ê²€ìƒ‰** - ChromaDB ì „ë¬¸ ë¬¸ì„œ í™œìš©
        3. ğŸ¤– **íŒŒì¸íŠœë‹ LLM** - KoLlama í•œêµ­ì–´ ì „ë¬¸ ëª¨ë¸
        
        **íŠ¹ì§•:**
        - OpenAI APIëŠ” ë¶„ì„ ì „ìš© (ìµœì¢… ë‹µë³€ ê¸ˆì§€)
        - ì§ˆì˜ë‹¹ 1íšŒë§Œ OpenAI í˜¸ì¶œ
        - ìµœì¢… ë‹µë³€ì€ íŒŒì¸íŠœë‹ëœ í•œêµ­ì–´ LLMë§Œ ë‹´ë‹¹
        """)
        
        with gr.Row():
            with gr.Column(scale=3):
                # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
                chatbot = gr.Chatbot(
                    label="ëŒ€í™”",
                    height=600,
                    show_label=True,
                    container=True,
                    bubble_full_width=False
                )
                
                with gr.Row():
                    question_input = gr.Textbox(
                        label="ì§ˆë¬¸",
                        placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
                        lines=2,
                        scale=4
                    )
                    image_input = gr.Image(
                        label="ì´ë¯¸ì§€ (ì„ íƒì‚¬í•­)",
                        type="pil",
                        scale=1
                    )
                
                with gr.Row():
                    submit_btn = gr.Button("ì „ì†¡", variant="primary", scale=1)
                    clear_btn = gr.Button("ëŒ€í™” ì´ˆê¸°í™”", scale=1)
            
            with gr.Column(scale=1):
                # ì‹œìŠ¤í…œ ìƒíƒœ íŒ¨ë„
                status_output = gr.Textbox(
                    label="ì‹œìŠ¤í…œ ìƒíƒœ",
                    lines=20,
                    interactive=False
                )
                status_btn = gr.Button("ìƒíƒœ ìƒˆë¡œê³ ì¹¨")
                
                # ì˜ˆì œ ì§ˆë¬¸ë“¤
                gr.Markdown("### ğŸ’¡ ì˜ˆì œ ì§ˆë¬¸")
                example_questions = [
                    "Pr'ì„ êµ¬í•  ë•Œ ì™œ Prì—ì„œ Qcë¥¼ ë¹¼ëŠ” ê±´ê°€ìš”?",
                    "ë¯¸ë¶„í•  ë•Œ d/daë¥¼ ì ìš©í•˜ë©´ ì™œ sëŠ” 1ì´ê³  aëŠ” 0ì´ ë˜ëŠ” ê±´ê°€ìš”?",
                    "ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ ë‹¨ìœ„ë²¡í„°ë¥¼ êµ¬í•˜ëŠ” ë°©ì‹ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                    "ë¼í”Œë¼ìŠ¤ ë³€í™˜ì˜ ì •ì˜ì™€ ì„±ì§ˆì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
                ]
                
                for i, example in enumerate(example_questions, 1):
                    gr.Button(f"{i}. {example[:30]}...", size="sm").click(
                        lambda x=example: (x, []),
                        outputs=[question_input, chatbot]
                    )
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        submit_btn.click(
            chat_function,
            inputs=[question_input, chatbot, image_input],
            outputs=[question_input, chatbot]
        ).then(
            lambda: ("", None),
            outputs=[question_input, image_input]
        )
        
        question_input.submit(
            chat_function,
            inputs=[question_input, chatbot, image_input],
            outputs=[question_input, chatbot]
        ).then(
            lambda: ("", None),
            outputs=[question_input, image_input]
        )
        
        clear_btn.click(
            lambda: ([], "", None),
            outputs=[chatbot, question_input, image_input]
        )
        
        status_btn.click(
            status_function,
            outputs=status_output
        )
        
        # ì´ˆê¸° ìƒíƒœ ë¡œë“œ
        iface.load(
            status_function,
            outputs=status_output
        )
    
    return iface


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ì„¤ì • ë¡œë“œ
        config = Config()
        logger.info("Configuration loaded")
        
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        service = IntegratedChatService(config)
        logger.info("IntegratedChatService initialized")
        
        # Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        iface = create_gradio_interface(service)
        
        # ì„œë²„ ì‹œì‘
        logger.info(f"Starting server on {config.app.server_name}:{config.app.server_port}")
        iface.launch(
            server_name=config.app.server_name,
            server_port=config.app.server_port,
            share=config.app.share,
            show_error=True
        )
        
    except Exception as e:
        logger.error(f"Failed to start application: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()