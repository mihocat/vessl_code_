#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Client Module for RAG System
LLM í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆ - ë²”ìš© AI ì‹œìŠ¤í…œ
"""

import requests
import logging
import time
from typing import Optional, Dict, Any
from config import LLMConfig

logger = logging.getLogger(__name__)


class LLMClient:
    """LLM ì„œë²„ í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, config: Optional[LLMConfig] = None):
        """
        LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        Args:
            config: LLM ì„¤ì • ê°ì²´
        """
        self.config = config or LLMConfig()
        logger.info(f"LLMClient initialized with model: {self.config.model_name}")
        self.session = requests.Session()
        self._setup_urls()
        
    def _setup_urls(self):
        """API ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •"""
        self.health_check_url = f"{self.config.base_url}/health"
        # vLLMì€ OpenAI í˜¸í™˜ API ì‚¬ìš© (chat/completions)
        self.completions_url = f"{self.config.base_url}/v1/chat/completions"
        
    def check_health(self) -> bool:
        """ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = self.session.get(
                self.health_check_url, 
                timeout=self.config.health_check_timeout
            )
            return response.status_code == 200
        except Exception as e:
            logger.debug(f"Health check failed: {e}")
            return False
    
    def query(
        self, 
        prompt: str, 
        context: str = "", 
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> str:
        """
        LLM ì§ˆì˜ - ë²”ìš© AI ì‹œìŠ¤í…œ
        
        Args:
            prompt: ì‚¬ìš©ì ì§ˆë¬¸
            context: ì°¸ê³  ì»¨í…ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„
            
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        try:
            # íŒŒë¼ë¯¸í„° ì„¤ì •
            max_tokens = max_tokens or self.config.max_tokens
            temperature = temperature or self.config.temperature
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = self._build_prompt(prompt, context)
            
            # API ìš”ì²­ í˜ì´ë¡œë“œ
            payload = self._build_payload(full_prompt, max_tokens, temperature)
            
            # API í˜¸ì¶œ
            response = self._make_request(payload)
            
            if response.status_code == 200:
                result = self._extract_response(response)
                return self._post_process_response(result)
            else:
                logger.error(f"LLM API error: {response.status_code} - {response.text}")
                return self._get_error_message()
                
        except requests.exceptions.Timeout:
            logger.error("LLM request timeout")
            return "ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        except Exception as e:
            logger.error(f"LLM query failed: {str(e)}")
            return self._get_error_message()
    
    def _build_prompt(self, prompt: str, context: str) -> str:
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„± - í† í° ì œí•œ ê³ ë ¤"""
        # ì „ê¸°ê³µí•™ ì „ë¬¸ AIë¡œ ëª…í™•íˆ ì„¤ì •
        system_role = """ë‹¹ì‹ ì€ ì „ê¸°ê³µí•™ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”:
1. ì°¸ê³ ìë£Œê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ìˆ˜ì‹ì´ë‚˜ ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš° ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”
3. ì „ê¸°ê³µí•™ ìš©ì–´ë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”
4. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”"""

        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (2048 í† í° ëª¨ë¸ ëŒ€ì‘)
        if context and context.strip():
            # ì»¨í…ìŠ¤íŠ¸ë¥¼ í† í° ì œí•œì— ë§ê²Œ ìë¥´ê¸°
            max_context_chars = 1500  # ì•½ 500-600 í† í°
            if len(context) > max_context_chars:
                context = context[:max_context_chars] + "..."
                logger.info(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ: {len(context)}ìë¡œ ì¶•ì†Œ")
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆì„ ë•ŒëŠ” ì°¸ê³ ìë£Œ ê°•ì¡°
            full_prompt = f"""{system_role}

=== ì°¸ê³ ìë£Œ ===
{context}
=================

ìœ„ ì°¸ê³ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {prompt}

ë‹µë³€:"""
        else:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì„ ë•ŒëŠ” ì¼ë°˜ì ì¸ ë‹µë³€
            full_prompt = f"""{system_role}

ì§ˆë¬¸: {prompt}

ë‹µë³€:"""
            
        return full_prompt
    
    def _build_payload(
        self, 
        prompt: str, 
        max_tokens: int, 
        temperature: float
    ) -> Dict[str, Any]:
        """API ìš”ì²­ í˜ì´ë¡œë“œ êµ¬ì„± - OpenAI í˜¸í™˜ í˜•ì‹"""
        # í† í° ì œí•œì„ ì ì ˆí•˜ê²Œ ì¡°ì • (2048 í† í° ëª¨ë¸ ëŒ€ì‘)
        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì¡°ì •
        prompt_tokens = len(prompt.split())  # ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •
        if prompt_tokens > 1200:
            max_tokens = min(max_tokens, 300)  # ê¸´ í”„ë¡¬í”„íŠ¸ì¼ ë•Œ ë” ì œí•œ
        elif prompt_tokens > 800:
            max_tokens = min(max_tokens, 500)  # ì¤‘ê°„ ê¸¸ì´
        else:
            max_tokens = min(max_tokens, 700)  # ì§§ì€ í”„ë¡¬í”„íŠ¸ì¼ ë•Œ ë” ë§ì´ í—ˆìš©
        
        # OpenAI í˜¸í™˜ messages í˜•ì‹ìœ¼ë¡œ ë³€ê²½
        payload = {
            "model": self.config.model_name,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": self.config.top_p,
            "presence_penalty": self.config.presence_penalty,
            "frequency_penalty": self.config.frequency_penalty,
            "stop": ["<|eot_id|>"]
        }
        
        # repetition_penalty ì¶”ê°€ (vLLMì´ ì§€ì›í•˜ëŠ” ê²½ìš°)
        if hasattr(self.config, 'repetition_penalty'):
            payload["repetition_penalty"] = self.config.repetition_penalty
        logger.debug(f"Building OpenAI-compatible payload with model: {self.config.model_name}")
        return payload
    
    def _make_request(self, payload: Dict[str, Any]) -> requests.Response:
        """API ìš”ì²­ ì‹¤í–‰"""
        return self.session.post(
            self.completions_url,
            json=payload,
            timeout=self.config.timeout
        )
    
    def _extract_response(self, response: requests.Response) -> str:
        """ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ - OpenAI í˜¸í™˜ í˜•ì‹"""
        result = response.json()
        
        # OpenAI í˜¸í™˜ ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬
        if "choices" in result and result["choices"]:
            choice = result["choices"][0]
            # chat/completions ì‘ë‹µ: message.content
            if "message" in choice:
                return choice["message"].get("content", "").strip()
            # completions ì‘ë‹µ: text (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
            elif "text" in choice:
                return choice["text"].strip()
        
        # ê¸°íƒ€ í˜•ì‹ ì²˜ë¦¬ (ê¸°ì¡´ í˜¸í™˜ì„±)
        elif "text" in result:
            return result["text"].strip()
        elif "generated_text" in result:
            return result["generated_text"].strip()
            
        return ""
    
    def _post_process_response(self, response: str) -> str:
        """ì‘ë‹µ í›„ì²˜ë¦¬"""
        if not response:
            return self._get_error_message()
            
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        response = response.strip()
        
        # ì‘ë‹µì´ ë„ˆë¬´ ì§§ì€ ê²½ìš°
        if len(response) < 10:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¶©ë¶„í•œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
        return response
    
    def _get_error_message(self) -> str:
        """ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜"""
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def wait_for_server(
        self, 
        max_retries: int = 60, 
        retry_interval: int = 3
    ) -> bool:
        """
        ì„œë²„ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        
        Args:
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            retry_interval: ì¬ì‹œë„ ê°„ê²© (ì´ˆ)
            
        Returns:
            ì„œë²„ ì¤€ë¹„ ì—¬ë¶€
        """
        logger.info("Waiting for LLM server...")
        
        for attempt in range(max_retries):
            if self.check_health():
                logger.info("LLM server is ready")
                return True
                
            if attempt < max_retries - 1:
                time.sleep(retry_interval)
                
        logger.error("LLM server failed to start")
        return False
    
    def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        self.session.close()
    
    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì§„ì…"""
        return self
    
    def generate_response(
        self, 
        question: str, 
        context: str = "", 
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> str:
        """
        ì‘ë‹µ ìƒì„± - integrated_pipeline.pyì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ì°¸ê³  ì»¨í…ìŠ¤íŠ¸ (RAG ê²°ê³¼)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„
            
        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        logger.info(f"ğŸ¤– LLM generate_response ì‹œì‘: {question[:100]}...")
        
        try:
            start_time = time.time()
            
            # query ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±
            response = self.query(
                prompt=question,
                context=context,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            generation_time = time.time() - start_time
            logger.info(f"âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ ({generation_time:.2f}ì´ˆ)")
            logger.info(f"ğŸ“ LLM ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
            logger.info(f"ğŸ“‹ LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response[:200]}...")
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì¢…ë£Œ"""
        self.close()
