#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gradio UI Application for RAG System
RAG ì‹œìŠ¤í…œ Gradio UI ì• í”Œë¦¬ì¼€ì´ì…˜
"""

import sys
import time
import logging
from typing import List, Tuple, Optional, Union
from PIL import Image
import torch

import gradio as gr

from config import Config
from llm_client import LLMClient
from rag_system import RAGSystem, SearchResult
from services import WebSearchService, ResponseGenerator
try:
    # ìƒˆë¡œìš´ Vision Transformer ë¶„ì„ê¸° ì‹œë„
    from vision_transformer_analyzer import Florence2ImageAnalyzer
    logger.info("Using Vision Transformer Analyzer")
except ImportError:
    # ê¸°ì¡´ ë¶„ì„ê¸°ë¡œ í´ë°±
    from new_image_analyzer import Florence2ImageAnalyzer
    logger.info("Using Real OCR Analyzer")
from image_analyzer import MultimodalRAGService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ChatService:
    """í†µí•© ì±—ë´‡ ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: Config, llm_client: LLMClient):
        """
        ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            config: ì „ì²´ ì„¤ì • ê°ì²´
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸
        """
        self.config = config
        self.llm_client = llm_client
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.rag_system = RAGSystem(
            rag_config=config.rag,
            dataset_config=config.dataset,
            llm_client=llm_client
        )
        self.web_search = WebSearchService(config.web_search)
        self.response_generator = ResponseGenerator(config.web_search)
        
        # ì´ë¯¸ì§€ ë¶„ì„ê¸° ì´ˆê¸°í™” (ì„ íƒì )
        self.image_analyzer = None
        self.multimodal_service = None
        
        # Florence-2 ì´ˆê¸°í™” ì¬ì‹œë„ ë¡œì§
        max_attempts = 3
        for attempt in range(max_attempts):
            try:
                logger.info(f"Initializing Florence-2 image analyzer (attempt {attempt + 1}/{max_attempts})...")
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # Vision Transformer ë˜ëŠ” Real OCR ì‚¬ìš©
                self.image_analyzer = Florence2ImageAnalyzer()
                self.multimodal_service = MultimodalRAGService(
                    self.image_analyzer,
                    self.rag_system.embedding_model
                )
                logger.info("Florence-2 image analyzer initialized successfully")
                break
            except torch.cuda.OutOfMemoryError:
                logger.error(f"GPU out of memory during Florence-2 initialization (attempt {attempt + 1})")
                # GPU ë©”ëª¨ë¦¬ ê°•ì œ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                self.image_analyzer = None
                self.multimodal_service = None
                
                if attempt == max_attempts - 1:
                    logger.error("Florence-2 initialization failed due to GPU memory. Image analysis will be disabled.")
                else:
                    time.sleep(3)  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•´ ë” ê¸´ ëŒ€ê¸°
            except Exception as e:
                logger.warning(f"Failed to initialize image analyzer (attempt {attempt + 1}/{max_attempts}): {e}")
                self.image_analyzer = None
                self.multimodal_service = None
                
                if attempt == max_attempts - 1:  # ë§ˆì§€ë§‰ ì‹œë„
                    logger.error("Florence-2 initialization failed after all attempts")
                else:
                    # ë‹¤ìŒ ì‹œë„ ì „ ëŒ€ê¸°
                    time.sleep(2)
        
        # ëŒ€í™” ì´ë ¥
        self.conversation_history = []
        
    def process_query(
        self, 
        question: str, 
        history: List[Tuple[str, str]],
        image: Optional[Image.Image] = None
    ) -> str:
        """
        ì‚¬ìš©ì ì§ˆì˜ ì²˜ë¦¬
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            history: ëŒ€í™” ì´ë ¥
            image: ì„ íƒì  ì´ë¯¸ì§€ ì…ë ¥
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        start_time = time.time()
        
        # ë¹ˆ ì§ˆë¬¸ ì²˜ë¦¬
        if not question or not question.strip():
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
        
        try:
            # ì´ë¯¸ì§€ ë¶„ì„ (ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
            image_context = None
            original_question = question  # ì›ë³¸ ì§ˆë¬¸ ì €ì¥
            
            if image:
                if self.multimodal_service and self.image_analyzer:
                    try:
                        logger.info("Processing image with Florence-2...")
                        # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ ë° ì¡°ì •
                        if hasattr(image, 'size'):
                            width, height = image.size
                            max_size = 1024
                            if width > max_size or height > max_size:
                                # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
                                ratio = min(max_size/width, max_size/height)
                                new_width = int(width * ratio)
                                new_height = int(height * ratio)
                                image = image.resize((new_width, new_height), Image.LANCZOS)
                                logger.info(f"Resized image from {width}x{height} to {new_width}x{new_height}")
                        
                        multimodal_result = self.multimodal_service.process_multimodal_query(
                            question, image
                        )
                        # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ì§ˆë¬¸ì— í¬í•¨
                        question = multimodal_result["combined_query"]
                        image_context = multimodal_result.get("image_analysis", {})
                        logger.info(f"Image analysis completed: {image_context}")
                    except Exception as e:
                        logger.error(f"Image analysis failed: {e}", exc_info=True)
                        # Florence-2 ì‹¤íŒ¨ ì‹œì—ë„ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ì‘ë‹µ ì œê³µ
                        image_context = {
                            "error": str(e),
                            "caption": "[ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨]",
                            "ocr_text": ""
                        }
                        # ì§ˆë¬¸ì€ ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ê°€ëŠ¥í•˜ê²Œ í•¨
                        question = original_question
                else:
                    # Florence-2 ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
                    logger.warning("Image analyzer not available")
                    image_context = {
                        "error": "Image analyzer not initialized",
                        "caption": "[ì´ë¯¸ì§€ ë¶„ì„ ê¸°ëŠ¥ ë¹„í™œì„±í™”]",
                        "ocr_text": ""
                    }
                    # ì§ˆë¬¸ì€ ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€
                    question = original_question
            
            # RAG ê²€ìƒ‰ ìˆ˜í–‰
            results, max_score = self.rag_system.search(question)
            
            # ì‘ë‹µ ìƒì„±
            response = self._generate_response(question, results, max_score, image_context)
            
            # ì‘ë‹µ ì‹œê°„ ì¶”ê°€
            elapsed_time = time.time() - start_time
            response += f"\n\n_ì‘ë‹µì‹œê°„: {elapsed_time:.2f}ì´ˆ_"
            
            # ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸
            self.conversation_history.append((question, response))
            
            return response
            
        except Exception as e:
            logger.error(f"Query processing failed: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    
    def _generate_response(
        self, 
        question: str, 
        results: List[SearchResult], 
        max_score: float,
        image_context: Optional[dict] = None
    ) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            results: RAG ê²€ìƒ‰ ê²°ê³¼
            max_score: ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜
            image_context: ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        # response_header = f"### ì§ˆë¬¸: {question}\n\n"
        response_header = "ë‹µë³€: "
        
        # ì‹ ë¢°ë„ ìˆ˜ì¤€ ê²°ì • (ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ í•­ìƒ LLM ì‚¬ìš©)
        if image_context:
            # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° í•­ìƒ LLM ì‚¬ìš©
            confidence_level = "medium" if max_score >= self.config.rag.medium_confidence_threshold else "low"
        else:
            # ê¸°ì¡´ ë¡œì§
            if max_score >= self.config.rag.high_confidence_threshold:
                confidence_level = "high"
            elif max_score >= self.config.rag.medium_confidence_threshold:
                confidence_level = "medium"
            else:
                confidence_level = "low"
        
        # ë†’ì€ ì‹ ë¢°ë„ - ì§ì ‘ ë‹µë³€ ì‚¬ìš© (ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        if confidence_level == "high" and results and not image_context:
            best_result = results[0]
            response = response_header + best_result.answer
            
            if best_result.category and best_result.category != "general":
                response += f"\n\n_[ì¹´í…Œê³ ë¦¬: {best_result.category}]_"
        
        # ì¤‘ê°„/ë‚®ì€ ì‹ ë¢°ë„ ë˜ëŠ” ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° - LLM í™œìš©
        else:
            # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (ë‚®ì€ ì‹ ë¢°ë„ì´ê³  ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°)
            web_results = []
            if confidence_level == "low" and not image_context:
                web_results = self.web_search.search(question)
            
            # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
            context = self.response_generator.prepare_context(results, web_results, image_context)
            
            # ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨ ë©”ì‹œì§€ ì¶”ê°€
            image_error_prefix = ""
            if image_context and "error" in image_context:
                if "caption" in image_context and "[ì´ë¯¸ì§€ ë¶„ì„" in image_context["caption"]:
                    image_error_prefix = "[ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨] "
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.response_generator.generate_prompt(
                question, context, confidence_level
            )
            
            # LLM ì‘ë‹µ ìƒì„±
            try:
                if prompt:  # í”„ë¡¬í”„íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ LLM í˜¸ì¶œ
                    llm_response = self.llm_client.query(prompt, "")
                    response = response_header + image_error_prefix + llm_response
                else:
                    response = response_header + image_error_prefix + "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            except Exception as e:
                logger.error(f"LLM response generation failed: {e}")
                if image_error_prefix:
                    response = response_header + image_error_prefix + "ì´ë¯¸ì§€ëŠ” ë¶„ì„í•  ìˆ˜ ì—†ì—ˆì§€ë§Œ, í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤. ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì¶”ê°€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                else:
                    response = response_header + "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ
        related_questions = self._get_related_questions(question, results)
        if related_questions:
            response += "\n\n**ğŸ’¡ ê´€ë ¨ ì§ˆë¬¸:**"
            for q in related_questions:
                response += f"\n- {q}"
        
        # ì ìˆ˜ì™€ ì‹ ë¢°ë„ ì •ë³´ í‘œì‹œ
        response += f"\n\n[ì ìˆ˜: {max_score:.3f}, ì‹ ë¢°ë„: {confidence_level}]"
        
        return response
    
    def _get_related_questions(
        self, 
        question: str, 
        results: List[SearchResult]
    ) -> List[str]:
        """
        ê´€ë ¨ ì§ˆë¬¸ ì¶”ì¶œ
        
        Args:
            question: ì›ë³¸ ì§ˆë¬¸
            results: ê²€ìƒ‰ ê²°ê³¼
            
        Returns:
            ê´€ë ¨ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        if not results or len(results) < 2:
            return []
        
        related = []
        seen_questions = {question.lower()}
        
        for result in results[1:]:  # ì²« ë²ˆì§¸ ê²°ê³¼ëŠ” ì œì™¸
            if result.score >= 0.6:
                q_lower = result.question.lower()
                if q_lower not in seen_questions:
                    related.append(result.question)
                    seen_questions.add(q_lower)
                    
                    if len(related) >= 3:
                        break
        
        return related


def create_gradio_app(config: Optional[Config] = None) -> gr.Blocks:
    """
    Gradio ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
    
    Args:
        config: ì„¤ì • ê°ì²´ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        
    Returns:
        Gradio Blocks ì¸ìŠ¤í„´ìŠ¤
    """
    # ì„¤ì • ì´ˆê¸°í™”
    if config is None:
        config = Config()
    
    # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    llm_client = LLMClient(config.llm)
    
    # ì„œë²„ ëŒ€ê¸°
    logger.info("Waiting for LLM server...")
    if not llm_client.wait_for_server():
        logger.error("Failed to connect to LLM server")
        raise RuntimeError("LLM server connection failed")
    
    # ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    chat_service = ChatService(config, llm_client)
    
    # Gradio ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(title=config.app.title, theme=gr.themes.Soft()) as app:
        gr.Markdown(config.app.description)
        
        # ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
        with gr.Row():
            with gr.Column(scale=8):
                # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
                chatbot = gr.Chatbot(
                    label="ëŒ€í™”",
                    height=500,
                    bubble_full_width=False,
                    show_label=True
                )
                
                # ì…ë ¥ ì˜ì—­
                with gr.Row():
                    with gr.Column(scale=4):
                        msg = gr.Textbox(
                            label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
                            placeholder="ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”...",
                            lines=2
                        )
                        image_input = gr.Image(
                            label="ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)",
                            type="pil",
                            height=200
                        )
                    with gr.Column(scale=1):
                        submit = gr.Button("ì „ì†¡", variant="primary")
                        clear = gr.Button("ì´ˆê¸°í™”")
            
            # ì‚¬ì´ë“œë°”
            with gr.Column(scale=2):
                # ì˜ˆì œ ì§ˆë¬¸
                gr.Markdown("### ğŸ’¡ ì˜ˆì œ ì§ˆë¬¸")
                examples = gr.Examples(
                    examples=config.app.example_questions,
                    inputs=msg,
                    label="í´ë¦­í•˜ì—¬ ì‚¬ìš©"
                )
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        def respond(message: str, image, chat_history: List[Tuple[str, str]]):
            """ë©”ì‹œì§€ ì‘ë‹µ ì²˜ë¦¬"""
            if not message.strip():
                return "", None, chat_history
            
            response = chat_service.process_query(message, chat_history, image)
            
            # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ëŒ€í™”ì— í‘œì‹œ
            if image:
                chat_history.append((f"{message}\n[ì´ë¯¸ì§€ ì²¨ë¶€ë¨]", response))
            else:
                chat_history.append((message, response))
            
            return "", None, chat_history
        
        def clear_chat():
            """ëŒ€í™” ì´ˆê¸°í™”"""
            chat_service.conversation_history.clear()
            return None, "", None
        
        # ì´ë²¤íŠ¸ ë°”ì¸ë”©
        submit.click(respond, [msg, image_input, chatbot], [msg, image_input, chatbot])
        msg.submit(respond, [msg, image_input, chatbot], [msg, image_input, chatbot])
        clear.click(clear_chat, None, [chatbot, msg, image_input])
        
        # í†µê³„ í‘œì‹œ
        with gr.Accordion("ğŸ“Š ì„œë¹„ìŠ¤ í†µê³„", open=False):
            with gr.Row():
                stats_display = gr.Textbox(
                    label="í†µê³„",
                    interactive=False,
                    lines=6
                )
                
                def update_stats():
                    """í†µê³„ ì—…ë°ì´íŠ¸"""
                    stats = chat_service.rag_system.get_stats()
                    total = stats['total_queries']
                    
                    if total > 0:
                        high_pct = (stats['high_confidence_hits'] / total) * 100
                        medium_pct = (stats['medium_confidence_hits'] / total) * 100
                        low_pct = (stats['low_confidence_hits'] / total) * 100
                    else:
                        high_pct = medium_pct = low_pct = 0
                    
                    return (
                        f"ì´ ì¿¼ë¦¬ ìˆ˜: {total}\n"
                        f"ë†’ì€ ì‹ ë¢°ë„: {stats['high_confidence_hits']} ({high_pct:.1f}%)\n"
                        f"ì¤‘ê°„ ì‹ ë¢°ë„: {stats['medium_confidence_hits']} ({medium_pct:.1f}%)\n"
                        f"ë‚®ì€ ì‹ ë¢°ë„: {stats['low_confidence_hits']} ({low_pct:.1f}%)\n"
                        f"í‰ê·  ì‘ë‹µì‹œê°„: {stats['avg_response_time']:.2f}ì´ˆ"
                    )
                
                refresh_stats = gr.Button("ìƒˆë¡œê³ ì¹¨", size="sm")
                refresh_stats.click(update_stats, None, stats_display)
                
                # ì´ˆê¸° í†µê³„ í‘œì‹œ
                app.load(update_stats, None, stats_display)
    
    return app


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="RAG System Gradio UI")
    parser.add_argument(
        "--config", 
        type=str, 
        help="Path to config file (JSON format)"
    )
    parser.add_argument(
        "--server-port", 
        type=int, 
        default=7860,
        help="Server port (default: 7860)"
    )
    parser.add_argument(
        "--share", 
        action="store_true",
        help="Create public Gradio link"
    )
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = Config()
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ ì˜¤ë²„ë¼ì´ë“œ
    if args.server_port:
        config.app.server_port = args.server_port
    if args.share:
        config.app.share = args.share
    
    # ì•± ìƒì„± ë° ì‹¤í–‰
    try:
        app = create_gradio_app(config)
        app.launch(
            server_name=config.app.server_name,
            server_port=config.app.server_port,
            share=config.app.share
        )
    except Exception as e:
        logger.error(f"Failed to launch app: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()