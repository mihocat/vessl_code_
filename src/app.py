#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì°¨ì„¸ëŒ€ AI ì±—ë´‡ í†µí•© Gradio UI ì• í”Œë¦¬ì¼€ì´ì…˜
Next-Generation AI Chatbot Integrated Gradio UI Application

í†µí•© ì„œë¹„ìŠ¤ + ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ + ê³ ê¸‰ AI ì‹œìŠ¤í…œ
Integrated Service + Multimodal Processing + Advanced AI System
"""

import sys
import time
import asyncio
import logging
import uuid
from typing import List, Tuple, Optional, Union, Dict, Any
from PIL import Image
import torch

import gradio as gr

from config import Config
# ì°¨ì„¸ëŒ€ í†µí•© ì‹œìŠ¤í…œ ì„í¬íŠ¸
from integrated_service import IntegratedAIService, ServiceConfig, ServiceRequest, ServiceResponse
from advanced_ai_system import ReasoningType

# ê¸°ì¡´ ì‹œìŠ¤í…œ í˜¸í™˜ì„± ìœ ì§€
from rag_system import RAGSystem, SearchResult
from services import WebSearchService, ResponseGenerator
from intelligent_rag_adapter import IntelligentRAGAdapter

# ì¡°ê±´ë¶€ LLM í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸ (í´ë°±ìš©)
import os
if os.getenv("USE_OPENAI_LLM", "false").lower() == "true" or os.getenv("SKIP_VLLM", "false").lower() == "true":
    from llm_client_openai import LLMClient
else:
    from llm_client import LLMClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    # ë‹¨ìˆœí™”ëœ ì´ë¯¸ì§€ ë¶„ì„ê¸° ìš°ì„  ì‹œë„
    from simple_image_analyzer import Florence2ImageAnalyzer, SimpleMultimodalRAGService
    logger.info("Using Simple Image Analyzer")
    USE_SIMPLE_ANALYZER = True
except ImportError as e:
    logger.warning(f"Simple Image Analyzer import failed: {e}")
    USE_SIMPLE_ANALYZER = False
    try:
        # Vision Transformer ë¶„ì„ê¸° í´ë°±
        from vision_transformer_analyzer import Florence2ImageAnalyzer
        logger.info("Using Vision Transformer Analyzer as fallback")
    except ImportError as e2:
        logger.warning(f"Vision Transformer import failed: {e2}")
        # ìµœì¢… í´ë°±: ì›ë³¸ Florence-2
        from image_analyzer import Florence2ImageAnalyzer
        logger.info("Using original Florence-2 Analyzer as final fallback")

# MultimodalRAGService import
if USE_SIMPLE_ANALYZER:
    MultimodalRAGService = SimpleMultimodalRAGService
else:
    from image_analyzer import MultimodalRAGService


class NextGenChatService:
    """ì°¨ì„¸ëŒ€ í†µí•© ì±—ë´‡ ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: Config):
        """
        ì°¨ì„¸ëŒ€ ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            config: ì „ì²´ ì„¤ì • ê°ì²´
        """
        self.config = config
        
        # ì„œë¹„ìŠ¤ ì„¤ì • êµ¬ì„±
        service_config = ServiceConfig(
            service_mode="hybrid",  # ê³ ê¸‰/ê¸°ë³¸ ì‹œìŠ¤í…œ ìë™ ì„ íƒ
            enable_openai_vision=True,
            enable_ncp_ocr=True,
            enable_rag=True,
            enable_fine_tuned_llm=True,
            enable_reasoning=True,
            enable_memory=True,
            enable_agents=True,
            max_concurrent_requests=10,
            request_timeout=120.0,
            cache_results=True,
            min_confidence_threshold=0.6,
            enable_result_validation=True,
            enable_fallback_chain=True,
            log_detailed_processing=True
        )
        
        # í†µí•© AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        try:
            self.ai_service = IntegratedAIService(service_config)
            self.service_available = True
            logger.info("Next-generation AI service initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize next-gen AI service: {e}")
            self.ai_service = None
            self.service_available = False
            
            # í´ë°±: ê¸°ì¡´ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            logger.info("Falling back to legacy system...")
            self._initialize_legacy_system()
        
        # ëŒ€í™” ì´ë ¥ ë° ì„¸ì…˜ ê´€ë¦¬
        self.conversation_history = []
        self.current_session_id = None
        self.session_stats = {
            'queries_count': 0,
            'successful_responses': 0,
            'failed_responses': 0,
            'total_processing_time': 0.0
        }
    
    def _initialize_legacy_system(self):
        """ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (í´ë°±ìš©)"""
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            llm_client = LLMClient(self.config.llm)
            
            # ì„œë²„ ëŒ€ê¸°
            logger.info("Waiting for LLM server...")
            if not llm_client.wait_for_server():
                logger.error("Failed to connect to LLM server")
                raise RuntimeError("LLM server connection failed")
            
            # ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            self.legacy_rag_system = RAGSystem(
                rag_config=self.config.rag,
                dataset_config=self.config.dataset,
                llm_client=llm_client
            )
            self.legacy_web_search = WebSearchService(self.config.web_search)
            self.legacy_response_generator = ResponseGenerator(self.config.web_search)
            self.legacy_intelligent_adapter = IntelligentRAGAdapter(self.config, llm_client)
            
            self.legacy_available = True
            logger.info("Legacy system initialized successfully")
            
        except Exception as e:
            logger.error(f"Legacy system initialization failed: {e}")
            self.legacy_available = False
        
    def process_query(
        self, 
        question: str, 
        history: List[Tuple[str, str]],
        image: Optional[Image.Image] = None,
        processing_mode: Optional[str] = None,
        reasoning_type: Optional[str] = None
    ) -> str:
        """
        ì‚¬ìš©ì ì§ˆì˜ ì²˜ë¦¬ (ì°¨ì„¸ëŒ€ í†µí•© ì‹œìŠ¤í…œ)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            history: ëŒ€í™” ì´ë ¥
            image: ì„ íƒì  ì´ë¯¸ì§€ ì…ë ¥
            processing_mode: ì²˜ë¦¬ ëª¨ë“œ (advanced, multimodal, basic)
            reasoning_type: ì¶”ë¡  íƒ€ì… (chain_of_thought, deductive, etc.)
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        start_time = time.time()
        self.session_stats['queries_count'] += 1
        
        # ë¹ˆ ì§ˆë¬¸ ì²˜ë¦¬
        if not question or not question.strip():
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
        
        try:
            # ì°¨ì„¸ëŒ€ ì‹œìŠ¤í…œ ì‚¬ìš©
            if self.service_available and self.ai_service:
                return self._process_with_nextgen_system(
                    question, history, image, processing_mode, reasoning_type
                )
            
            # í´ë°±: ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ì‚¬ìš©
            elif hasattr(self, 'legacy_available') and self.legacy_available:
                return self._process_with_legacy_system(question, history, image)
            
            else:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ AI ì‹œìŠ¤í…œì´ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            self.session_stats['failed_responses'] += 1
            logger.error(f"Query processing failed: {e}")
            return f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _process_with_nextgen_system(self, 
                                   question: str, 
                                   history: List[Tuple[str, str]], 
                                   image: Optional[Image.Image],
                                   processing_mode: Optional[str],
                                   reasoning_type: Optional[str]) -> str:
        """ì°¨ì„¸ëŒ€ ì‹œìŠ¤í…œìœ¼ë¡œ ì²˜ë¦¬"""
        try:
            # ì´ë¯¸ì§€ ë°ì´í„° ì¤€ë¹„
            image_data = None
            if image:
                import io
                import base64
                
                # PIL ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
                img_buffer = io.BytesIO()
                image.save(img_buffer, format='PNG')
                img_buffer.seek(0)
                image_data = img_buffer.read()
            
            # ì„¸ì…˜ ID ê´€ë¦¬
            if not self.current_session_id:
                self.current_session_id = str(uuid.uuid4())
            
            # ì„œë¹„ìŠ¤ ìš”ì²­ ìƒì„±
            request = ServiceRequest(
                request_id=str(uuid.uuid4()),
                query=question,
                image_data=image_data,
                session_id=self.current_session_id,
                processing_mode=processing_mode,
                reasoning_type=reasoning_type,
                metadata={
                    'conversation_history': len(history),
                    'has_image': image is not None
                }
            )
            
            # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ë™ê¸°ë¡œ ë˜í•‘
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                response = loop.run_until_complete(
                    self.ai_service.process_request(request)
                )
            finally:
                loop.close()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = response.processing_time
            self.session_stats['total_processing_time'] += processing_time
            
            if response.success:
                self.session_stats['successful_responses'] += 1
                
                # ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸
                self.conversation_history.append((question, response.response))
                
                # ì‘ë‹µ êµ¬ì„±
                final_response = response.response
                
                # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
                if response.metadata:
                    system_info = []
                    if 'processing_system' in response.metadata:
                        system_info.append(f"ì‹œìŠ¤í…œ: {response.metadata['processing_system']}")
                    if 'system_capabilities' in response.metadata:
                        capabilities = response.metadata['system_capabilities']
                        if len(capabilities) > 3:
                            system_info.append(f"ê¸°ëŠ¥: {', '.join(capabilities[:3])}...")
                        else:
                            system_info.append(f"ê¸°ëŠ¥: {', '.join(capabilities)}")
                    
                    if system_info:
                        final_response += f"\n\n_[{', '.join(system_info)}]_"
                
                # ì²˜ë¦¬ ì‹œê°„ ë° ì‹ ë¢°ë„ ì •ë³´
                final_response += (f"\n\n_ì‘ë‹µì‹œê°„: {processing_time:.2f}ì´ˆ, "
                                 f"ì‹ ë¢°ë„: {response.confidence_score:.2f}_")
                
                return final_response
                
            else:
                self.session_stats['failed_responses'] += 1
                error_msg = response.response
                if response.error_message:
                    error_msg += f"\n\nì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­: {response.error_message}"
                return error_msg
                
        except Exception as e:
            logger.error(f"Next-gen system processing failed: {e}")
            raise
    
    def _process_with_legacy_system(self, 
                                  question: str, 
                                  history: List[Tuple[str, str]], 
                                  image: Optional[Image.Image]) -> str:
        """ë ˆê±°ì‹œ ì‹œìŠ¤í…œìœ¼ë¡œ ì²˜ë¦¬"""
        try:
            # ê¸°ì¡´ ë¡œì§ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ë‹¨ìˆœí™”
            logger.info("Using legacy system for processing")
            
            # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ê²½ê³  ë©”ì‹œì§€
            image_warning = ""
            if image:
                image_warning = "\n\nâš ï¸ ë ˆê±°ì‹œ ëª¨ë“œì—ì„œëŠ” ì´ë¯¸ì§€ ë¶„ì„ì´ ì œí•œì ì…ë‹ˆë‹¤."
            
            # RAG ê²€ìƒ‰ ìˆ˜í–‰
            results, max_score = self.legacy_rag_system.search(question)
            
            # ì‹ ë¢°ë„ì— ë”°ë¥¸ ì‘ë‹µ ìƒì„±
            if max_score >= 0.8:
                response = results[0].answer if results else "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            elif max_score >= 0.6:
                # ì¤‘ê°„ ì‹ ë¢°ë„: ì›¹ ê²€ìƒ‰ ì¶”ê°€
                web_results = self.legacy_web_search.search(question)
                context = self.legacy_response_generator.prepare_context(results, web_results)
                prompt = self.legacy_response_generator.generate_prompt(question, context, "medium")
                response = "RAG ì‹œìŠ¤í…œì„ í†µí•´ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.\n\n"
                if results:
                    response += results[0].answer
                else:
                    response += "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            else:
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ìµœì¢… ì‘ë‹µ êµ¬ì„±
            final_response = response + image_warning
            final_response += f"\n\n_[ë ˆê±°ì‹œ ëª¨ë“œ, ì ìˆ˜: {max_score:.3f}]_"
            
            # ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸
            self.conversation_history.append((question, final_response))
            
            return final_response
            
        except Exception as e:
            logger.error(f"Legacy system processing failed: {e}")
            return f"ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def get_service_status(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ"""
        status = {
            'next_gen_available': self.service_available,
            'legacy_available': getattr(self, 'legacy_available', False),
            'current_session_id': self.current_session_id,
            'session_stats': self.session_stats.copy()
        }
        
        # ì°¨ì„¸ëŒ€ ì‹œìŠ¤í…œ ìƒíƒœ ì¶”ê°€
        if self.service_available and self.ai_service:
            ai_status = self.ai_service.get_service_status()
            status['ai_service_status'] = ai_status
        
        return status
    
    
    def _generate_response(
        self, 
        question: str, 
        results: List[SearchResult], 
        max_score: float,
        image_context: Optional[dict] = None
    ) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            results: RAG ê²€ìƒ‰ ê²°ê³¼
            max_score: ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜
            image_context: ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        # response_header = f"### ì§ˆë¬¸: {question}\n\n"
        response_header = "ë‹µë³€: "
        
        # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: 2ë¶„ê¸° êµ¬ì¡° (0.75 ì´ìƒ = RAG ì§ì ‘, 0.75 ë¯¸ë§Œ = ìì²´ LLM)
        # ChatGPT APIëŠ” ì§ˆì˜ë‹¹ 1ë²ˆë§Œ í˜¸ì¶œ
        
        # ì‹ ë¢°ë„ 0.75 ì´ìƒ: RAG ì§ì ‘ ë‹µë³€ ì‚¬ìš©
        if max_score >= self.config.rag.high_confidence_threshold and results:
            best_result = results[0]
            response = response_header + best_result.answer
            
            if best_result.category and best_result.category != "general":
                response += f"\n\n_[ì¹´í…Œê³ ë¦¬: {best_result.category}]_"
                
            confidence_level = "high"
        
        # ì‹ ë¢°ë„ 0.75 ë¯¸ë§Œ: ìì²´ LLM ì‚¬ìš© (ChatGPT API ë˜ëŠ” vLLM)
        else:
            confidence_level = "low"
            
            # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ChatGPT API 1íšŒ í˜¸ì¶œ (Vision + Text í†µí•©)
            if image_context:
                # ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨ ë©”ì‹œì§€ ì¶”ê°€
                image_error_prefix = ""
                if "error" in image_context:
                    if "caption" in image_context and "[ì´ë¯¸ì§€ ë¶„ì„" in image_context["caption"]:
                        image_error_prefix = "[ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨] "
                
                # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
                context = self.response_generator.prepare_context(results, [], image_context)
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = self.response_generator.generate_prompt(
                    question, context, confidence_level
                )
                
                # ChatGPT API 1íšŒ í˜¸ì¶œ (ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ í†µí•©)
                try:
                    if prompt:
                        llm_response = self.llm_client.query(prompt, "")
                        response = response_header + image_error_prefix + llm_response
                    else:
                        response = response_header + image_error_prefix + "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                except Exception as e:
                    logger.error(f"LLM response generation failed: {e}")
                    response = response_header + image_error_prefix + "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
            # ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš° ìì²´ LLM ì‚¬ìš© (vLLM ìš°ì„ )
            else:
                # ì›¹ ê²€ìƒ‰ ì¶”ê°€ (í•„ìš”ì‹œ)
                web_results = self.web_search.search(question)
                
                # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
                context = self.response_generator.prepare_context(results, web_results, None)
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = self.response_generator.generate_prompt(
                    question, context, confidence_level
                )
                
                # ìì²´ LLM í˜¸ì¶œ (vLLM ìš°ì„ , ChatGPT í´ë°±)
                try:
                    if prompt:
                        llm_response = self.llm_client.query(prompt, "")
                        response = response_header + llm_response
                    else:
                        response = response_header + "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                except Exception as e:
                    logger.error(f"LLM response generation failed: {e}")
                    response = response_header + "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ
        related_questions = self._get_related_questions(question, results)
        if related_questions:
            response += "\n\n**ğŸ’¡ ê´€ë ¨ ì§ˆë¬¸:**"
            for q in related_questions:
                response += f"\n- {q}"
        
        # ì ìˆ˜ì™€ ì‹ ë¢°ë„ ì •ë³´ í‘œì‹œ (2ë¶„ê¸° êµ¬ì¡° í‘œì‹œ)
        system_type = "RAG ì§ì ‘ì‘ë‹µ" if confidence_level == "high" else "ìì²´ LLM"
        response += f"\n\n[ì ìˆ˜: {max_score:.3f}, ì‹œìŠ¤í…œ: {system_type}]"
        
        return response
    
    def _get_related_questions(
        self, 
        question: str, 
        results: List[SearchResult]
    ) -> List[str]:
        """
        ê´€ë ¨ ì§ˆë¬¸ ì¶”ì¶œ
        
        Args:
            question: ì›ë³¸ ì§ˆë¬¸
            results: ê²€ìƒ‰ ê²°ê³¼
            
        Returns:
            ê´€ë ¨ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        if not results or len(results) < 2:
            return []
        
        related = []
        seen_questions = {question.lower()}
        
        for result in results[1:]:  # ì²« ë²ˆì§¸ ê²°ê³¼ëŠ” ì œì™¸
            if result.score >= 0.6:
                q_lower = result.question.lower()
                if q_lower not in seen_questions:
                    related.append(result.question)
                    seen_questions.add(q_lower)
                    
                    if len(related) >= 3:
                        break
        
        return related


def create_gradio_app(config: Optional[Config] = None) -> gr.Blocks:
    """
    ì°¨ì„¸ëŒ€ Gradio ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
    
    Args:
        config: ì„¤ì • ê°ì²´ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        
    Returns:
        Gradio Blocks ì¸ìŠ¤í„´ìŠ¤
    """
    # ì„¤ì • ì´ˆê¸°í™”
    if config is None:
        config = Config()
    
    # ì°¨ì„¸ëŒ€ ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    chat_service = NextGenChatService(config)
    
    # Gradio ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(title=config.app.title, theme=gr.themes.Soft()) as app:
        gr.Markdown(config.app.description)
        
        # ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
        with gr.Row():
            with gr.Column(scale=8):
                # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
                chatbot = gr.Chatbot(
                    label="ëŒ€í™”",
                    height=500,
                    bubble_full_width=False,
                    show_label=True
                )
                
                # ì…ë ¥ ì˜ì—­
                with gr.Row():
                    with gr.Column(scale=4):
                        msg = gr.Textbox(
                            label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
                            placeholder="ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”...",
                            lines=2
                        )
                        image_input = gr.Image(
                            label="ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)",
                            type="pil",
                            height=200
                        )
                    with gr.Column(scale=1):
                        submit = gr.Button("ì „ì†¡", variant="primary")
                        clear = gr.Button("ì´ˆê¸°í™”")
            
            # ì‚¬ì´ë“œë°”
            with gr.Column(scale=2):
                # ì˜ˆì œ ì§ˆë¬¸
                gr.Markdown("### ğŸ’¡ ì˜ˆì œ ì§ˆë¬¸")
                examples = gr.Examples(
                    examples=config.app.example_questions,
                    inputs=msg,
                    label="í´ë¦­í•˜ì—¬ ì‚¬ìš©"
                )
        
        # ê³ ê¸‰ ì„¤ì • íŒ¨ë„
        with gr.Accordion("ğŸ”§ ê³ ê¸‰ ì„¤ì •", open=False):
            with gr.Row():
                processing_mode = gr.Dropdown(
                    choices=["auto", "advanced", "multimodal", "basic"],
                    value="auto",
                    label="ì²˜ë¦¬ ëª¨ë“œ",
                    info="auto: ìë™ ì„ íƒ, advanced: ê³ ê¸‰ AI, multimodal: ë©€í‹°ëª¨ë‹¬, basic: ê¸°ë³¸"
                )
                reasoning_type = gr.Dropdown(
                    choices=["chain_of_thought", "deductive", "inductive", "abductive", "causal"],
                    value="chain_of_thought",
                    label="ì¶”ë¡  íƒ€ì…",
                    info="ì‚¬ê³  ê³¼ì •ì˜ ìœ í˜• ì„ íƒ"
                )
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        def respond(message: str, image, chat_history: List[Tuple[str, str]], 
                   proc_mode: str, reason_type: str):
            """ë©”ì‹œì§€ ì‘ë‹µ ì²˜ë¦¬ (ì°¨ì„¸ëŒ€ ì‹œìŠ¤í…œ)"""
            if not message.strip():
                return "", None, chat_history, proc_mode, reason_type
            
            # ì²˜ë¦¬ ëª¨ë“œ ê²°ì •
            selected_mode = None if proc_mode == "auto" else proc_mode
            selected_reasoning = None if reason_type == "chain_of_thought" else reason_type
            
            response = chat_service.process_query(
                message, chat_history, image, selected_mode, selected_reasoning
            )
            
            # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ëŒ€í™”ì— í‘œì‹œ
            if image:
                chat_history.append((f"{message}\n[ì´ë¯¸ì§€ ì²¨ë¶€ë¨]", response))
            else:
                chat_history.append((message, response))
            
            return "", None, chat_history, proc_mode, reason_type
        
        def clear_chat():
            """ëŒ€í™” ì´ˆê¸°í™”"""
            chat_service.conversation_history.clear()
            # ì„¸ì…˜ IDë„ ë¦¬ì…‹
            chat_service.current_session_id = None
            return None, "", None, "auto", "chain_of_thought"
        
        # ì´ë²¤íŠ¸ ë°”ì¸ë”©
        submit.click(
            respond, 
            [msg, image_input, chatbot, processing_mode, reasoning_type], 
            [msg, image_input, chatbot, processing_mode, reasoning_type]
        )
        msg.submit(
            respond, 
            [msg, image_input, chatbot, processing_mode, reasoning_type], 
            [msg, image_input, chatbot, processing_mode, reasoning_type]
        )
        clear.click(
            clear_chat, 
            None, 
            [chatbot, msg, image_input, processing_mode, reasoning_type]
        )
        
        # í†µê³„ í‘œì‹œ
        with gr.Accordion("ğŸ“Š ì„œë¹„ìŠ¤ í†µê³„", open=False):
            with gr.Row():
                with gr.Column():
                    stats_display = gr.Textbox(
                        label="ì„¸ì…˜ í†µê³„",
                        interactive=False,
                        lines=8
                    )
                with gr.Column():
                    system_status_display = gr.Textbox(
                        label="ì‹œìŠ¤í…œ ìƒíƒœ",
                        interactive=False,
                        lines=8
                    )
                
                def update_stats():
                    """í†µê³„ ì—…ë°ì´íŠ¸"""
                    try:
                        service_status = chat_service.get_service_status()
                        session_stats = service_status['session_stats']
                        
                        # ì„¸ì…˜ í†µê³„
                        total_queries = session_stats['queries_count']
                        success_rate = 0.0
                        avg_time = 0.0
                        
                        if total_queries > 0:
                            success_rate = (session_stats['successful_responses'] / total_queries) * 100
                            avg_time = session_stats['total_processing_time'] / total_queries
                        
                        session_info = (
                            f"ì„¸ì…˜ ID: {service_status['current_session_id'] or 'None'}\n"
                            f"ì´ ì§ˆë¬¸ ìˆ˜: {total_queries}\n"
                            f"ì„±ê³µí•œ ì‘ë‹µ: {session_stats['successful_responses']}\n"
                            f"ì‹¤íŒ¨í•œ ì‘ë‹µ: {session_stats['failed_responses']}\n"
                            f"ì„±ê³µë¥ : {success_rate:.1f}%\n"
                            f"í‰ê·  ì²˜ë¦¬ì‹œê°„: {avg_time:.2f}ì´ˆ\n"
                            f"ì´ ì²˜ë¦¬ì‹œê°„: {session_stats['total_processing_time']:.2f}ì´ˆ"
                        )
                        
                        # ì‹œìŠ¤í…œ ìƒíƒœ
                        system_info = (
                            f"ì°¨ì„¸ëŒ€ ì‹œìŠ¤í…œ: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if service_status['next_gen_available'] else 'âŒ ë¹„í™œì„±í™”'}\n"
                            f"ë ˆê±°ì‹œ ì‹œìŠ¤í…œ: {'âœ… ì‚¬ìš© ê°€ëŠ¥' if service_status['legacy_available'] else 'âŒ ë¹„í™œì„±í™”'}\n"
                        )
                        
                        # AI ì„œë¹„ìŠ¤ ìƒíƒœ ì¶”ê°€
                        if 'ai_service_status' in service_status:
                            ai_status = service_status['ai_service_status']
                            system_info += (
                                f"AI ì„œë¹„ìŠ¤ ìƒíƒœ: {ai_status['status']}\n"
                                f"ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œìŠ¤í…œ: {len(ai_status['available_systems'])}\n"
                                f"ì§€ì› ê¸°ëŠ¥: {len(ai_status['capabilities'])}\n"
                                f"í™œì„± ìš”ì²­: {ai_status['active_requests']}\n"
                                f"ìºì‹œ í¬ê¸°: {ai_status['cache_size']}"
                            )
                        
                        return session_info, system_info
                        
                    except Exception as e:
                        error_msg = f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
                        return error_msg, error_msg
                
                refresh_stats = gr.Button("ìƒˆë¡œê³ ì¹¨", size="sm")
                refresh_stats.click(update_stats, None, [stats_display, system_status_display])
                
                # ì´ˆê¸° í†µê³„ í‘œì‹œ
                app.load(update_stats, None, [stats_display, system_status_display])
    
    return app


def launch_app():
    """ì•± ì‹¤í–‰ í•¨ìˆ˜ (run_app.pyì—ì„œ í˜¸ì¶œ)"""
    # ì„¤ì • ë¡œë“œ
    config = Config()
    
    # ì•± ìƒì„± ë° ì‹¤í–‰
    try:
        app = create_gradio_app(config)
        app.launch(
            server_name=config.app.server_name,
            server_port=config.app.server_port,
            share=config.app.share,
            show_error=True,
            quiet=False
        )
    except Exception as e:
        logger.error(f"Failed to launch app: {e}")
        sys.exit(1)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="RAG System Gradio UI")
    parser.add_argument(
        "--config", 
        type=str, 
        help="Path to config file (JSON format)"
    )
    parser.add_argument(
        "--server-port", 
        type=int, 
        default=7860,
        help="Server port (default: 7860)"
    )
    parser.add_argument(
        "--share", 
        action="store_true",
        help="Create public Gradio link"
    )
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = Config()
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ ì˜¤ë²„ë¼ì´ë“œ
    if args.server_port:
        config.app.server_port = args.server_port
    if args.share:
        config.app.share = args.share
    
    # ì•± ìƒì„± ë° ì‹¤í–‰
    try:
        app = create_gradio_app(config)
        app.launch(
            server_name=config.app.server_name,
            server_port=config.app.server_port,
            share=config.app.share
        )
    except Exception as e:
        logger.error(f"Failed to launch app: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()