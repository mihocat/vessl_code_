#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG System Core Module
RAG ì‹œìŠ¤í…œ í•µì‹¬ ëª¨ë“ˆ
"""

import os
import json
import time
import logging
from typing import Dict, List, Optional
from collections import defaultdict

import numpy as np
import chromadb
from sentence_transformers import SentenceTransformer
from duckduckgo_search import DDGS

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ConcreteKoreanElectricalRAG:
    """í†µí•© RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, embedding_model_name: str = "sentence-transformers/distiluse-base-multilingual-cased"):
        """
        Args:
            embedding_model_name: í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        """
        try:
            os.environ["SAFETENSORS_FAST_GPU"] = "1"
            os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
            
            # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œë„: {embedding_model_name}")
            self.embedding_model = SentenceTransformer(embedding_model_name, trust_remote_code=True)
            logger.info(f"í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {embedding_model_name}")
            
            # ëª¨ë¸ ì •ë³´ ë¡œê¹…
            try:
                model_info = {
                    "model_name": embedding_model_name,
                    "embedding_dimension": self.embedding_model.get_sentence_embedding_dimension(),
                    "max_seq_length": getattr(self.embedding_model, 'max_seq_length', 'Unknown'),
                    "device": getattr(self.embedding_model, 'device', 'Unknown')
                }
                logger.info(f"ë¡œë“œëœ ì„ë² ë”© ëª¨ë¸ ì •ë³´: {model_info}")
            except Exception as info_e:
                logger.warning(f"ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {info_e}")
                
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - {embedding_model_name}: {str(e)}")
            logger.error(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            logger.info("í´ë°± ëª¨ë¸ë¡œ ì „í™˜ ì¤‘...")
            
            # í´ë°± ëª¨ë¸
            fallback_model = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            try:
                self.embedding_model = SentenceTransformer(fallback_model)
                logger.info(f"í´ë°± ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {fallback_model}")
                
                # í´ë°± ëª¨ë¸ ì •ë³´ ë¡œê¹…
                fallback_info = {
                    "model_name": fallback_model,
                    "embedding_dimension": self.embedding_model.get_sentence_embedding_dimension(),
                    "max_seq_length": getattr(self.embedding_model, 'max_seq_length', 'Unknown'),
                    "device": getattr(self.embedding_model, 'device', 'Unknown')
                }
                logger.info(f"í´ë°± ëª¨ë¸ ì •ë³´: {fallback_info}")
                
            except Exception as fallback_e:
                logger.error(f"í´ë°± ëª¨ë¸ ë¡œë“œë„ ì‹¤íŒ¨: {fallback_e}")
                raise RuntimeError(f"ëª¨ë“  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì›ë³¸ ì—ëŸ¬: {e}, í´ë°± ì—ëŸ¬: {fallback_e}")
        
        # ChromaDB ì´ˆê¸°í™”
        self.chroma_client = chromadb.PersistentClient(path="/tmp/chroma_db_korean_electrical")
        self.collection = self.chroma_client.get_or_create_collection(name="electrical_engineering_docs")
        
        # ë¬¸ì„œ ë° í†µê³„
        self.documents = []
        self.user_history = defaultdict(list)
        self.service_stats = {
            "total_queries": 0,
            "successful_answers": 0,
            "db_hits": 0,
            "web_searches": 0,
            "user_satisfaction": []
        }
        
        logger.info("ConcreteKoreanElectricalRAG ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸
        self._test_embedding_model()
    
    def _test_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            test_texts = ["ì•ˆë…•í•˜ì„¸ìš”", "ì „ê¸°ê³µí•™ í…ŒìŠ¤íŠ¸"]
            logger.info("ì„ë² ë”© ëª¨ë¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            
            embeddings = self.embedding_model.encode(test_texts)
            
            test_info = {
                "test_texts": test_texts,
                "embedding_shape": embeddings.shape,
                "embedding_type": type(embeddings).__name__,
                "sample_values": embeddings[0][:5].tolist() if len(embeddings[0]) >= 5 else embeddings[0].tolist()
            }
            logger.info(f"ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {test_info}")
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ì„ë² ë”© ëª¨ë¸ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {e}")
    
    def get_current_embedding_model_info(self) -> dict:
        """í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì„ë² ë”© ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        try:
            return {
                "model_class": self.embedding_model.__class__.__name__,
                "embedding_dimension": self.embedding_model.get_sentence_embedding_dimension(),
                "max_seq_length": getattr(self.embedding_model, 'max_seq_length', 'Unknown'),
                "device": str(getattr(self.embedding_model, 'device', 'Unknown')),
                "modules": [str(module) for module in getattr(self.embedding_model, '_modules', {}).keys()],
                "tokenizer_type": type(getattr(self.embedding_model, 'tokenizer', None)).__name__ if hasattr(self.embedding_model, 'tokenizer') else 'Unknown'
            }
        except Exception as e:
            return {"error": str(e)}
    
    def load_documents_from_dataset(self, dataset_path: str = "/dataset", max_docs: int = 6000):
        """ë°ì´í„°ì…‹ì—ì„œ ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„°í™”"""
        docs_count = 0
        categories = defaultdict(int)
        
        if not os.path.exists(dataset_path):
            logger.warning(f"ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {dataset_path}")
            self._load_sample_data()
            return
        
        # JSONL íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ
        for root, dirs, files in os.walk(dataset_path):
            for file in files:
                if file.endswith(".jsonl") and docs_count < max_docs:
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            for line in f:
                                if docs_count >= max_docs:
                                    break
                                try:
                                    data = json.loads(line)
                                    if "Context" in data and "Response" in data:
                                        context_part = data["Context"]
                                        response_part = data["Response"]
                                        
                                        # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
                                        category = self._categorize_document(context_part)
                                        categories[category] += 1
                                        
                                        content = f"ì§ˆë¬¸: {context_part} ë‹µë³€: {response_part} ë¶„ë¥˜: {category}"
                                        doc_item = {
                                            "id": str(docs_count),
                                            "text": content,
                                            "question": context_part,
                                            "answer": response_part,
                                            "category": category
                                        }
                                        self.documents.append(doc_item)
                                        docs_count += 1
                                except:
                                    continue
                    except:
                        continue
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ë¡œê·¸
        logger.info("ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
        for cat, count in categories.items():
            logger.info(f"- {cat}: {count}ê°œ")
        
        # ë²¡í„°í™” ë° ì €ì¥
        self._vectorize_documents()
    
    def _categorize_document(self, text: str) -> str:
        """ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ - ì „ê¸° ìê²©ì¦ ì¢…ëª©ë³„ ì„¸ë¶„í™”"""
        text_lower = text.lower()
        
        # 1. ì „ê¸° ìê²©ì¦ ì¢…ëª©ë³„ ë¶„ë¥˜ (ìš°ì„ ìˆœìœ„)
        if any(word in text_lower for word in ["ì „ê¸°ê¸°ì‚¬", "ê¸°ì‚¬ì‹œí—˜", "ê¸°ì‚¬ ì‹œí—˜", "ê¸°ì‚¬í•„ê¸°", "ê¸°ì‚¬ì‹¤ê¸°"]):
            return "ì „ê¸°ê¸°ì‚¬"
        elif any(word in text_lower for word in ["ì „ê¸°ì‚°ì—…ê¸°ì‚¬", "ì‚°ì—…ê¸°ì‚¬ì‹œí—˜", "ì‚°ì—…ê¸°ì‚¬ ì‹œí—˜", "ì‚°ì—…ê¸°ì‚¬í•„ê¸°", "ì‚°ì—…ê¸°ì‚¬ì‹¤ê¸°"]):
            return "ì „ê¸°ì‚°ì—…ê¸°ì‚¬"
        elif any(word in text_lower for word in ["ì „ê¸°ê¸°ëŠ¥ì‚¬", "ê¸°ëŠ¥ì‚¬ì‹œí—˜", "ê¸°ëŠ¥ì‚¬ ì‹œí—˜", "ê¸°ëŠ¥ì‚¬í•„ê¸°", "ê¸°ëŠ¥ì‚¬ì‹¤ê¸°"]):
            return "ì „ê¸°ê¸°ëŠ¥ì‚¬"
        elif any(word in text_lower for word in ["ì „ê¸°ê³µì‚¬ê¸°ì‚¬", "ê³µì‚¬ê¸°ì‚¬"]):
            return "ì „ê¸°ê³µì‚¬ê¸°ì‚¬"
        elif any(word in text_lower for word in ["ì „ê¸°ê³µì‚¬ì‚°ì—…ê¸°ì‚¬", "ê³µì‚¬ì‚°ì—…ê¸°ì‚¬"]):
            return "ì „ê¸°ê³µì‚¬ì‚°ì—…ê¸°ì‚¬"
        
        # 2. ê¸°ë³¸ ì „ê¸°ê³µí•™ ì´ë¡  ë¶„ì•¼
        elif any(word in text_lower for word in ["ì˜´ì˜ë²•ì¹™", "í‚¤ë¥´íˆí˜¸í”„", "ì „ìê¸°í•™", "ë§¥ìŠ¤ì›°", "ì¿¨ë¡±", "íŒ¨ëŸ¬ë°ì´", "ë Œì¸ "]):
            return "ê¸°ì´ˆì´ë¡ "
        elif any(word in text_lower for word in ["íšŒë¡œì´ë¡ ", "íšŒë¡œí•´ì„", "êµë¥˜íšŒë¡œ", "ì§ë¥˜íšŒë¡œ", "rlcíšŒë¡œ", "ê³µì§„íšŒë¡œ"]):
            return "íšŒë¡œì´ë¡ "
        
        # 3. ì „ê¸°ê¸°ê¸° ë¶„ì•¼  
        elif any(word in text_lower for word in ["ë³€ì••ê¸°", "íŠ¸ëœìŠ¤í¬ë¨¸", "ì „ë ¥ë³€ì••ê¸°", "ë°°ì „ìš©ë³€ì••ê¸°"]):
            return "ë³€ì••ê¸°"
        elif any(word in text_lower for word in ["ìœ ë„ì „ë™ê¸°", "ë™ê¸°ì „ë™ê¸°", "ì§ë¥˜ì „ë™ê¸°", "ëª¨í„°", "ì „ë™ê¸°"]):
            return "ì „ë™ê¸°"
        elif any(word in text_lower for word in ["ë°œì „ê¸°", "ë™ê¸°ë°œì „ê¸°", "ìœ ë„ë°œì „ê¸°", "ì§ë¥˜ë°œì „ê¸°"]):
            return "ë°œì „ê¸°"
        
        # 4. ì „ë ¥ê³µí•™ ë¶„ì•¼
        elif any(word in text_lower for word in ["ì†¡ì „", "ì†¡ì „ì„ ë¡œ", "ì†¡ì „ê³„í†µ", "ê³ ì••ì†¡ì „"]):
            return "ì†¡ì „ê³µí•™"
        elif any(word in text_lower for word in ["ë°°ì „", "ë°°ì „ì„ ë¡œ", "ë°°ì „ê³„í†µ", "ë°°ì „ìš©ë³€ì••ê¸°"]):
            return "ë°°ì „ê³µí•™"  
        elif any(word in text_lower for word in ["ì „ë ¥ê³„í†µ", "ê³„í†µìš´ìš©", "ì „ë ¥í’ˆì§ˆ", "ì•ˆì •ë„", "ì¡°ìƒì„¤ë¹„"]):
            return "ì „ë ¥ê³„í†µ"
        elif any(word in text_lower for word in ["ë³´í˜¸ê³„ì „", "ê³„ì „ê¸°", "ì°¨ë‹¨ê¸°", "ê°œíê¸°", "í”¼ë¢°ê¸°"]):
            return "ë³´í˜¸ì œì–´"
        
        # 5. ì „ê¸°ì„¤ë¹„ ë° ì‹œê³µ ë¶„ì•¼
        elif any(word in text_lower for word in ["ì „ê¸°ì„¤ë¹„", "ìˆ˜ë³€ì „ì„¤ë¹„", "ë°°ì „ë°˜", "ë¶„ì „ë°˜"]):
            return "ì „ê¸°ì„¤ë¹„"
        elif any(word in text_lower for word in ["ì „ê¸°ê³µì‚¬", "ë°°ì„ ê³µì‚¬", "ì¼€ì´ë¸”", "ì „ì„ ", "ë„ê´€"]):
            return "ì „ê¸°ê³µì‚¬"
        elif any(word in text_lower for word in ["ì ‘ì§€", "í”¼ë¢°", "ì „ê¸°ì•ˆì „", "ê°ì „", "ëˆ„ì „"]):
            return "ì „ê¸°ì•ˆì „"
        
        # 6. ì‹ ì¬ìƒì—ë„ˆì§€ ë° ìµœì‹ ê¸°ìˆ 
        elif any(word in text_lower for word in ["íƒœì–‘ê´‘", "í’ë ¥", "ì—°ë£Œì „ì§€", "íƒœì–‘ì „ì§€", "ì‹ ì¬ìƒì—ë„ˆì§€"]):
            return "ì‹ ì¬ìƒì—ë„ˆì§€"
        elif any(word in text_lower for word in ["ì „ê¸°ìë™ì°¨", "evì¶©ì „", "ë°°í„°ë¦¬", "ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ"]):
            return "ìµœì‹ ê¸°ìˆ "
        
        # 7. ì „ìê³µí•™ ê´€ë ¨
        elif any(word in text_lower for word in ["ë°˜ë„ì²´", "ë‹¤ì´ì˜¤ë“œ", "íŠ¸ëœì§€ìŠ¤í„°", "ic", "ì¦í­ê¸°"]):
            return "ì „ìê³µí•™"
        elif any(word in text_lower for word in ["ì œì–´ê³µí•™", "ìë™ì œì–´", "pidì œì–´", "ëª¨í„°ì œì–´"]):
            return "ì œì–´ê³µí•™"
        
        # 8. ê¸°íƒ€ ì¼ë°˜
        else:
            return "ê¸°íƒ€"
    
    def _load_sample_data(self):
        """ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ"""
        sample_docs = [
            {
                "question": "ì˜´ì˜ ë²•ì¹™ì´ ë¬´ì—‡ì¸ê°€ìš”?",
                "answer": "ì˜´ì˜ ë²•ì¹™ì€ ì „ì••(V) = ì „ë¥˜(I) Ã— ì €í•­(R)ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì „ê¸°ê³µí•™ì˜ ê¸°ë³¸ ë²•ì¹™ì…ë‹ˆë‹¤.",
                "category": "ê¸°ì´ˆì´ë¡ "
            },
            {
                "question": "êµë¥˜ì™€ ì§ë¥˜ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "answer": "ì§ë¥˜(DC)ëŠ” ì „ë¥˜ê°€ í•œ ë°©í–¥ìœ¼ë¡œë§Œ íë¥´ë©°, êµë¥˜(AC)ëŠ” ì „ë¥˜ì˜ ë°©í–¥ì´ ì£¼ê¸°ì ìœ¼ë¡œ ë°”ë€ë‹ˆë‹¤.",
                "category": "íšŒë¡œì´ë¡ "
            },
            {
                "question": "ë³€ì••ê¸°ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
                "answer": "ë³€ì••ê¸°ëŠ” íŒ¨ëŸ¬ë°ì´ì˜ ì „ìê¸°ìœ ë„ ë²•ì¹™ì„ ì´ìš©í•˜ì—¬ ê¶Œìˆ˜ë¹„ì— ë”°ë¼ ì „ì••ì„ ë³€í™˜í•©ë‹ˆë‹¤.",
                "category": "ë³€ì••ê¸°"
            },
            {
                "question": "ì „ê¸°ê¸°ì‚¬ ì‹œí—˜ì€ ì–´ë–»ê²Œ ì¤€ë¹„í•˜ë‚˜ìš”?",
                "answer": "ì „ê¸°ê¸°ì‚¬ ì‹œí—˜ì€ í•„ê¸°ì™€ ì‹¤ê¸°ë¡œ êµ¬ì„±ë˜ë©°, ê¸°ë³¸ì„œ í•™ìŠµ í›„ ê¸°ì¶œë¬¸ì œë¥¼ ë°˜ë³µ í’€ì´í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.",
                "category": "ì „ê¸°ê¸°ì‚¬"
            },
            {
                "question": "ìœ ë„ì „ë™ê¸°ì˜ ë™ì‘ ì›ë¦¬ëŠ”?",
                "answer": "ìœ ë„ì „ë™ê¸°ëŠ” íšŒì „ìê¸°ì¥ì— ì˜í•´ íšŒì „ìê°€ íšŒì „í•˜ëŠ” ì›ë¦¬ë¡œ ë™ì‘í•˜ë©°, ìŠ¬ë¦½ì— ë”°ë¼ í† í¬ê°€ ê²°ì •ë©ë‹ˆë‹¤.",
                "category": "ì „ë™ê¸°"
            },
            {
                "question": "ì†¡ì „ì„ ë¡œì˜ íŠ¹ì„± ì„í”¼ë˜ìŠ¤ëŠ”?",
                "answer": "ì†¡ì „ì„ ë¡œì˜ íŠ¹ì„± ì„í”¼ë˜ìŠ¤ëŠ” âˆš(L/C)ë¡œ ê³„ì‚°ë˜ë©°, ì¼ë°˜ì ìœ¼ë¡œ 400-500Î© ë²”ìœ„ì…ë‹ˆë‹¤.",
                "category": "ì†¡ì „ê³µí•™"
            }
        ]
        
        self.documents = []
        for i, doc_data in enumerate(sample_docs):
            content = f"ì§ˆë¬¸: {doc_data['question']} ë‹µë³€: {doc_data['answer']} ë¶„ë¥˜: {doc_data['category']}"
            self.documents.append({
                "id": str(i),
                "text": content,
                "question": doc_data["question"],
                "answer": doc_data["answer"],
                "category": doc_data["category"]
            })
        
        self._vectorize_documents()
    
    def _vectorize_documents(self):
        """ë¬¸ì„œ ë²¡í„°í™” ë° ChromaDB ì €ì¥"""
        if not self.documents:
            logger.warning("ë²¡í„°í™”í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        texts = [doc["text"] for doc in self.documents]
        logger.info(f"í•œêµ­ì–´ ë²¡í„° ì„ë² ë”© ì‹œì‘: {len(texts)}ê°œ ì „ê¸°ê³µí•™ ë¬¸ì„œ")
        
        batch_size = 50
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_ids = [self.documents[i+j]["id"] for j in range(len(batch_texts))]
            
            embeddings = self.embedding_model.encode(batch_texts, convert_to_tensor=True)
            embeddings_np = embeddings.cpu().numpy()
            
            self.collection.add(
                embeddings=embeddings_np.tolist(),
                documents=batch_texts,
                ids=batch_ids
            )
            
            logger.info(f"ë°°ì¹˜ {i//batch_size + 1} ì™„ë£Œ: {len(batch_texts)}ê°œ")
        
        logger.info(f"ì „ê¸°ê³µí•™ ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")
    
    def search_vector_database(self, query: str, k: int = 5) -> tuple:
        """ê³ ë„í™”ëœ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
        try:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬ ë° í™•ì¥
            enhanced_queries = self._expand_query(query)
            query_category = self._advanced_categorize_document(query)
            
            best_results = []
            
            # ë‹¤ì¤‘ ì¿¼ë¦¬ ì „ëµ
            for enhanced_query in enhanced_queries:
                query_embedding = self.embedding_model.encode([enhanced_query], convert_to_tensor=True)
                query_embedding_np = query_embedding.cpu().numpy()
                
                results = self.collection.query(
                    query_embeddings=query_embedding_np.tolist(),
                    n_results=k * 3,  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ ê³ í’ˆì§ˆ í•„í„°ë§
                    include=['distances', 'documents', 'metadatas']
                )
                
                if results["documents"] and len(results["documents"]) > 0:
                    documents = results["documents"][0]
                    distances = results["distances"][0]
                    ids = results["ids"][0]
                    
                    # ê³ ë„í™”ëœ ì ìˆ˜ ê³„ì‚°
                    for doc, distance, doc_id in zip(documents, distances, ids):
                        doc_info = self._get_document_info(doc_id)
                        if not doc_info:
                            continue
                            
                        # ë‹¤ì¤‘ ì§€í‘œ ê¸°ë°˜ ì ìˆ˜
                        scores = self._calculate_multi_score(query, enhanced_query, doc, doc_info, distance, query_category)
                        
                        # ê³ í’ˆì§ˆ ê²°ê³¼ë§Œ ì„ ë³„
                        if scores['final_score'] > 0.65:  # ë” ì—„ê²©í•œ ì„ê³„ê°’
                            best_results.append({
                                "content": doc,
                                "doc_info": doc_info,
                                "scores": scores,
                                "similarity": scores['cosine_similarity'],
                                "final_score": scores['final_score'],
                                "query_type": enhanced_query
                            })
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_results = self._deduplicate_results(best_results)
            unique_results.sort(key=lambda x: x["final_score"], reverse=True)
            
            if unique_results:
                self.service_stats["db_hits"] += 1
                return unique_results[:k], True
            
            return [], False
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return [], False
    
    def _expand_query(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ í™•ì¥ ë° ë‹¤ì¤‘ ë²„ì „ ìƒì„±"""
        queries = [query]  # ì›ë³¸ ì¿¼ë¦¬
        
        # ì „ê¸°ê³µí•™ ë™ì˜ì–´ í™•ì¥
        synonyms = {
            'ì „ì••': ['ë³¼íŠ¸', 'V', 'ì „ìœ„ì°¨'],
            'ì „ë¥˜': ['ì•”í˜ì–´', 'A', 'ì „ë¥˜ê°’'],
            'ì €í•­': ['ì˜¤ì˜ˆ', 'Î©', 'R'],
            'ë³€ì••ê¸°': ['íŠ¸ëœìŠ¤í¬ë¨¸', 'ì „ë ¥ë³€ì••ê¸°'],
            'ëª¨í„°': ['ì „ë™ê¸°', 'ìœ ë„ì „ë™ê¸°'],
            'ë°œì „ê¸°': ['ë™ê¸°', 'ì œë„ˆë ˆì´í„°']
        }
        
        for word, alternatives in synonyms.items():
            if word in query:
                for alt in alternatives:
                    queries.append(query.replace(word, alt))
        
        # ê¸°ìˆ ì  ë§¥ë½ ì¶”ê°€
        if any(term in query for term in ['ê³„ì‚°', 'êµ¬í•˜ê¸°', 'ë°©ë²•']):
            queries.append(f"{query} ê³µì‹ ë‹¨ê³„")
            queries.append(f"{query} í•´ê²° ë°©ë²•")
        
        return queries[:3]  # ìµœëŒ€ 3ê°œë¡œ ì œí•œ
    
    def _advanced_categorize_document(self, text: str) -> str:
        """ê³ ë„í™”ëœ ë¬¸ì„œ ë¶„ë¥˜ - í™•ì¥ëœ ì¹´í…Œê³ ë¦¬ ì§€ì›"""
        category_keywords = {
            # ìê²©ì¦ ì¢…ëª©ë³„
            'ì „ê¸°ê¸°ì‚¬': ['ì „ê¸°ê¸°ì‚¬', 'ê¸°ì‚¬ì‹œí—˜', 'ê¸°ì‚¬í•„ê¸°', 'ê¸°ì‚¬ì‹¤ê¸°'],
            'ì „ê¸°ì‚°ì—…ê¸°ì‚¬': ['ì „ê¸°ì‚°ì—…ê¸°ì‚¬', 'ì‚°ì—…ê¸°ì‚¬ì‹œí—˜', 'ì‚°ì—…ê¸°ì‚¬í•„ê¸°', 'ì‚°ì—…ê¸°ì‚¬ì‹¤ê¸°'],
            'ì „ê¸°ê¸°ëŠ¥ì‚¬': ['ì „ê¸°ê¸°ëŠ¥ì‚¬', 'ê¸°ëŠ¥ì‚¬ì‹œí—˜', 'ê¸°ëŠ¥ì‚¬í•„ê¸°', 'ê¸°ëŠ¥ì‚¬ì‹¤ê¸°'],
            'ì „ê¸°ê³µì‚¬ê¸°ì‚¬': ['ì „ê¸°ê³µì‚¬ê¸°ì‚¬', 'ê³µì‚¬ê¸°ì‚¬'],
            
            # ì´ë¡  ë¶„ì•¼ë³„
            'ê¸°ì´ˆì´ë¡ ': ['ì˜´ì˜ë²•ì¹™', 'í‚¤ë¥´íˆí˜¸í”„', 'ì „ìê¸°í•™', 'ë§¥ìŠ¤ì›°', 'ì¿¨ë¡±', 'íŒ¨ëŸ¬ë°ì´'],
            'íšŒë¡œì´ë¡ ': ['íšŒë¡œì´ë¡ ', 'íšŒë¡œí•´ì„', 'êµë¥˜íšŒë¡œ', 'ì§ë¥˜íšŒë¡œ', 'rlcíšŒë¡œ'],
            
            # ê¸°ê¸°ë³„
            'ë³€ì••ê¸°': ['ë³€ì••ê¸°', 'íŠ¸ëœìŠ¤í¬ë¨¸', 'ì „ë ¥ë³€ì••ê¸°'],
            'ì „ë™ê¸°': ['ìœ ë„ì „ë™ê¸°', 'ë™ê¸°ì „ë™ê¸°', 'ì§ë¥˜ì „ë™ê¸°', 'ëª¨í„°'],
            'ë°œì „ê¸°': ['ë°œì „ê¸°', 'ë™ê¸°ë°œì „ê¸°', 'ìœ ë„ë°œì „ê¸°'],
            
            # ì‹œìŠ¤í…œë³„
            'ì†¡ì „ê³µí•™': ['ì†¡ì „', 'ì†¡ì „ì„ ë¡œ', 'ì†¡ì „ê³„í†µ'],
            'ë°°ì „ê³µí•™': ['ë°°ì „', 'ë°°ì „ì„ ë¡œ', 'ë°°ì „ê³„í†µ'],
            'ì „ë ¥ê³„í†µ': ['ì „ë ¥ê³„í†µ', 'ê³„í†µìš´ìš©', 'ì „ë ¥í’ˆì§ˆ', 'ì•ˆì •ë„'],
            'ë³´í˜¸ì œì–´': ['ë³´í˜¸ê³„ì „', 'ê³„ì „ê¸°', 'ì°¨ë‹¨ê¸°'],
            
            # ì„¤ë¹„/ê³µì‚¬ë³„  
            'ì „ê¸°ì„¤ë¹„': ['ì „ê¸°ì„¤ë¹„', 'ìˆ˜ë³€ì „ì„¤ë¹„', 'ë°°ì „ë°˜'],
            'ì „ê¸°ê³µì‚¬': ['ì „ê¸°ê³µì‚¬', 'ë°°ì„ ê³µì‚¬', 'ì¼€ì´ë¸”'],
            'ì „ê¸°ì•ˆì „': ['ì ‘ì§€', 'í”¼ë¢°', 'ì „ê¸°ì•ˆì „', 'ê°ì „'],
            
            # ì‹ ê¸°ìˆ ë³„
            'ì‹ ì¬ìƒì—ë„ˆì§€': ['íƒœì–‘ê´‘', 'í’ë ¥', 'ì—°ë£Œì „ì§€', 'ì‹ ì¬ìƒì—ë„ˆì§€'],
            'ìµœì‹ ê¸°ìˆ ': ['ì „ê¸°ìë™ì°¨', 'evì¶©ì „', 'ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ'],
            'ì „ìê³µí•™': ['ë°˜ë„ì²´', 'ë‹¤ì´ì˜¤ë“œ', 'íŠ¸ëœì§€ìŠ¤í„°'],
            'ì œì–´ê³µí•™': ['ì œì–´ê³µí•™', 'ìë™ì œì–´', 'pidì œì–´']
        }
        
        text_lower = text.lower()
        scores = {}
        
        for category, keywords in category_keywords.items():
            score = sum(1 for keyword in keywords if keyword.lower() in text_lower)
            if score > 0:
                scores[category] = score
        
        return max(scores.items(), key=lambda x: x[1])[0] if scores else 'ê¸°íƒ€'
    
    def _get_document_info(self, doc_id: str) -> Optional[Dict]:
        """ë¬¸ì„œ ì •ë³´ ê³ ì† ê²€ìƒ‰"""
        for doc in self.documents:
            if doc["id"] == doc_id:
                return doc
        return None
    
    def _calculate_multi_score(self, original_query: str, enhanced_query: str, doc: str, doc_info: Dict, distance: float, query_category: str) -> Dict:
        """ë‹¤ì¤‘ ì§€í‘œ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°"""
        cosine_similarity = 1 - distance
        
        # 1. ì˜ë¯¸ì  ìœ ì‚¬ë„
        semantic_score = cosine_similarity
        
        # 2. ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ë„
        category_score = 0.15 if doc_info["category"] == query_category else 0
        
        # 3. í‚¤ì›Œë“œ ë§¤ì¹­ (ì •ë°€ ê²€ìƒ‰)
        original_words = set(original_query.split())
        doc_words = set(doc.lower().split())
        keyword_overlap = len(original_words.intersection(doc_words)) / max(len(original_words), 1)
        keyword_score = keyword_overlap * 0.2
        
        # 4. ë¬¸ì„œ í’ˆì§ˆ ì ìˆ˜
        quality_score = 0.1 if len(doc_info.get("answer", "")) > 50 else 0
        
        # 5. ê¸¸ì´ ê¸°ë°˜ ì •ê·œí™”
        length_penalty = 0.05 if len(doc) > 1000 else 0  # ë„ˆë¬´ ê¸´ ë¬¸ì„œ í˜ë„í‹°
        
        final_score = semantic_score + category_score + keyword_score + quality_score - length_penalty
        
        return {
            'cosine_similarity': cosine_similarity,
            'semantic_score': semantic_score,
            'category_score': category_score,
            'keyword_score': keyword_score,
            'quality_score': quality_score,
            'final_score': min(final_score, 1.0)  # 1.0 ìµœëŒ€ê°’ ì œí•œ
        }
    
    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ê²°ê³¼ ì œê±°"""
        seen_ids = set()
        unique_results = []
        
        for result in results:
            doc_id = result["doc_info"]["id"]
            if doc_id not in seen_ids:
                seen_ids.add(doc_id)
                unique_results.append(result)
        
        return unique_results
    
    def search_web(self, query: str, max_results: int = 3) -> List[Dict]:
        """ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
        try:
            with DDGS() as ddgs:
                # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ
                search_queries = [
                    f"ì „ê¸°ê³µí•™ {query}",
                    f"{query} í•´ì„¤",
                    f"{query} ê¸°ì´ˆ ì´ë¡ ",
                    query  # ì›ë³¸ ì¿¼ë¦¬
                ]
                
                all_results = []
                
                for search_query in search_queries[:2]:  # ìƒìœ„ 2ê°œ ì „ëµë§Œ ì‚¬ìš©
                    try:
                        web_results = list(ddgs.text(search_query, region="ko-kr", max_results=max_results))
                
                        for result in web_results:
                            all_results.append(result)
                    except:
                        continue
                
                # ê²°ê³¼ ì²˜ë¦¬ ë° í•„í„°ë§
                processed_results = []
                seen_urls = set()
                
                for result in all_results:
                    url = result.get("href", "")
                    if url in seen_urls:  # ì¤‘ë³µ URL ì œê±°
                        continue
                    seen_urls.add(url)
                    
                    title = result.get("title", "")
                    body = result.get("body", "")
                    
                    # ì „ê¸°ê³µí•™ ê´€ë ¨ì„± ê²€ì¦
                    relevance_score = self._calculate_web_relevance(query, title, body)
                    if relevance_score < 0.3:  # ê´€ë ¨ì„± ë‚®ì€ ê²°ê³¼ ì œì™¸
                        continue
                    
                    # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
                    trust_domains = [".edu", ".ac.kr", ".go.kr", "kea.kr", "kstec.or.kr", "keit.re.kr"]
                    trust_score = 2.0 if any(domain in url for domain in trust_domains) else 1.0
                    
                    # ìµœì¢… ì ìˆ˜
                    final_score = relevance_score * trust_score
                    
                    processed_results.append({
                        "title": title[:100],  # ì œëª© ê¸¸ì´ ì œí•œ
                        "snippet": body[:300],  # ë‚´ìš© ê¸¸ì´ ì¦ê°€
                        "url": url,
                        "trust_score": trust_score,
                        "relevance_score": relevance_score,
                        "final_score": final_score
                    })
                
                # ìµœì¢… ì ìˆ˜ìˆœ ì •ë ¬
                processed_results.sort(key=lambda x: x["final_score"], reverse=True)
                
                # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
                top_results = processed_results[:max_results]
                if top_results:
                    self.service_stats["web_searches"] += 1
                
                return top_results
        except Exception as e:
            logger.error(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _calculate_web_relevance(self, query: str, title: str, body: str) -> float:
        """ì›¹ ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± ê³„ì‚°"""
        try:
            # í™•ì¥ëœ ì „ê¸°ê³µí•™ í•µì‹¬ í‚¤ì›Œë“œ
            electrical_keywords = [
                # ê¸°ë³¸ ì „ê¸° ê°œë…
                'ì „ê¸°', 'ì „ë ¥', 'ì „ì••', 'ì „ë¥˜', 'ì €í•­', 'íšŒë¡œ', 'ì„í”¼ë˜ìŠ¤', 'ì¸ë•í„´ìŠ¤', 'ì»¤íŒ¨ì‹œí„´ìŠ¤',
                # ì „ê¸°ê¸°ê¸°
                'ë³€ì••ê¸°', 'ëª¨í„°', 'ë°œì „ê¸°', 'ì „ë™ê¸°', 'ë™ê¸°ê¸°', 'ìœ ë„ê¸°', 'ì§ë¥˜ê¸°',
                # ì „ë ¥ì‹œìŠ¤í…œ
                'ì†¡ì „', 'ë°°ì „', 'ì „ë ¥ê³„í†µ', 'ìˆ˜ë³€ì „', 'ë³´í˜¸ê³„ì „', 'ì°¨ë‹¨ê¸°', 'ê°œíê¸°',
                # ì „ê¸°ì„¤ë¹„/ê³µì‚¬
                'ì „ê¸°ì„¤ë¹„', 'ë°°ì„ ', 'ì¼€ì´ë¸”', 'ì „ì„ ', 'ì ‘ì§€', 'í”¼ë¢°', 'ë¶„ì „ë°˜', 'ë°°ì „ë°˜',
                # ì œì–´/ì „ì
                'ì œì–´', 'ìë™ì œì–´', 'pid', 'ë°˜ë„ì²´', 'ë‹¤ì´ì˜¤ë“œ', 'íŠ¸ëœì§€ìŠ¤í„°', 'ic',
                # ì‹ ê¸°ìˆ 
                'íƒœì–‘ê´‘', 'í’ë ¥', 'ì‹ ì¬ìƒ', 'ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ', 'ì „ê¸°ìë™ì°¨', 'evì¶©ì „',
                # ë‹¨ìœ„/ì¸¡ì •
                'ì™€íŠ¸', 'ì•”í˜ì–´', 'ë³¼íŠ¸', 'ì˜´', 'í—¤ë¥´ì¸ ', 'kw', 'kv', 'a', 'v', 'hz',
                # ìê²©ì¦/ì‹œí—˜
                'ì „ê¸°ê¸°ì‚¬', 'ì „ê¸°ì‚°ì—…ê¸°ì‚¬', 'ì „ê¸°ê¸°ëŠ¥ì‚¬', 'ì „ê¸°ê³µì‚¬ê¸°ì‚¬', 'ê¸°ì‚¬', 'ì‚°ì—…ê¸°ì‚¬', 'ê¸°ëŠ¥ì‚¬', 'ìê²©ì¦', 'ì‹œí—˜', 'í•„ê¸°', 'ì‹¤ê¸°'
            ]
            
            combined_text = f"{title} {body}".lower()
            query_lower = query.lower()
            
            # 1. ì§ì ‘ ì¿¼ë¦¬ ë§¤ì¹­
            query_match = 0.5 if query_lower in combined_text else 0
            
            # 2. ì „ê¸°ê³µí•™ í‚¤ì›Œë“œ ë§¤ì¹­
            keyword_matches = sum(1 for keyword in electrical_keywords if keyword in combined_text)
            keyword_score = min(keyword_matches * 0.1, 0.4)
            
            # 3. ì œëª© ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜
            title_bonus = 0.2 if any(keyword in title.lower() for keyword in electrical_keywords) else 0
            
            # 4. ë¬¸ì„œ í’ˆì§ˆ í‰ê°€
            quality_score = 0.1 if len(body) > 100 else 0  # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆìŒ
            
            total_score = query_match + keyword_score + title_bonus + quality_score
            return min(total_score, 1.0)
            
        except:
            return 0.3  # ê¸°ë³¸ê°’
    
    def check_electrical_relevance(self, query: str) -> bool:
        """í™•ì¥ëœ ì „ê¸°ê³µí•™ ê´€ë ¨ì„± í™•ì¸"""
        electrical_keywords = [
            # ê¸°ë³¸ ì „ê¸° ê°œë…
            "ì „ê¸°", "ì „ë ¥", "ì „ì••", "ì „ë¥˜", "ì €í•­", "íšŒë¡œ", "ì„í”¼ë˜ìŠ¤", "ì¸ë•í„´ìŠ¤", "ì»¤íŒ¨ì‹œí„´ìŠ¤",
            "êµë¥˜", "ì§ë¥˜", "AC", "DC", "ì£¼íŒŒìˆ˜", "ìœ„ìƒ", "ì—­ë¥ ", "ì „ë ¥ì¸ìˆ˜",
            
            # ì „ê¸°ê¸°ê¸°
            "ë³€ì••ê¸°", "íŠ¸ëœìŠ¤í¬ë¨¸", "ëª¨í„°", "ì „ë™ê¸°", "ë°œì „ê¸°", "ë™ê¸°ê¸°", "ìœ ë„ê¸°", "ì§ë¥˜ê¸°",
            "ë‹¨ìƒ", "ì‚¼ìƒ", "ê¶Œì„ ", "ì² ì‹¬", "ìì†", "í† í¬", "ìŠ¬ë¦½", "íšŒì „ìˆ˜",
            
            # ì „ë ¥ì‹œìŠ¤í…œ  
            "ì†¡ì „", "ë°°ì „", "ì „ë ¥ê³„í†µ", "ìˆ˜ë³€ì „", "ë³€ì „ì†Œ", "ë³´í˜¸ê³„ì „", "ì°¨ë‹¨ê¸°", "ê°œíê¸°",
            "ì•ˆì •ë„", "ì¡°ìƒì„¤ë¹„", "ë¬´íš¨ì „ë ¥", "ì „ë ¥í’ˆì§ˆ", "ê³ ì¡°íŒŒ", "í”Œë¦¬ì»¤",
            
            # ì „ê¸°ì„¤ë¹„/ê³µì‚¬
            "ì „ê¸°ì„¤ë¹„", "ìˆ˜ë³€ì „ì„¤ë¹„", "ë°°ì„ ", "ì¼€ì´ë¸”", "ì „ì„ ", "ë„ê´€", "ë•íŠ¸",
            "ì ‘ì§€", "í”¼ë¢°", "ëˆ„ì „", "ê°ì „", "ë¶„ì „ë°˜", "ë°°ì „ë°˜", "ì œì–´ë°˜",
            
            # ì œì–´/ì „ì
            "ì œì–´", "ìë™ì œì–´", "pidì œì–´", "ì‹œí€€ìŠ¤ì œì–´", "í”„ë¡œê·¸ë˜ë¨¸ë¸”ë¡œì§ì»¨íŠ¸ë¡¤ëŸ¬", "plc",
            "ë°˜ë„ì²´", "ë‹¤ì´ì˜¤ë“œ", "íŠ¸ëœì§€ìŠ¤í„°", "thyristor", "ic", "ì¦í­ê¸°", "ì¸ë²„í„°",
            
            # ì‹ ê¸°ìˆ /ì—ë„ˆì§€
            "íƒœì–‘ê´‘", "íƒœì–‘ì „ì§€", "í’ë ¥", "ì—°ë£Œì „ì§€", "ì‹ ì¬ìƒì—ë„ˆì§€", "esg",
            "ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ", "ì „ê¸°ìë™ì°¨", "evì¶©ì „", "ë°°í„°ë¦¬", "ess", "ë§ˆì´í¬ë¡œê·¸ë¦¬ë“œ",
            
            # ë‹¨ìœ„/ì¸¡ì •
            "ì™€íŠ¸", "ì•”í˜ì–´", "ë³¼íŠ¸", "ì˜´", "í—¤ë¥´ì¸ ", "ë°”", "var", "va",
            "kw", "kv", "ka", "mw", "gw", "kva", "mva", "kwh", "mwh",
            "a", "v", "w", "hz", "Ï‰", "Â°", "Ï†", "cosÏ†",
            
            # ìê²©ì¦/ì‹œí—˜/êµìœ¡
            "ì „ê¸°ê¸°ì‚¬", "ì „ê¸°ì‚°ì—…ê¸°ì‚¬", "ì „ê¸°ê¸°ëŠ¥ì‚¬", "ì „ê¸°ê³µì‚¬ê¸°ì‚¬", "ì „ê¸°ê³µì‚¬ì‚°ì—…ê¸°ì‚¬",
            "ê¸°ì‚¬", "ì‚°ì—…ê¸°ì‚¬", "ê¸°ëŠ¥ì‚¬", "ìê²©ì¦", "ì‹œí—˜", "í•„ê¸°", "ì‹¤ê¸°", "ê¸°ì¶œë¬¸ì œ",
            "ì „ê¸°ê³µí•™", "ì „ë ¥ê³µí•™", "ì „ê¸°ê¸°ê¸°", "íšŒë¡œì´ë¡ ", "ì œì–´ê³µí•™", "ì „ìê³µí•™"
        ]
        
        query_lower = query.lower()
        return any(keyword.lower() in query_lower for keyword in electrical_keywords)
    
    def get_service_statistics(self) -> str:
        """ì„œë¹„ìŠ¤ í†µê³„ ì œê³µ"""
        stats = []
        stats.append("ğŸ“Š **ì „ê¸°ê³µí•™ í†µí•© ì„œë¹„ìŠ¤ í†µê³„**\n")
        stats.append(f"â€¢ ì´ ì§ˆì˜: {self.service_stats['total_queries']}ê±´")
        stats.append(f"â€¢ ì„±ê³µ ë‹µë³€: {self.service_stats['successful_answers']}ê±´")
        stats.append(f"â€¢ DB ì ì¤‘: {self.service_stats['db_hits']}ê±´")
        stats.append(f"â€¢ ì›¹ ê²€ìƒ‰: {self.service_stats['web_searches']}ê±´")
        
        if self.service_stats["total_queries"] > 0:
            success_rate = self.service_stats["successful_answers"] / self.service_stats["total_queries"] * 100
            db_hit_rate = self.service_stats["db_hits"] / self.service_stats["total_queries"] * 100
            stats.append(f"\nâ€¢ ì‘ë‹µ ì„±ê³µë¥ : {round(success_rate, 1)}%")
            stats.append(f"â€¢ DB í™œìš©ë¥ : {round(db_hit_rate, 1)}%")
        
        stats.append(f"\nâ€¢ ì§€ì‹ë² ì´ìŠ¤: {len(self.documents)}ê°œ ë¬¸ì„œ")
        stats.append(f"â€¢ í™œì„± ì‚¬ìš©ì: {len(self.user_history)}ëª…")
        
        # ì„ë² ë”© ëª¨ë¸ ì •ë³´ ì¶”ê°€
        stats.append("\n**ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´:**")
        model_info = self.get_current_embedding_model_info()
        if "error" not in model_info:
            stats.append(f"â€¢ ì„ë² ë”© ëª¨ë¸: {model_info.get('model_class', 'Unknown')}")
            stats.append(f"â€¢ ì„ë² ë”© ì°¨ì›: {model_info.get('embedding_dimension', 'Unknown')}")
            stats.append(f"â€¢ ìµœëŒ€ ì‹œí€€ìŠ¤: {model_info.get('max_seq_length', 'Unknown')}")
            stats.append(f"â€¢ ë””ë°”ì´ìŠ¤: {model_info.get('device', 'Unknown')}")
            if model_info.get('tokenizer_type') != 'Unknown':
                stats.append(f"â€¢ í† í¬ë‚˜ì´ì €: {model_info.get('tokenizer_type', 'Unknown')}")
        else:
            stats.append(f"â€¢ ëª¨ë¸ ì •ë³´ ì˜¤ë¥˜: {model_info['error']}")
        
        return "\n".join(stats)
    
    def enhanced_search_pipeline(self, query: str) -> tuple:
        """í–¥ìƒëœ í†µí•© ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ - í™•ì¥ëœ ë²”ìœ„ ì§€ì›"""
        # 1. ì „ê¸°ê³µí•™ ê´€ë ¨ì„± í™•ì¸ (í™•ì¥ëœ í‚¤ì›Œë“œë¡œ)
        if not self.check_electrical_relevance(query):
            return [], "non_electrical"
        
        # 2. ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰ (ê³ ë„í™”ëœ ë‹¤ì¤‘ ì§€í‘œ)
        db_results, db_found = self.search_vector_database(query)
        
        # 3. ì‹ ë¢°ë„ ê¸°ë°˜ ê²€ìƒ‰ ì „ëµ ê²°ì •
        if db_found and len(db_results) > 0:
            highest_score = db_results[0]["final_score"]
            
            if highest_score > 0.80:
                # ë§¤ìš° ê³ ì‹ ë¢°ë„ - ì§ì ‘ DB ë‹µë³€
                return db_results, "very_high_confidence_db"
            elif highest_score > 0.70:
                # ê³ ì‹ ë¢°ë„ - DB ë‹µë³€ + ë³´ê°•
                return db_results, "high_confidence_db"
            elif highest_score > 0.60:
                # ì¤‘ê°„ì‹ ë¢°ë„ - LLM ì¬êµ¬ì„±
                return db_results, "medium_confidence_db"
            elif highest_score > 0.45:
                # ì €ì‹ ë¢°ë„ - í•˜ì´ë¸Œë¦¬ë“œ ì‹œë„
                web_results = self.search_web(query)
                if web_results:
                    return (db_results, web_results), "hybrid_search"
                else:
                    return db_results, "low_confidence_db"
        
        # 4. DB ê²°ê³¼ê°€ ë¶€ì¡±í•œ ê²½ìš° ì›¹ê²€ìƒ‰ ì‹œë„
        web_results = self.search_web(query)
        if web_results:
            if db_found and len(db_results) > 0:
                # DB + ì›¹ í•˜ì´ë¸Œë¦¬ë“œ
                return (db_results, web_results), "hybrid_search"
            else:
                # ì›¹ê²€ìƒ‰ ì „ìš©
                return web_results, "web_only"
        
        # 5. ëª¨ë“  ê²€ìƒ‰ì´ ì‹¤íŒ¨í•œ ê²½ìš°
        if db_found and len(db_results) > 0:
            return db_results, "fallback_db"
        else:
            return [], "no_results"