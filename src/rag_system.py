#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Improved RAG System
ê°œì„ ëœ RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ
"""

import os
import time
import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass

import torch
import numpy as np
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

from config import RAGConfig, DatasetConfig
from document_loader import DatasetLoader
from llm_client import LLMClient

logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["SENTENCE_TRANSFORMERS_TRUST_REMOTE_CODE"] = "true"


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    question: str
    answer: str
    score: float
    category: str = "general"
    metadata: Dict = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class ChromaEmbeddingFunction:
    """ChromaDBìš© ì„ë² ë”© í•¨ìˆ˜"""
    
    def __init__(self, model: SentenceTransformer):
        self.model = model
        
    def __call__(self, input: List[str]) -> List[List[float]]:
        """ì„ë² ë”© ìƒì„±"""
        embeddings = self.model.encode(input, normalize_embeddings=True)
        return embeddings.tolist()


class VectorStore:
    """ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: RAGConfig, embedding_model: SentenceTransformer):
        self.config = config
        self.embedding_function = ChromaEmbeddingFunction(embedding_model)
        self.client = None
        self.collection = None
        self._initialize()
        
    def _initialize(self):
        """ChromaDB ì´ˆê¸°í™”"""
        try:
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            self.client = chromadb.PersistentClient(
                path=self.config.chroma_db_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
            try:
                self.client.delete_collection(name=self.config.collection_name)
                logger.info(f"Deleted existing collection: {self.config.collection_name}")
            except Exception:
                pass
            
            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
            self.collection = self.client.create_collection(
                name=self.config.collection_name,
                embedding_function=self.embedding_function,
                metadata={"hnsw:space": "cosine"}
            )
            
            logger.info(f"Created new collection: {self.config.collection_name}")
            
        except Exception as e:
            logger.error(f"Failed to initialize ChromaDB: {e}")
            raise
    
    def add_documents(self, documents: List[Dict[str, str]]):
        """ë¬¸ì„œ ë²¡í„°í™” ë° ì €ì¥"""
        if not documents:
            logger.warning("No documents to vectorize")
            return
            
        total_docs = len(documents)
        logger.info(f"Starting vectorization of {total_docs} documents...")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        for i in range(0, total_docs, self.config.batch_size):
            batch = documents[i:i + self.config.batch_size]
            
            # ë°ì´í„° ì¤€ë¹„
            texts = []
            metadatas = []
            ids = []
            
            for idx, doc in enumerate(batch):
                doc_id = i + idx
                texts.append(doc["question"])
                metadatas.append({
                    "question": doc["question"],
                    "answer": doc["answer"],
                    "category": self._categorize(doc["question"]),
                })
                ids.append(str(doc_id))
            
            # ChromaDBì— ì¶”ê°€
            self.collection.add(
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            current_batch = (i // self.config.batch_size) + 1
            total_batches = (total_docs + self.config.batch_size - 1) // self.config.batch_size
            logger.info(f"Vectorization progress: {current_batch}/{total_batches} batches")
        
        # ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
        count = self.collection.count()
        logger.info(f"Total documents in ChromaDB: {count}")
    
    def search(self, query: str, k: int) -> List[SearchResult]:
        """ë²¡í„° ê²€ìƒ‰"""
        try:
            count = self.collection.count()
            if count == 0:
                logger.warning("No documents in ChromaDB")
                return []
            
            # ê²€ìƒ‰ ì‹¤í–‰
            results = self.collection.query(
                query_texts=[query],
                n_results=min(k, count),
                include=["metadatas", "distances", "documents"]
            )
            
            if not results["ids"] or not results["ids"][0]:
                return []
            
            # ê²°ê³¼ ë³€í™˜
            search_results = []
            for i in range(len(results["ids"][0])):
                metadata = results["metadatas"][0][i]
                distance = results["distances"][0][i]
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                # ChromaDBì˜ cosine distance ë²”ìœ„ í™•ì¸ì„ ìœ„í•œ ìƒì„¸ ë¡œê¹…
                logger.debug(f"Raw distance from ChromaDB: {distance}")
                
                # ChromaDBì˜ cosine distanceë¥¼ similarityë¡œ ë³€í™˜
                # cosine distance = 1 - cosine similarity
                # ChromaDBëŠ” ì¼ë°˜ì ìœ¼ë¡œ 0~2 ë²”ìœ„ì˜ cosine distance ë°˜í™˜
                if distance <= 0:
                    similarity = 1.0  # ì™„ì „íˆ ë™ì¼
                elif distance >= 2:
                    similarity = 0.0  # ì™„ì „íˆ ë°˜ëŒ€
                else:
                    # í‘œì¤€ cosine similarity ê³„ì‚°
                    similarity = 1.0 - (distance / 2.0)
                
                # ë¡œê·¸ ë³€í™˜ì„ í†µí•œ ì ìˆ˜ ë¶„í¬ ê°œì„ 
                # ì‘ì€ ì°¨ì´ë¥¼ ë” í¬ê²Œ ë§Œë“¤ì–´ ì ìˆ˜ ë¶„í¬ë¥¼ ë„“í˜
                import math
                if similarity > 0.99:
                    # ë§¤ìš° ë†’ì€ ìœ ì‚¬ë„ëŠ” ì•½ê°„ ë‚®ì¶¤
                    similarity = 0.95 + 0.05 * (similarity - 0.99) / 0.01
                elif similarity > 0.9:
                    # ë†’ì€ ìœ ì‚¬ë„ êµ¬ê°„ í™•ì¥
                    similarity = 0.8 + 0.15 * (similarity - 0.9) / 0.1
                elif similarity > 0.7:
                    # ì¤‘ê°„ ìœ ì‚¬ë„ êµ¬ê°„ ìœ ì§€
                    similarity = 0.6 + 0.2 * (similarity - 0.7) / 0.2
                else:
                    # ë‚®ì€ ìœ ì‚¬ë„ëŠ” ë” ë‚®ì¶¤
                    similarity = similarity * 0.8
                
                # ê²°ê³¼ ë¡œê¹…
                if i < 3:  # ì²˜ìŒ 3ê°œ ê²°ê³¼ ìƒì„¸ ë¡œê·¸
                    logger.info(f"Result {i}: distance={distance:.6f}, similarity={similarity:.4f}, Q: {metadata['question'][:50]}...")
                
                search_results.append(SearchResult(
                    question=metadata["question"],
                    answer=metadata["answer"],
                    score=similarity,
                    category=metadata.get("category", "general"),
                    metadata=metadata
                ))
            
            return search_results
            
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []
    
    def _categorize(self, text: str) -> str:
        """ê°„ë‹¨í•œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë¶„ë¥˜ë§Œ ìˆ˜í–‰
        # í•„ìš”ì‹œ ë” ë³µì¡í•œ ë¶„ë¥˜ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
        return "general"


class SearchRanker:
    """ê²€ìƒ‰ ê²°ê³¼ ì¬ì •ë ¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        
    def rerank(self, query: str, results: List[SearchResult], k: int) -> List[SearchResult]:
        """ê²€ìƒ‰ ê²°ê³¼ ì¬ì •ë ¬ ë° ì ìˆ˜ ì¡°ì •"""
        if not results:
            return []
        
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        # ì ìˆ˜ ì¬ê³„ì‚°
        for result in results:
            # ê¸°ë³¸ ì ìˆ˜
            base_score = result.score
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            question_lower = result.question.lower()
            question_words = set(question_lower.split())
            
            # ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­
            exact_matches = len(query_words.intersection(question_words))
            keyword_bonus = exact_matches * self.config.keyword_match_bonus
            
            # ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
            substring_bonus = 0
            for word in query_words:
                if len(word) > 2 and word in question_lower:
                    substring_bonus += self.config.substring_match_bonus
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚°
            final_score = base_score + keyword_bonus + substring_bonus
            final_score = min(final_score, 1.0)
            
            result.score = final_score
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        results.sort(key=lambda x: x.score, reverse=True)
        
        # ì ìˆ˜ ë¶„í¬ ë¡œê¹…
        if results:
            scores = [r.score for r in results[:10]]  # ìƒìœ„ 10ê°œ
            logger.info(f"Score distribution (top 10): {scores}")
            logger.info(f"Score range: {min(scores):.4f} ~ {max(scores):.4f}")
        
        # ì„ê³„ê°’ í•„í„°ë§
        filtered_results = [
            r for r in results 
            if r.score >= self.config.low_confidence_threshold
        ]
        
        return filtered_results[:k]


class RAGSystem:
    """í†µí•© RAG ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        rag_config: Optional[RAGConfig] = None,
        dataset_config: Optional[DatasetConfig] = None,
        llm_client: Optional[LLMClient] = None
    ):
        """
        RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            rag_config: RAG ì„¤ì •
            dataset_config: ë°ì´í„°ì…‹ ì„¤ì •
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸
        """
        self.rag_config = rag_config or RAGConfig()
        self.dataset_config = dataset_config or DatasetConfig()
        self.llm_client = llm_client
        
        # í†µê³„
        self.stats = {
            "total_queries": 0,
            "high_confidence_hits": 0,
            "medium_confidence_hits": 0,
            "low_confidence_hits": 0,
            "avg_response_time": 0.0
        }
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()
        
        # ë¬¸ì„œ ë¡œë“œ
        self._load_documents()
    
    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        # ì„ë² ë”© ëª¨ë¸
        logger.info(f"Loading embedding model: {self.rag_config.embedding_model_name}")
        self.embedding_model = SentenceTransformer(
            self.rag_config.embedding_model_name,
            trust_remote_code=True,
            device="cuda" if torch.cuda.is_available() else "cpu"
        )
        
        # ì„ë² ë”© ëª¨ë¸ ê²€ì¦
        try:
            test_embedding = self.embedding_model.encode("í…ŒìŠ¤íŠ¸ ë¬¸ì¥")
            logger.info(f"Embedding model loaded successfully. Embedding dim: {len(test_embedding)}")
        except Exception as e:
            logger.error(f"Embedding model test failed: {e}")
        
        # ë²¡í„° ì €ì¥ì†Œ
        self.vector_store = VectorStore(self.rag_config, self.embedding_model)
        
        # ê²€ìƒ‰ ë­ì»¤
        self.ranker = SearchRanker(self.rag_config)
    
    def _load_documents(self):
        """ë¬¸ì„œ ë¡œë“œ ë° ì¸ë±ì‹±"""
        loader = DatasetLoader(
            paths=self.dataset_config.paths,
            file_extensions=self.dataset_config.file_extensions,
            max_documents=self.rag_config.max_documents
        )
        
        documents = loader.load_documents()
        
        if documents:
            self.vector_store.add_documents(documents)
        else:
            logger.warning("No documents loaded")
    
    def search(self, query: str, k: Optional[int] = None) -> Tuple[List[SearchResult], float]:
        """
        í–¥ìƒëœ ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            (ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸, ìµœê³  ì ìˆ˜)
        """
        start_time = time.time()
        k = k or self.rag_config.rerank_k
        
        logger.info(f"ğŸ” RAG ê²€ìƒ‰ ì‹œì‘: '{query[:100]}...', ìš”ì²­ {k}ê°œ")
        
        # ë²¡í„° ê²€ìƒ‰
        results = self.vector_store.search(query, self.rag_config.search_k)
        logger.info(f"ğŸ“Š 1ì°¨ ë²¡í„° ê²€ìƒ‰: {len(results)}ê°œ ê²°ê³¼")
        
        # ì¬ì •ë ¬
        final_results = self.ranker.rerank(query, results, k)
        logger.info(f"âš–ï¸ ì¬ì •ë ¬ ì™„ë£Œ: {len(final_results)}ê°œ ì±„íƒ")
        
        # ìµœê³  ì ìˆ˜ ê³„ì‚°
        max_score = max([r.score for r in final_results]) if final_results else 0.0
        
        # ìƒìœ„ 3ê°œ ê²°ê³¼ ë¡œê¹…
        for i, result in enumerate(final_results[:3]):
            logger.info(f"ğŸ“„ TOP{i+1} (Score: {result.score:.3f}): {result.question[:80]}...")
            if result.answer:
                logger.info(f"   ë‹µë³€: {result.answer[:100]}...")
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_stats(max_score, time.time() - start_time)
        
        search_time = time.time() - start_time
        logger.info(
            f"âœ… RAG ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼, "
            f"ìµœê³ ì ìˆ˜: {max_score:.3f}, ì‹œê°„: {search_time:.2f}s"
        )
        
        return final_results, max_score
    
    def _update_stats(self, max_score: float, response_time: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats["total_queries"] += 1
        
        # ì‹ ë¢°ë„ ë¶„ë¥˜ ë° ë¡œê¹…
        if max_score >= self.rag_config.high_confidence_threshold:
            self.stats["high_confidence_hits"] += 1
            confidence_level = "ğŸ”¥ ê³ ì‹ ë¢°ë„"
        elif max_score >= self.rag_config.medium_confidence_threshold:
            self.stats["medium_confidence_hits"] += 1
            confidence_level = "ğŸŸ¡ ì¤‘ì‹ ë¢°ë„"
        else:
            self.stats["low_confidence_hits"] += 1
            confidence_level = "ğŸ”´ ì €ì‹ ë¢°ë„"
            
        logger.info(f"ğŸ“Š RAG ê²€ìƒ‰ í’ˆì§ˆ: {confidence_level} (Score: {max_score:.3f})")
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        n = self.stats["total_queries"]
        prev_avg = self.stats["avg_response_time"]
        logger.debug(f"ğŸ“ˆ RAG í†µê³„ ì—…ë°ì´íŠ¸: ì´ {n}ê°œ ì§ˆì˜, ì‘ë‹µì‹œê°„: {response_time:.2f}s")
        self.stats["avg_response_time"] = (prev_avg * (n - 1) + response_time) / n
    
    def get_stats(self) -> Dict:
        """í†µê³„ ë°˜í™˜"""
        return self.stats.copy()


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
ImprovedRAGSystem = RAGSystem