#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG System Core Module
RAG ì‹œìŠ¤í…œ í•µì‹¬ ëª¨ë“ˆ
"""

import os
import json
import time
import logging
from typing import Dict, List, Optional
from collections import defaultdict

import numpy as np
import chromadb
from sentence_transformers import SentenceTransformer
from duckduckgo_search import DDGS

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ConcreteKoreanElectricalRAG:
    """í†µí•© RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, embedding_model_name: str = "jhgan/ko-sroberta-multitask"):
        """
        Args:
            embedding_model_name: í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        """
        try:
            os.environ["SAFETENSORS_FAST_GPU"] = "1"
            os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
            
            # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            self.embedding_model = SentenceTransformer(embedding_model_name)
            logger.info(f"í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {embedding_model_name}")
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            # í´ë°± ëª¨ë¸
            self.embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            logger.info("í´ë°± ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: paraphrase-multilingual-MiniLM")
        
        # ChromaDB ì´ˆê¸°í™”
        self.chroma_client = chromadb.PersistentClient(path="/tmp/chroma_db_korean_electrical")
        self.collection = self.chroma_client.get_or_create_collection(name="electrical_engineering_docs")
        
        # ë¬¸ì„œ ë° í†µê³„
        self.documents = []
        self.user_history = defaultdict(list)
        self.service_stats = {
            "total_queries": 0,
            "successful_answers": 0,
            "db_hits": 0,
            "web_searches": 0,
            "user_satisfaction": []
        }
        
        logger.info("ConcreteKoreanElectricalRAG ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_documents_from_dataset(self, dataset_path: str = "/dataset", max_docs: int = 6000):
        """ë°ì´í„°ì…‹ì—ì„œ ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„°í™”"""
        docs_count = 0
        categories = defaultdict(int)
        
        if not os.path.exists(dataset_path):
            logger.warning(f"ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {dataset_path}")
            self._load_sample_data()
            return
        
        # JSONL íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ
        for root, dirs, files in os.walk(dataset_path):
            for file in files:
                if file.endswith(".jsonl") and docs_count < max_docs:
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            for line in f:
                                if docs_count >= max_docs:
                                    break
                                try:
                                    data = json.loads(line)
                                    if "Context" in data and "Response" in data:
                                        context_part = data["Context"]
                                        response_part = data["Response"]
                                        
                                        # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
                                        category = self._categorize_document(context_part)
                                        categories[category] += 1
                                        
                                        content = f"ì§ˆë¬¸: {context_part} ë‹µë³€: {response_part} ë¶„ë¥˜: {category}"
                                        doc_item = {
                                            "id": str(docs_count),
                                            "text": content,
                                            "question": context_part,
                                            "answer": response_part,
                                            "category": category
                                        }
                                        self.documents.append(doc_item)
                                        docs_count += 1
                                except:
                                    continue
                    except:
                        continue
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ë¡œê·¸
        logger.info("ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
        for cat, count in categories.items():
            logger.info(f"- {cat}: {count}ê°œ")
        
        # ë²¡í„°í™” ë° ì €ì¥
        self._vectorize_documents()
    
    def _categorize_document(self, text: str) -> str:
        """ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""
        if any(word in text for word in ["ì˜´ì˜", "í‚¤ë¥´íˆí˜¸í”„", "ì „ìê¸°", "ë§¥ìŠ¤ì›°"]):
            return "ê¸°ë³¸ì´ë¡ "
        elif any(word in text for word in ["ë³€ì••ê¸°", "ëª¨í„°", "ë°œì „ê¸°", "ì „ë™ê¸°"]):
            return "ì „ê¸°ê¸°ê¸°"
        elif any(word in text for word in ["ì†¡ì „", "ë°°ì „", "ì „ë ¥ê³„í†µ", "ì•ˆì •ë„"]):
            return "ì „ë ¥ê³µí•™"
        elif any(word in text for word in ["ì‹œí—˜", "ìê²©ì¦", "ê¸°ì‚¬", "ì‚°ì—…ê¸°ì‚¬"]):
            return "ìê²©ì¦"
        else:
            return "ì¼ë°˜"
    
    def _load_sample_data(self):
        """ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ"""
        sample_docs = [
            {
                "question": "ì˜´ì˜ ë²•ì¹™ì´ ë¬´ì—‡ì¸ê°€ìš”?",
                "answer": "ì˜´ì˜ ë²•ì¹™ì€ ì „ì••(V) = ì „ë¥˜(I) Ã— ì €í•­(R)ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì „ê¸°ê³µí•™ì˜ ê¸°ë³¸ ë²•ì¹™ì…ë‹ˆë‹¤.",
                "category": "ê¸°ë³¸ì´ë¡ "
            },
            {
                "question": "êµë¥˜ì™€ ì§ë¥˜ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "answer": "ì§ë¥˜(DC)ëŠ” ì „ë¥˜ê°€ í•œ ë°©í–¥ìœ¼ë¡œë§Œ íë¥´ë©°, êµë¥˜(AC)ëŠ” ì „ë¥˜ì˜ ë°©í–¥ì´ ì£¼ê¸°ì ìœ¼ë¡œ ë°”ë€ë‹ˆë‹¤.",
                "category": "ê¸°ë³¸ì´ë¡ "
            },
            {
                "question": "ë³€ì••ê¸°ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
                "answer": "ë³€ì••ê¸°ëŠ” íŒ¨ëŸ¬ë°ì´ì˜ ì „ìê¸°ìœ ë„ ë²•ì¹™ì„ ì´ìš©í•˜ì—¬ ê¶Œìˆ˜ë¹„ì— ë”°ë¼ ì „ì••ì„ ë³€í™˜í•©ë‹ˆë‹¤.",
                "category": "ì „ê¸°ê¸°ê¸°"
            },
            {
                "question": "ì „ê¸°ê¸°ì‚¬ ì‹œí—˜ì€ ì–´ë–»ê²Œ ì¤€ë¹„í•˜ë‚˜ìš”?",
                "answer": "ì „ê¸°ê¸°ì‚¬ ì‹œí—˜ì€ í•„ê¸°ì™€ ì‹¤ê¸°ë¡œ êµ¬ì„±ë˜ë©°, ê¸°ë³¸ì„œ í•™ìŠµ í›„ ê¸°ì¶œë¬¸ì œë¥¼ ë°˜ë³µ í’€ì´í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.",
                "category": "ìê²©ì¦"
            }
        ]
        
        self.documents = []
        for i, doc_data in enumerate(sample_docs):
            content = f"ì§ˆë¬¸: {doc_data['question']} ë‹µë³€: {doc_data['answer']} ë¶„ë¥˜: {doc_data['category']}"
            self.documents.append({
                "id": str(i),
                "text": content,
                "question": doc_data["question"],
                "answer": doc_data["answer"],
                "category": doc_data["category"]
            })
        
        self._vectorize_documents()
    
    def _vectorize_documents(self):
        """ë¬¸ì„œ ë²¡í„°í™” ë° ChromaDB ì €ì¥"""
        if not self.documents:
            logger.warning("ë²¡í„°í™”í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        texts = [doc["text"] for doc in self.documents]
        logger.info(f"í•œêµ­ì–´ ë²¡í„° ì„ë² ë”© ì‹œì‘: {len(texts)}ê°œ ì „ê¸°ê³µí•™ ë¬¸ì„œ")
        
        batch_size = 50
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_ids = [self.documents[i+j]["id"] for j in range(len(batch_texts))]
            
            embeddings = self.embedding_model.encode(batch_texts, convert_to_tensor=True)
            embeddings_np = embeddings.cpu().numpy()
            
            self.collection.add(
                embeddings=embeddings_np.tolist(),
                documents=batch_texts,
                ids=batch_ids
            )
            
            logger.info(f"ë°°ì¹˜ {i//batch_size + 1} ì™„ë£Œ: {len(batch_texts)}ê°œ")
        
        logger.info(f"ì „ê¸°ê³µí•™ ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")
    
    def search_vector_database(self, query: str, k: int = 5) -> tuple:
        """ì§€ëŠ¥í˜• ë²¡í„° ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì¹´í…Œê³ ë¦¬ ì¶”ì •
            query_category = self._categorize_document(query)
            
            # ë²¡í„° ê²€ìƒ‰
            query_embedding = self.embedding_model.encode([query], convert_to_tensor=True)
            query_embedding_np = query_embedding.cpu().numpy()
            
            results = self.collection.query(
                query_embeddings=query_embedding_np.tolist(),
                n_results=k * 2  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
            )
            
            if results["documents"] and len(results["documents"]) > 0:
                documents = results["documents"][0]
                distances = results["distances"][0] if "distances" in results else [1.0] * len(documents)
                ids = results["ids"][0] if "ids" in results else []
                
                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ë° í•„í„°ë§
                relevant_docs = []
                for doc, distance, doc_id in zip(documents, distances, ids):
                    similarity = 1 - distance
                    
                    # ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    doc_info = None
                    for d in self.documents:
                        if d["id"] == doc_id:
                            doc_info = d
                            break
                    
                    # ì¹´í…Œê³ ë¦¬ ë³´ë„ˆìŠ¤
                    category_bonus = 0.1 if doc_info and doc_info["category"] == query_category else 0
                    
                    # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
                    keyword_bonus = sum(0.05 for word in query.split() if len(word) > 1 and word in doc)
                    
                    # ìµœì¢… ì ìˆ˜
                    final_score = similarity + category_bonus + keyword_bonus
                    
                    # ì„ê³„ê°’ ì´ìƒë§Œ ì¶”ê°€
                    if similarity > 0.5 or final_score > 0.6:
                        relevant_docs.append({
                            "content": doc,
                            "similarity": similarity,
                            "final_score": final_score,
                            "doc_info": doc_info
                        })
                
                # ì ìˆ˜ìˆœ ì •ë ¬
                relevant_docs.sort(key=lambda x: x["final_score"], reverse=True)
                
                if relevant_docs:
                    self.service_stats["db_hits"] += 1
                    return relevant_docs[:k], True
            
            return [], False
        except Exception as e:
            logger.error(f"ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return [], False
    
    def search_web(self, query: str, max_results: int = 3) -> List[Dict]:
        """ì›¹ ê²€ìƒ‰ ë³´ì¡°"""
        try:
            with DDGS() as ddgs:
                enhanced_query = f"ë‹¤ì‚°ì—ë“€ {query}"
                web_results = list(ddgs.text(enhanced_query, region="ko-kr", max_results=max_results))
                
                processed_results = []
                for result in web_results:
                    trust_score = 1.5 if any(domain in result.get("href", "") for domain in [".edu", ".ac.kr", ".go.kr", "kea.kr"]) else 1.0
                    
                    processed_results.append({
                        "title": result.get("title", ""),
                        "snippet": result.get("body", "")[:200],
                        "url": result.get("href", ""),
                        "trust_score": trust_score
                    })
                
                # ì‹ ë¢°ë„ìˆœ ì •ë ¬
                processed_results.sort(key=lambda x: x["trust_score"], reverse=True)
                self.service_stats["web_searches"] += 1
                return processed_results
        except Exception as e:
            logger.error(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def check_electrical_relevance(self, query: str) -> bool:
        """ì „ê¸°ê³µí•™ ê´€ë ¨ì„± í™•ì¸"""
        electrical_keywords = [
            "ì „ê¸°", "ì „ë ¥", "ì „ì••", "ì „ë¥˜", "ì €í•­", "íšŒë¡œ", "ë³€ì••ê¸°", "ëª¨í„°", "ë°œì „",
            "ë°°ì „", "ì†¡ì „", "ì „ì", "ì—ë„ˆì§€", "ì™€íŠ¸", "ì•”í˜ì–´", "ë³¼íŠ¸", "ì˜´",
            "AC", "DC", "êµë¥˜", "ì§ë¥˜", "ì£¼íŒŒìˆ˜", "ì„í”¼ë˜ìŠ¤", "ì¸ë•í„´ìŠ¤",
            "ì»¤íŒ¨ì‹œí„´ìŠ¤", "ê¸°ì‚¬", "ìê²©ì¦", "ì‹œí—˜", "ê³µë¶€"
        ]
        
        query_lower = query.lower()
        return any(keyword.lower() in query_lower for keyword in electrical_keywords)
    
    def get_service_statistics(self) -> str:
        """ì„œë¹„ìŠ¤ í†µê³„ ì œê³µ"""
        stats = []
        stats.append("ğŸ“Š **ì „ê¸°ê³µí•™ í†µí•© ì„œë¹„ìŠ¤ í†µê³„**\n")
        stats.append(f"â€¢ ì´ ì§ˆì˜: {self.service_stats['total_queries']}ê±´")
        stats.append(f"â€¢ ì„±ê³µ ë‹µë³€: {self.service_stats['successful_answers']}ê±´")
        stats.append(f"â€¢ DB ì ì¤‘: {self.service_stats['db_hits']}ê±´")
        stats.append(f"â€¢ ì›¹ ê²€ìƒ‰: {self.service_stats['web_searches']}ê±´")
        
        if self.service_stats["total_queries"] > 0:
            success_rate = self.service_stats["successful_answers"] / self.service_stats["total_queries"] * 100
            db_hit_rate = self.service_stats["db_hits"] / self.service_stats["total_queries"] * 100
            stats.append(f"\nâ€¢ ì‘ë‹µ ì„±ê³µë¥ : {round(success_rate, 1)}%")
            stats.append(f"â€¢ DB í™œìš©ë¥ : {round(db_hit_rate, 1)}%")
        
        stats.append(f"\nâ€¢ ì§€ì‹ë² ì´ìŠ¤: {len(self.documents)}ê°œ ë¬¸ì„œ")
        stats.append(f"â€¢ í™œì„± ì‚¬ìš©ì: {len(self.user_history)}ëª…")
        
        return "\n".join(stats)