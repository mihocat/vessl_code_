#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG System Core Module
RAG ì‹œìŠ¤í…œ í•µì‹¬ ëª¨ë“ˆ
"""

import os
import json
import time
import logging
from typing import Dict, List, Optional
from collections import defaultdict

import numpy as np
import chromadb
from sentence_transformers import SentenceTransformer
from duckduckgo_search import DDGS

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ConcreteKoreanElectricalRAG:
    """í†µí•© RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, embedding_model_name: str = "jhgan/ko-sroberta-multitask"):
        """
        Args:
            embedding_model_name: í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        """
        try:
            os.environ["SAFETENSORS_FAST_GPU"] = "1"
            os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
            
            # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            self.embedding_model = SentenceTransformer(embedding_model_name)
            logger.info(f"í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {embedding_model_name}")
        except Exception as e:
            logger.error(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            # í´ë°± ëª¨ë¸
            self.embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            logger.info("í´ë°± ì„ë² ë”© ëª¨ë¸ ë¡œë“œ: paraphrase-multilingual-MiniLM")
        
        # ChromaDB ì´ˆê¸°í™”
        self.chroma_client = chromadb.PersistentClient(path="/tmp/chroma_db_korean_electrical")
        self.collection = self.chroma_client.get_or_create_collection(name="electrical_engineering_docs")
        
        # ë¬¸ì„œ ë° í†µê³„
        self.documents = []
        self.user_history = defaultdict(list)
        self.service_stats = {
            "total_queries": 0,
            "successful_answers": 0,
            "db_hits": 0,
            "web_searches": 0,
            "user_satisfaction": []
        }
        
        logger.info("ConcreteKoreanElectricalRAG ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_documents_from_dataset(self, dataset_path: str = "/dataset", max_docs: int = 6000):
        """ë°ì´í„°ì…‹ì—ì„œ ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„°í™”"""
        docs_count = 0
        categories = defaultdict(int)
        
        if not os.path.exists(dataset_path):
            logger.warning(f"ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {dataset_path}")
            self._load_sample_data()
            return
        
        # JSONL íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ
        for root, dirs, files in os.walk(dataset_path):
            for file in files:
                if file.endswith(".jsonl") and docs_count < max_docs:
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            for line in f:
                                if docs_count >= max_docs:
                                    break
                                try:
                                    data = json.loads(line)
                                    if "Context" in data and "Response" in data:
                                        context_part = data["Context"]
                                        response_part = data["Response"]
                                        
                                        # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
                                        category = self._categorize_document(context_part)
                                        categories[category] += 1
                                        
                                        content = f"ì§ˆë¬¸: {context_part} ë‹µë³€: {response_part} ë¶„ë¥˜: {category}"
                                        doc_item = {
                                            "id": str(docs_count),
                                            "text": content,
                                            "question": context_part,
                                            "answer": response_part,
                                            "category": category
                                        }
                                        self.documents.append(doc_item)
                                        docs_count += 1
                                except:
                                    continue
                    except:
                        continue
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ë¡œê·¸
        logger.info("ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
        for cat, count in categories.items():
            logger.info(f"- {cat}: {count}ê°œ")
        
        # ë²¡í„°í™” ë° ì €ì¥
        self._vectorize_documents()
    
    def _categorize_document(self, text: str) -> str:
        """ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""
        if any(word in text for word in ["ì˜´ì˜", "í‚¤ë¥´íˆí˜¸í”„", "ì „ìê¸°", "ë§¥ìŠ¤ì›°"]):
            return "ê¸°ë³¸ì´ë¡ "
        elif any(word in text for word in ["ë³€ì••ê¸°", "ëª¨í„°", "ë°œì „ê¸°", "ì „ë™ê¸°"]):
            return "ì „ê¸°ê¸°ê¸°"
        elif any(word in text for word in ["ì†¡ì „", "ë°°ì „", "ì „ë ¥ê³„í†µ", "ì•ˆì •ë„"]):
            return "ì „ë ¥ê³µí•™"
        elif any(word in text for word in ["ì‹œí—˜", "ìê²©ì¦", "ê¸°ì‚¬", "ì‚°ì—…ê¸°ì‚¬"]):
            return "ìê²©ì¦"
        else:
            return "ì¼ë°˜"
    
    def _load_sample_data(self):
        """ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ"""
        sample_docs = [
            {
                "question": "ì˜´ì˜ ë²•ì¹™ì´ ë¬´ì—‡ì¸ê°€ìš”?",
                "answer": "ì˜´ì˜ ë²•ì¹™ì€ ì „ì••(V) = ì „ë¥˜(I) Ã— ì €í•­(R)ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì „ê¸°ê³µí•™ì˜ ê¸°ë³¸ ë²•ì¹™ì…ë‹ˆë‹¤.",
                "category": "ê¸°ë³¸ì´ë¡ "
            },
            {
                "question": "êµë¥˜ì™€ ì§ë¥˜ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "answer": "ì§ë¥˜(DC)ëŠ” ì „ë¥˜ê°€ í•œ ë°©í–¥ìœ¼ë¡œë§Œ íë¥´ë©°, êµë¥˜(AC)ëŠ” ì „ë¥˜ì˜ ë°©í–¥ì´ ì£¼ê¸°ì ìœ¼ë¡œ ë°”ë€ë‹ˆë‹¤.",
                "category": "ê¸°ë³¸ì´ë¡ "
            },
            {
                "question": "ë³€ì••ê¸°ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
                "answer": "ë³€ì••ê¸°ëŠ” íŒ¨ëŸ¬ë°ì´ì˜ ì „ìê¸°ìœ ë„ ë²•ì¹™ì„ ì´ìš©í•˜ì—¬ ê¶Œìˆ˜ë¹„ì— ë”°ë¼ ì „ì••ì„ ë³€í™˜í•©ë‹ˆë‹¤.",
                "category": "ì „ê¸°ê¸°ê¸°"
            },
            {
                "question": "ì „ê¸°ê¸°ì‚¬ ì‹œí—˜ì€ ì–´ë–»ê²Œ ì¤€ë¹„í•˜ë‚˜ìš”?",
                "answer": "ì „ê¸°ê¸°ì‚¬ ì‹œí—˜ì€ í•„ê¸°ì™€ ì‹¤ê¸°ë¡œ êµ¬ì„±ë˜ë©°, ê¸°ë³¸ì„œ í•™ìŠµ í›„ ê¸°ì¶œë¬¸ì œë¥¼ ë°˜ë³µ í’€ì´í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.",
                "category": "ìê²©ì¦"
            }
        ]
        
        self.documents = []
        for i, doc_data in enumerate(sample_docs):
            content = f"ì§ˆë¬¸: {doc_data['question']} ë‹µë³€: {doc_data['answer']} ë¶„ë¥˜: {doc_data['category']}"
            self.documents.append({
                "id": str(i),
                "text": content,
                "question": doc_data["question"],
                "answer": doc_data["answer"],
                "category": doc_data["category"]
            })
        
        self._vectorize_documents()
    
    def _vectorize_documents(self):
        """ë¬¸ì„œ ë²¡í„°í™” ë° ChromaDB ì €ì¥"""
        if not self.documents:
            logger.warning("ë²¡í„°í™”í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        texts = [doc["text"] for doc in self.documents]
        logger.info(f"í•œêµ­ì–´ ë²¡í„° ì„ë² ë”© ì‹œì‘: {len(texts)}ê°œ ì „ê¸°ê³µí•™ ë¬¸ì„œ")
        
        batch_size = 50
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_ids = [self.documents[i+j]["id"] for j in range(len(batch_texts))]
            
            embeddings = self.embedding_model.encode(batch_texts, convert_to_tensor=True)
            embeddings_np = embeddings.cpu().numpy()
            
            self.collection.add(
                embeddings=embeddings_np.tolist(),
                documents=batch_texts,
                ids=batch_ids
            )
            
            logger.info(f"ë°°ì¹˜ {i//batch_size + 1} ì™„ë£Œ: {len(batch_texts)}ê°œ")
        
        logger.info(f"ì „ê¸°ê³µí•™ ì§€ì‹ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(texts)}ê°œ ë¬¸ì„œ")
    
    def search_vector_database(self, query: str, k: int = 5) -> tuple:
        """ê³ ë„í™”ëœ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
        try:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬ ë° í™•ì¥
            enhanced_queries = self._expand_query(query)
            query_category = self._advanced_categorize_document(query)
            
            best_results = []
            
            # ë‹¤ì¤‘ ì¿¼ë¦¬ ì „ëµ
            for enhanced_query in enhanced_queries:
                query_embedding = self.embedding_model.encode([enhanced_query], convert_to_tensor=True)
                query_embedding_np = query_embedding.cpu().numpy()
                
                results = self.collection.query(
                    query_embeddings=query_embedding_np.tolist(),
                    n_results=k * 3,  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ ê³ í’ˆì§ˆ í•„í„°ë§
                    include=['distances', 'documents', 'metadatas']
                )
                
                if results["documents"] and len(results["documents"]) > 0:
                    documents = results["documents"][0]
                    distances = results["distances"][0]
                    ids = results["ids"][0]
                    
                    # ê³ ë„í™”ëœ ì ìˆ˜ ê³„ì‚°
                    for doc, distance, doc_id in zip(documents, distances, ids):
                        doc_info = self._get_document_info(doc_id)
                        if not doc_info:
                            continue
                            
                        # ë‹¤ì¤‘ ì§€í‘œ ê¸°ë°˜ ì ìˆ˜
                        scores = self._calculate_multi_score(query, enhanced_query, doc, doc_info, distance, query_category)
                        
                        # ê³ í’ˆì§ˆ ê²°ê³¼ë§Œ ì„ ë³„
                        if scores['final_score'] > 0.65:  # ë” ì—„ê²©í•œ ì„ê³„ê°’
                            best_results.append({
                                "content": doc,
                                "doc_info": doc_info,
                                "scores": scores,
                                "similarity": scores['cosine_similarity'],
                                "final_score": scores['final_score'],
                                "query_type": enhanced_query
                            })
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_results = self._deduplicate_results(best_results)
            unique_results.sort(key=lambda x: x["final_score"], reverse=True)
            
            if unique_results:
                self.service_stats["db_hits"] += 1
                return unique_results[:k], True
            
            return [], False
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return [], False
    
    def _expand_query(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ í™•ì¥ ë° ë‹¤ì¤‘ ë²„ì „ ìƒì„±"""
        queries = [query]  # ì›ë³¸ ì¿¼ë¦¬
        
        # ì „ê¸°ê³µí•™ ë™ì˜ì–´ í™•ì¥
        synonyms = {
            'ì „ì••': ['ë³¼íŠ¸', 'V', 'ì „ìœ„ì°¨'],
            'ì „ë¥˜': ['ì•”í˜ì–´', 'A', 'ì „ë¥˜ê°’'],
            'ì €í•­': ['ì˜¤ì˜ˆ', 'Î©', 'R'],
            'ë³€ì••ê¸°': ['íŠ¸ëœìŠ¤í¬ë¨¸', 'ì „ë ¥ë³€ì••ê¸°'],
            'ëª¨í„°': ['ì „ë™ê¸°', 'ìœ ë„ì „ë™ê¸°'],
            'ë°œì „ê¸°': ['ë™ê¸°', 'ì œë„ˆë ˆì´í„°']
        }
        
        for word, alternatives in synonyms.items():
            if word in query:
                for alt in alternatives:
                    queries.append(query.replace(word, alt))
        
        # ê¸°ìˆ ì  ë§¥ë½ ì¶”ê°€
        if any(term in query for term in ['ê³„ì‚°', 'êµ¬í•˜ê¸°', 'ë°©ë²•']):
            queries.append(f"{query} ê³µì‹ ë‹¨ê³„")
            queries.append(f"{query} í•´ê²° ë°©ë²•")
        
        return queries[:3]  # ìµœëŒ€ 3ê°œë¡œ ì œí•œ
    
    def _advanced_categorize_document(self, text: str) -> str:
        """ê³ ë„í™”ëœ ë¬¸ì„œ ë¶„ë¥˜"""
        category_keywords = {
            'ê¸°ë³¸ì´ë¡ ': ['ì˜´ì˜ë²•ì¹™', 'í‚¤ë¥´íˆí˜¸í”„', 'ì „ìê¸°', 'ë§¥ìŠ¤ì›°', 'ì¿¨ë¡±ì˜ë²•ì¹™', 'ë ‰ìŠ¤ì˜ë²•ì¹™'],
            'ì „ê¸°ê¸°ê¸°': ['ë³€ì••ê¸°', 'ëª¨í„°', 'ë°œì „ê¸°', 'ì „ë™ê¸°', 'ë™ê¸°ê¸°', 'ìœ ë„ì „ë™ê¸°', 'ë™ê¸°ëª¨í„°'],
            'ì „ë ¥ê³µí•™': ['ì†¡ì „', 'ë°°ì „', 'ì „ë ¥ê³„í†µ', 'ì•ˆì •ë„', 'ë³´í˜¸ê³„ì „', 'ì „ë ¥í’ˆì§ˆ'],
            'ìê²©ì¦': ['ì‹œí—˜', 'ìê²©ì¦', 'ê¸°ì‚¬', 'ì‚°ì—…ê¸°ì‚¬', 'ê¸°ëŠ¥ì‚¬', 'ì „ê¸°ê¸°ì‚¬']
        }
        
        text_lower = text.lower()
        scores = {}
        
        for category, keywords in category_keywords.items():
            score = sum(1 for keyword in keywords if keyword.lower() in text_lower)
            if score > 0:
                scores[category] = score
        
        return max(scores.items(), key=lambda x: x[1])[0] if scores else 'ì¼ë°˜'
    
    def _get_document_info(self, doc_id: str) -> Optional[Dict]:
        """ë¬¸ì„œ ì •ë³´ ê³ ì† ê²€ìƒ‰"""
        for doc in self.documents:
            if doc["id"] == doc_id:
                return doc
        return None
    
    def _calculate_multi_score(self, original_query: str, enhanced_query: str, doc: str, doc_info: Dict, distance: float, query_category: str) -> Dict:
        """ë‹¤ì¤‘ ì§€í‘œ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°"""
        cosine_similarity = 1 - distance
        
        # 1. ì˜ë¯¸ì  ìœ ì‚¬ë„
        semantic_score = cosine_similarity
        
        # 2. ì¹´í…Œê³ ë¦¬ ì¼ì¹˜ë„
        category_score = 0.15 if doc_info["category"] == query_category else 0
        
        # 3. í‚¤ì›Œë“œ ë§¤ì¹­ (ì •ë°€ ê²€ìƒ‰)
        original_words = set(original_query.split())
        doc_words = set(doc.lower().split())
        keyword_overlap = len(original_words.intersection(doc_words)) / max(len(original_words), 1)
        keyword_score = keyword_overlap * 0.2
        
        # 4. ë¬¸ì„œ í’ˆì§ˆ ì ìˆ˜
        quality_score = 0.1 if len(doc_info.get("answer", "")) > 50 else 0
        
        # 5. ê¸¸ì´ ê¸°ë°˜ ì •ê·œí™”
        length_penalty = 0.05 if len(doc) > 1000 else 0  # ë„ˆë¬´ ê¸´ ë¬¸ì„œ í˜ë„í‹°
        
        final_score = semantic_score + category_score + keyword_score + quality_score - length_penalty
        
        return {
            'cosine_similarity': cosine_similarity,
            'semantic_score': semantic_score,
            'category_score': category_score,
            'keyword_score': keyword_score,
            'quality_score': quality_score,
            'final_score': min(final_score, 1.0)  # 1.0 ìµœëŒ€ê°’ ì œí•œ
        }
    
    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ê²°ê³¼ ì œê±°"""
        seen_ids = set()
        unique_results = []
        
        for result in results:
            doc_id = result["doc_info"]["id"]
            if doc_id not in seen_ids:
                seen_ids.add(doc_id)
                unique_results.append(result)
        
        return unique_results
    
    def search_web(self, query: str, max_results: int = 3) -> List[Dict]:
        """ì§€ëŠ¥í˜• ì›¹ ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
        try:
            with DDGS() as ddgs:
                # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ
                search_queries = [
                    f"ì „ê¸°ê³µí•™ {query}",
                    f"{query} í•´ì„¤",
                    f"{query} ê¸°ì´ˆ ì´ë¡ ",
                    query  # ì›ë³¸ ì¿¼ë¦¬
                ]
                
                all_results = []
                
                for search_query in search_queries[:2]:  # ìƒìœ„ 2ê°œ ì „ëµë§Œ ì‚¬ìš©
                    try:
                        web_results = list(ddgs.text(search_query, region="ko-kr", max_results=max_results))
                
                        for result in web_results:
                            all_results.append(result)
                    except:
                        continue
                
                # ê²°ê³¼ ì²˜ë¦¬ ë° í•„í„°ë§
                processed_results = []
                for result in web_results:
                    trust_score = 1.5 if any(domain in result.get("href", "") for domain in [".edu", ".ac.kr", ".go.kr", "kea.kr"]) else 1.0
                    
                    processed_results.append({
                        "title": result.get("title", ""),
                        "snippet": result.get("body", "")[:200],
                        "url": result.get("href", ""),
                        "trust_score": trust_score
                    })
                
                # ì‹ ë¢°ë„ìˆœ ì •ë ¬
                processed_results.sort(key=lambda x: x["trust_score"], reverse=True)
                self.service_stats["web_searches"] += 1
                return processed_results
        except Exception as e:
            logger.error(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def check_electrical_relevance(self, query: str) -> bool:
        """ì „ê¸°ê³µí•™ ê´€ë ¨ì„± í™•ì¸"""
        electrical_keywords = [
            "ì „ê¸°", "ì „ë ¥", "ì „ì••", "ì „ë¥˜", "ì €í•­", "íšŒë¡œ", "ë³€ì••ê¸°", "ëª¨í„°", "ë°œì „",
            "ë°°ì „", "ì†¡ì „", "ì „ì", "ì—ë„ˆì§€", "ì™€íŠ¸", "ì•”í˜ì–´", "ë³¼íŠ¸", "ì˜´",
            "AC", "DC", "êµë¥˜", "ì§ë¥˜", "ì£¼íŒŒìˆ˜", "ì„í”¼ë˜ìŠ¤", "ì¸ë•í„´ìŠ¤",
            "ì»¤íŒ¨ì‹œí„´ìŠ¤", "ê¸°ì‚¬", "ìê²©ì¦", "ì‹œí—˜", "ê³µë¶€"
        ]
        
        query_lower = query.lower()
        return any(keyword.lower() in query_lower for keyword in electrical_keywords)
    
    def get_service_statistics(self) -> str:
        """ì„œë¹„ìŠ¤ í†µê³„ ì œê³µ"""
        stats = []
        stats.append("ğŸ“Š **ì „ê¸°ê³µí•™ í†µí•© ì„œë¹„ìŠ¤ í†µê³„**\n")
        stats.append(f"â€¢ ì´ ì§ˆì˜: {self.service_stats['total_queries']}ê±´")
        stats.append(f"â€¢ ì„±ê³µ ë‹µë³€: {self.service_stats['successful_answers']}ê±´")
        stats.append(f"â€¢ DB ì ì¤‘: {self.service_stats['db_hits']}ê±´")
        stats.append(f"â€¢ ì›¹ ê²€ìƒ‰: {self.service_stats['web_searches']}ê±´")
        
        if self.service_stats["total_queries"] > 0:
            success_rate = self.service_stats["successful_answers"] / self.service_stats["total_queries"] * 100
            db_hit_rate = self.service_stats["db_hits"] / self.service_stats["total_queries"] * 100
            stats.append(f"\nâ€¢ ì‘ë‹µ ì„±ê³µë¥ : {round(success_rate, 1)}%")
            stats.append(f"â€¢ DB í™œìš©ë¥ : {round(db_hit_rate, 1)}%")
        
        stats.append(f"\nâ€¢ ì§€ì‹ë² ì´ìŠ¤: {len(self.documents)}ê°œ ë¬¸ì„œ")
        stats.append(f"â€¢ í™œì„± ì‚¬ìš©ì: {len(self.user_history)}ëª…")
        
        return "\n".join(stats)