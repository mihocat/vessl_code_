#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Gradio UI Application with Improved Image and Formula Processing
í–¥ìƒëœ ì´ë¯¸ì§€ ë° ìˆ˜ì‹ ì²˜ë¦¬ ê¸°ëŠ¥ì„ ê°–ì¶˜ Gradio UI ì• í”Œë¦¬ì¼€ì´ì…˜
"""

import sys
import time
import logging
from typing import List, Tuple, Optional, Union, Dict, Any
from PIL import Image
import torch
import gradio as gr

from config import Config
from llm_client import LLMClient
from rag_system import RAGSystem, SearchResult
from services import WebSearchService, ResponseGenerator
from intelligent_rag_adapter import IntelligentRAGAdapter

# í–¥ìƒëœ ë©€í‹°ëª¨ë‹¬ í”„ë¡œì„¸ì„œ
from enhanced_multimodal_processor import EnhancedMultimodalProcessor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedChatService:
    """í–¥ìƒëœ ì±—ë´‡ ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: Config, llm_client: LLMClient):
        """
        ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            config: ì „ì²´ ì„¤ì • ê°ì²´
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸
        """
        self.config = config
        self.llm_client = llm_client
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.rag_system = RAGSystem(
            rag_config=config.rag,
            dataset_config=config.dataset,
            llm_client=llm_client
        )
        self.web_search = WebSearchService(config.web_search)
        self.response_generator = ResponseGenerator(config.web_search)
        
        # Intelligent RAG ì–´ëŒ‘í„° ì´ˆê¸°í™”
        self.intelligent_adapter = IntelligentRAGAdapter(config, llm_client)
        
        # í–¥ìƒëœ ë©€í‹°ëª¨ë‹¬ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        use_openai_vision = config.openai.use_vision_api if hasattr(config, 'openai') else False
        self.multimodal_processor = EnhancedMultimodalProcessor(
            use_gpu=torch.cuda.is_available(),
            use_openai_vision=use_openai_vision
        )
        logger.info(f"Enhanced multimodal processor initialized (OpenAI Vision: {use_openai_vision})")
        
        # ëŒ€í™” ì´ë ¥
        self.conversation_history = []
        
        # ì²˜ë¦¬ ìƒíƒœ ì¶”ì 
        self.last_processing_status = {
            'ocr_engines': [],
            'formulas_detected': 0,
            'processing_time': 0,
            'errors': []
        }
    
    def process_query(
        self, 
        question: str, 
        history: List[Tuple[str, str]],
        image: Optional[Image.Image] = None,
        **kwargs
    ) -> Tuple[str, Dict[str, Any]]:
        """
        ì‚¬ìš©ì ì§ˆì˜ ì²˜ë¦¬ (í–¥ìƒëœ ë²„ì „)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            history: ëŒ€í™” ì´ë ¥
            image: ì„ íƒì  ì´ë¯¸ì§€ ì…ë ¥
            **kwargs: ì¶”ê°€ ì˜µì…˜
            
        Returns:
            (ì‘ë‹µ í…ìŠ¤íŠ¸, ì²˜ë¦¬ ìƒíƒœ)
        """
        start_time = time.time()
        
        # ìƒíƒœ ì´ˆê¸°í™”
        self.last_processing_status = {
            'ocr_engines': [],
            'formulas_detected': 0,
            'processing_time': 0,
            'errors': []
        }
        
        # ë¹ˆ ì§ˆë¬¸ ì²˜ë¦¬
        if not question or not question.strip():
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", self.last_processing_status
        
        try:
            # Intelligent RAG ì‚¬ìš© ì—¬ë¶€ ê²°ì •
            context = {
                'conversation_history': history,
                'image': image
            }
            
            use_intelligent = self.intelligent_adapter.should_use_intelligent(question, context)
            
            if use_intelligent:
                logger.info("Using Intelligent RAG for complex query")
                try:
                    result = self.intelligent_adapter.process_sync(question, context)
                    response = result.get('response', "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    self.last_processing_status['intelligent_rag'] = True
                except Exception as e:
                    logger.error(f"Intelligent RAG failed, falling back to standard: {e}")
                    self.last_processing_status['errors'].append(f"Intelligent RAG: {str(e)}")
                    use_intelligent = False
            
            if not use_intelligent:
                # í‘œì¤€ ì²˜ë¦¬ (í–¥ìƒëœ ë²„ì „)
                response = self._process_standard_query(question, history, image)
            
            # ì‘ë‹µ ì‹œê°„ ì¶”ê°€
            elapsed_time = time.time() - start_time
            self.last_processing_status['processing_time'] = elapsed_time
            response += f"\n\n_ì‘ë‹µì‹œê°„: {elapsed_time:.2f}ì´ˆ_"
            
            # ëŒ€í™” ì´ë ¥ ì—…ë°ì´íŠ¸
            self.conversation_history.append((question, response))
            
            return response, self.last_processing_status
            
        except Exception as e:
            logger.error(f"Query processing failed: {e}")
            self.last_processing_status['errors'].append(str(e))
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", self.last_processing_status
    
    def _process_standard_query(
        self,
        question: str,
        history: List[Tuple[str, str]],
        image: Optional[Image.Image]
    ) -> str:
        """í‘œì¤€ ì¿¼ë¦¬ ì²˜ë¦¬ (í–¥ìƒëœ ë²„ì „)"""
        # ì´ë¯¸ì§€ ë¶„ì„ (ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
        image_context = None
        original_question = question
        
        if image:
            try:
                logger.info("Processing image with enhanced multimodal processor...")
                
                # ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬
                multimodal_result = self.multimodal_processor.process_multimodal_query(
                    question, image
                )
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                if 'image_analysis' in multimodal_result:
                    analysis = multimodal_result['image_analysis']
                    self.last_processing_status['ocr_engines'] = analysis.get('engines_used', [])
                    self.last_processing_status['formulas_detected'] = len(analysis.get('formulas', []))
                    
                    # ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
                    image_context = analysis
                    
                    # ì§ˆë¬¸ í™•ì¥
                    if multimodal_result.get('combined_query'):
                        question = multimodal_result['combined_query']
                    
                    logger.info(f"Multimodal processing completed: {self.last_processing_status}")
                    
            except Exception as e:
                logger.error(f"Multimodal processing failed: {e}", exc_info=True)
                self.last_processing_status['errors'].append(f"Multimodal: {str(e)}")
                image_context = {
                    "error": str(e),
                    "caption": "[ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨]",
                    "ocr_text": ""
                }
                question = original_question
        
        # RAG ê²€ìƒ‰ ìˆ˜í–‰
        results, max_score = self.rag_system.search(question)
        
        # ì‘ë‹µ ìƒì„±
        response = self._generate_enhanced_response(question, results, max_score, image_context)
        
        return response
    
    def _generate_enhanced_response(
        self, 
        question: str, 
        results: List[SearchResult], 
        max_score: float,
        image_context: Optional[dict] = None
    ) -> str:
        """í–¥ìƒëœ ì‘ë‹µ ìƒì„±"""
        response_header = "ë‹µë³€: "
        
        # ì‹ ë¢°ë„ ìˆ˜ì¤€ ê²°ì •
        if image_context:
            confidence_level = "medium" if max_score >= self.config.rag.medium_confidence_threshold else "low"
        else:
            if max_score >= self.config.rag.high_confidence_threshold:
                confidence_level = "high"
            elif max_score >= self.config.rag.medium_confidence_threshold:
                confidence_level = "medium"
            else:
                confidence_level = "low"
        
        # ë†’ì€ ì‹ ë¢°ë„ - ì§ì ‘ ë‹µë³€ ì‚¬ìš© (ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
        if confidence_level == "high" and results and not image_context:
            best_result = results[0]
            response = response_header + best_result.answer
            
            if best_result.category and best_result.category != "general":
                response += f"\n\n_[ì¹´í…Œê³ ë¦¬: {best_result.category}]_"
        
        # ì¤‘ê°„/ë‚®ì€ ì‹ ë¢°ë„ ë˜ëŠ” ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° - LLM í™œìš©
        else:
            # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (ë‚®ì€ ì‹ ë¢°ë„ì´ê³  ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°)
            web_results = []
            if confidence_level == "low" and not image_context:
                web_results = self.web_search.search(question)
            
            # í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            context = self._prepare_enhanced_context(results, web_results, image_context)
            
            # ìˆ˜ì‹ í•´ê²° ê²°ê³¼ ì¶”ê°€
            if image_context and 'formula_solutions' in image_context:
                context += "\n\nìˆ˜ì‹ í•´ê²° ê²°ê³¼:\n"
                for formula, solution in image_context['formula_solutions'].items():
                    context += f"- {formula} = {solution}\n"
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.response_generator.generate_prompt(
                question, context, confidence_level
            )
            
            # LLM ì‘ë‹µ ìƒì„±
            try:
                if prompt:
                    llm_response = self.llm_client.query(prompt, "")
                    response = response_header + llm_response
                else:
                    response = response_header + "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            except Exception as e:
                logger.error(f"LLM response generation failed: {e}")
                response = response_header + "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # ì´ë¯¸ì§€ ë¶„ì„ ì •ë³´ ì¶”ê°€
        if image_context:
            if image_context.get('formulas'):
                response += f"\n\nğŸ“ ê°ì§€ëœ ìˆ˜ì‹: {len(image_context['formulas'])}ê°œ"
            if image_context.get('ocr_text'):
                ocr_preview = image_context['ocr_text'][:100]
                if len(image_context['ocr_text']) > 100:
                    ocr_preview += "..."
                response += f"\nğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {ocr_preview}"
        
        # ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ
        related_questions = self._get_related_questions(question, results)
        if related_questions:
            response += "\n\n**ğŸ’¡ ê´€ë ¨ ì§ˆë¬¸:**"
            for q in related_questions:
                response += f"\n- {q}"
        
        # ì ìˆ˜ì™€ ì‹ ë¢°ë„ ì •ë³´ í‘œì‹œ
        response += f"\n\n[ì ìˆ˜: {max_score:.3f}, ì‹ ë¢°ë„: {confidence_level}]"
        
        return response
    
    def _prepare_enhanced_context(
        self,
        results: List[SearchResult],
        web_results: List[dict],
        image_context: Optional[dict]
    ) -> str:
        """í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„"""
        context_parts = []
        
        # ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸
        if image_context:
            if image_context.get('caption'):
                context_parts.append(f"ì´ë¯¸ì§€ ì„¤ëª…: {image_context['caption']}")
            
            if image_context.get('ocr_text'):
                context_parts.append(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸:\n{image_context['ocr_text']}")
            
            if image_context.get('formulas'):
                formula_text = "ê°ì§€ëœ ìˆ˜ì‹:\n"
                for i, formula in enumerate(image_context['formulas'], 1):
                    formula_text += f"{i}. {formula}\n"
                context_parts.append(formula_text)
        
        # RAG ê²€ìƒ‰ ê²°ê³¼
        if results:
            rag_context = self.response_generator.prepare_context(results, web_results, None)
            context_parts.append(rag_context)
        
        return "\n\n".join(context_parts)
    
    def _get_related_questions(
        self, 
        question: str, 
        results: List[SearchResult]
    ) -> List[str]:
        """ê´€ë ¨ ì§ˆë¬¸ ì¶”ì¶œ"""
        if not results or len(results) < 2:
            return []
        
        related = []
        seen_questions = {question.lower()}
        
        for result in results[1:]:
            if result.score >= 0.6:
                q_lower = result.question.lower()
                if q_lower not in seen_questions:
                    related.append(result.question)
                    seen_questions.add(q_lower)
                    
                    if len(related) >= 3:
                        break
        
        return related


def create_enhanced_gradio_app(config: Optional[Config] = None) -> gr.Blocks:
    """
    í–¥ìƒëœ Gradio ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
    
    Args:
        config: ì„¤ì • ê°ì²´ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        
    Returns:
        Gradio Blocks ì¸ìŠ¤í„´ìŠ¤
    """
    # ì„¤ì • ì´ˆê¸°í™”
    if config is None:
        config = Config()
    
    # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    llm_client = LLMClient(config.llm)
    
    # ì„œë²„ ëŒ€ê¸°
    logger.info("Waiting for LLM server...")
    if not llm_client.wait_for_server():
        logger.error("Failed to connect to LLM server")
        raise RuntimeError("LLM server connection failed")
    
    # í–¥ìƒëœ ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    chat_service = EnhancedChatService(config, llm_client)
    
    # Gradio ì¸í„°í˜ì´ìŠ¤
    with gr.Blocks(title=config.app.title, theme=gr.themes.Soft()) as app:
        gr.Markdown(config.app.description)
        
        # ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
        with gr.Row():
            with gr.Column(scale=8):
                # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
                chatbot = gr.Chatbot(
                    label="ëŒ€í™”",
                    height=500,
                    bubble_full_width=False,
                    show_label=True
                )
                
                # ì…ë ¥ ì˜ì—­
                with gr.Row():
                    with gr.Column(scale=4):
                        msg = gr.Textbox(
                            label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
                            placeholder="ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”...",
                            lines=2
                        )
                        image_input = gr.Image(
                            label="ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)",
                            type="pil",
                            height=200
                        )
                    with gr.Column(scale=1):
                        submit = gr.Button("ì „ì†¡", variant="primary")
                        clear = gr.Button("ì´ˆê¸°í™”")
                
                # ì²˜ë¦¬ ìƒíƒœ í‘œì‹œ
                with gr.Accordion("ì²˜ë¦¬ ìƒíƒœ", open=False):
                    status_display = gr.JSON(label="ìµœê·¼ ì²˜ë¦¬ ìƒíƒœ")
            
            # ì‚¬ì´ë“œë°”
            with gr.Column(scale=2):
                # ì˜ˆì œ ì§ˆë¬¸
                gr.Markdown("### ğŸ’¡ ì˜ˆì œ ì§ˆë¬¸")
                examples = gr.Examples(
                    examples=[
                        ["ë‹¤ì‚°ì—ë“€ëŠ” ë„ˆì˜ ì¹œêµ¬ì…ë‹ˆê¹Œ?"],
                        ["íšŒë¡œë„ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜."],
                        ["ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë°ì˜ íŠ¹ì§•ì€?"],
                        ["ì´ ìˆ˜ì‹ì„ í’€ì–´ì¤˜: âˆ«xÂ²dx"],
                        ["í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬ë¥¼ ì¦ëª…í•´ì¤˜."]
                    ],
                    inputs=msg,
                    label="í´ë¦­í•˜ì—¬ ì‚¬ìš©"
                )
                
                # ê¸°ëŠ¥ ì•ˆë‚´
                gr.Markdown("""
                ### ğŸš€ í–¥ìƒëœ ê¸°ëŠ¥
                - **ë‹¤ì¤‘ OCR ì—”ì§„**: í•œêµ­ì–´/ì˜ì–´ í…ìŠ¤íŠ¸ ì •í™•ë„ í–¥ìƒ
                - **ìˆ˜ì‹ ì¸ì‹**: LaTeX ìˆ˜ì‹ ìë™ ê°ì§€ ë° í•´ê²°
                - **ì§€ëŠ¥í˜• ì´ë¯¸ì§€ ë¶„ì„**: ìº¡ì…˜ ìƒì„± ë° ì»¨í…ìŠ¤íŠ¸ ì´í•´
                """)
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        def respond(message: str, image, chat_history: List[Tuple[str, str]]):
            """ë©”ì‹œì§€ ì‘ë‹µ ì²˜ë¦¬"""
            if not message.strip():
                return "", None, chat_history, {}
            
            response, status = chat_service.process_query(message, chat_history, image)
            
            # ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš° ëŒ€í™”ì— í‘œì‹œ
            if image:
                display_msg = f"{message}\n[ì´ë¯¸ì§€ ì²¨ë¶€ë¨]"
                if status.get('formulas_detected', 0) > 0:
                    display_msg += f" (ìˆ˜ì‹ {status['formulas_detected']}ê°œ ê°ì§€)"
                chat_history.append((display_msg, response))
            else:
                chat_history.append((message, response))
            
            return "", None, chat_history, status
        
        def clear_chat():
            """ëŒ€í™” ì´ˆê¸°í™”"""
            chat_service.conversation_history.clear()
            return None, "", None, {}
        
        # ì´ë²¤íŠ¸ ë°”ì¸ë”©
        submit.click(
            respond, 
            [msg, image_input, chatbot], 
            [msg, image_input, chatbot, status_display]
        )
        msg.submit(
            respond, 
            [msg, image_input, chatbot], 
            [msg, image_input, chatbot, status_display]
        )
        clear.click(
            clear_chat, 
            None, 
            [chatbot, msg, image_input, status_display]
        )
        
        # í†µê³„ í‘œì‹œ
        with gr.Accordion("ğŸ“Š ì„œë¹„ìŠ¤ í†µê³„", open=False):
            with gr.Row():
                stats_display = gr.Textbox(
                    label="í†µê³„",
                    interactive=False,
                    lines=6
                )
                
                def update_stats():
                    """í†µê³„ ì—…ë°ì´íŠ¸"""
                    stats = chat_service.rag_system.get_stats()
                    total = stats['total_queries']
                    
                    if total > 0:
                        high_pct = (stats['high_confidence_hits'] / total) * 100
                        medium_pct = (stats['medium_confidence_hits'] / total) * 100
                        low_pct = (stats['low_confidence_hits'] / total) * 100
                    else:
                        high_pct = medium_pct = low_pct = 0
                    
                    # ë©€í‹°ëª¨ë‹¬ í”„ë¡œì„¸ì„œ ìƒíƒœ
                    mm_status = "í™œì„±í™”" if chat_service.multimodal_processor else "ë¹„í™œì„±í™”"
                    
                    return (
                        f"ì´ ì¿¼ë¦¬ ìˆ˜: {total}\n"
                        f"ë†’ì€ ì‹ ë¢°ë„: {stats['high_confidence_hits']} ({high_pct:.1f}%)\n"
                        f"ì¤‘ê°„ ì‹ ë¢°ë„: {stats['medium_confidence_hits']} ({medium_pct:.1f}%)\n"
                        f"ë‚®ì€ ì‹ ë¢°ë„: {stats['low_confidence_hits']} ({low_pct:.1f}%)\n"
                        f"í‰ê·  ì‘ë‹µì‹œê°„: {stats['avg_response_time']:.2f}ì´ˆ\n"
                        f"ë©€í‹°ëª¨ë‹¬ í”„ë¡œì„¸ì„œ: {mm_status}"
                    )
                
                refresh_stats = gr.Button("ìƒˆë¡œê³ ì¹¨", size="sm")
                refresh_stats.click(update_stats, None, stats_display)
                
                # ì´ˆê¸° í†µê³„ í‘œì‹œ
                app.load(update_stats, None, stats_display)
    
    return app


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Enhanced RAG System Gradio UI")
    parser.add_argument(
        "--config", 
        type=str, 
        help="Path to config file (JSON format)"
    )
    parser.add_argument(
        "--server-port", 
        type=int, 
        default=7860,
        help="Server port (default: 7860)"
    )
    parser.add_argument(
        "--share", 
        action="store_true",
        help="Create public Gradio link"
    )
    
    args = parser.parse_args()
    
    # ì„¤ì • ë¡œë“œ
    config = Config()
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ ì˜¤ë²„ë¼ì´ë“œ
    if args.server_port:
        config.app.server_port = args.server_port
    if args.share:
        config.app.share = args.share
    
    # ì•± ìƒì„± ë° ì‹¤í–‰
    try:
        app = create_enhanced_gradio_app(config)
        app.launch(
            server_name=config.app.server_name,
            server_port=config.app.server_port,
            share=config.app.share
        )
    except Exception as e:
        logger.error(f"Failed to launch app: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()