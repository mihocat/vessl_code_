#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© íŒŒì´í”„ë¼ì¸
OpenAI ë¶„ì„ (1íšŒ) â†’ RAG ê²€ìƒ‰ â†’ íŒŒì¸íŠœë‹ LLM â†’ ìµœì¢… ë‹µë³€
"""

import logging
import time
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass
from PIL import Image

from config import Config
from unified_analysis_processor import UnifiedAnalysisProcessor
from rag_system import RAGSystem, SearchResult
from llm_client import LLMClient

logger = logging.getLogger(__name__)

@dataclass
class PipelineResult:
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼"""
    success: bool
    final_answer: str
    analysis_result: Optional[Dict] = None
    rag_results: Optional[List[SearchResult]] = None
    processing_times: Optional[Dict[str, float]] = None
    total_cost: Optional[float] = None
    error_message: Optional[str] = None
    pipeline_steps: Optional[List[str]] = None


class IntegratedPipeline:
    """í†µí•© ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: Config):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            config: ì „ì²´ ì„¤ì • ê°ì²´
        """
        self.config = config
        
        # 1. OpenAI í†µí•© ë¶„ì„ í”„ë¡œì„¸ì„œ (1íšŒ í˜¸ì¶œ ì œí•œ)
        openai_config = {
            'api_key': config.openai.api_key,
            'unified_model': config.openai.unified_model,
            'max_tokens': config.openai.max_tokens,
            'temperature': config.openai.temperature,
            'max_calls_per_query': config.openai.max_calls_per_query
        }
        self.openai_processor = UnifiedAnalysisProcessor(openai_config)
        
        # 2. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (íŒŒì¸íŠœë‹ LLM í´ë¼ì´ì–¸íŠ¸ í•„ìš”)
        try:
            # vLLM ê¸°ë°˜ íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©
            self.llm_client = LLMClient(config.llm)
            self.rag_system = RAGSystem(
                rag_config=config.rag,
                dataset_config=config.dataset,
                llm_client=self.llm_client
            )
            logger.info("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ RAG/LLM ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.llm_client = None
            self.rag_system = None
        
        # ì²˜ë¦¬ í†µê³„
        self.processing_stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'openai_calls': 0,
            'total_cost': 0.0
        }
        
        logger.info("IntegratedPipeline initialized")
    
    def process_query(
        self,
        question: str,
        image: Optional[Union[Image.Image, str, bytes]] = None,
        use_rag: bool = True,
        use_llm: bool = True
    ) -> PipelineResult:
        """
        ì§ˆì˜ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            image: ì„ íƒì  ì´ë¯¸ì§€
            use_rag: RAG ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
            use_llm: íŒŒì¸íŠœë‹ LLM ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            PipelineResult: ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        pipeline_steps = []
        processing_times = {}
        total_cost = 0.0
        
        self.processing_stats['total_queries'] += 1
        self.openai_processor.reset_call_count()  # ì§ˆì˜ë‹¹ í˜¸ì¶œ íšŸìˆ˜ ì´ˆê¸°í™”
        
        try:
            # ========== ë‹¨ê³„ 1: OpenAI í†µí•© ë¶„ì„ (1íšŒ í˜¸ì¶œ) ==========
            step1_start = time.time()
            pipeline_steps.append("OpenAI_Analysis")
            
            logger.info("ğŸ” ë‹¨ê³„ 1: OpenAI GPT-4.1 í†µí•© ë¶„ì„ ì‹œì‘")
            analysis_result = self.openai_processor.analyze_image_and_text(question, image)
            
            if not analysis_result.success:
                return PipelineResult(
                    success=False,
                    final_answer="ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                    error_message=analysis_result.error_message,
                    pipeline_steps=pipeline_steps
                )
            
            processing_times['openai_analysis'] = time.time() - step1_start
            total_cost += analysis_result.cost or 0.0
            self.processing_stats['openai_calls'] += 1
            
            logger.info(f"âœ… OpenAI ë¶„ì„ ì™„ë£Œ - ë¹„ìš©: ${analysis_result.cost:.4f}")
            
            # ========== ë‹¨ê³„ 2: RAG ê²€ìƒ‰ ==========
            rag_results = []
            if use_rag and self.rag_system:
                step2_start = time.time()
                pipeline_steps.append("RAG_Search")
                
                logger.info("ğŸ“š ë‹¨ê³„ 2: RAG ê²€ìƒ‰ ì‹œì‘")
                
                # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (ë¶„ì„ ê²°ê³¼ í™œìš©)
                search_query = question
                if analysis_result.key_concepts:
                    search_query += " " + " ".join(analysis_result.key_concepts[:3])
                if analysis_result.extracted_text:
                    search_query += " " + analysis_result.extracted_text[:200]
                
                rag_results, max_score = self.rag_system.search(search_query)
                processing_times['rag_search'] = time.time() - step2_start
                
                logger.info(f"ğŸ“š RAG ê²€ìƒ‰ ì™„ë£Œ - {len(rag_results)}ê°œ ë¬¸ì„œ ë°œê²¬, ìµœê³ ì ìˆ˜: {max_score:.3f}")
            else:
                logger.warning("âš ï¸ RAG ì‹œìŠ¤í…œ ë¹„í™œì„±í™” ë˜ëŠ” ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # ========== ë‹¨ê³„ 3: íŒŒì¸íŠœë‹ LLM ìµœì¢… ë‹µë³€ ìƒì„± ==========
            if use_llm and self.llm_client:
                step3_start = time.time()
                pipeline_steps.append("LLM_Response")
                
                logger.info("ğŸ¤– ë‹¨ê³„ 3: íŒŒì¸íŠœë‹ LLM ë‹µë³€ ìƒì„± ì‹œì‘")
                
                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                context = self._build_context(analysis_result, rag_results, question)
                
                # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                prompt = self._build_prompt(context, question, analysis_result)
                
                # LLM ë‹µë³€ ìƒì„±
                final_answer = self.llm_client.generate_response(prompt)
                processing_times['llm_generation'] = time.time() - step3_start
                
                logger.info("ğŸ¤– íŒŒì¸íŠœë‹ LLM ë‹µë³€ ìƒì„± ì™„ë£Œ")
                
            else:
                logger.error("âŒ íŒŒì¸íŠœë‹ LLM ì‚¬ìš© ë¶ˆê°€")
                final_answer = "íŒŒì¸íŠœë‹ëœ LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            
            # ========== ê²°ê³¼ ì •ë¦¬ ==========
            total_time = time.time() - start_time
            processing_times['total'] = total_time
            
            self.processing_stats['successful_queries'] += 1
            self.processing_stats['total_cost'] += total_cost
            
            logger.info(f"âœ… í†µí•© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ - ì´ ì‹œê°„: {total_time:.2f}s, ë¹„ìš©: ${total_cost:.4f}")
            
            return PipelineResult(
                success=True,
                final_answer=final_answer,
                analysis_result={
                    'extracted_text': analysis_result.extracted_text,
                    'formulas': analysis_result.formulas,
                    'key_concepts': analysis_result.key_concepts,
                    'question_intent': analysis_result.question_intent,
                    'token_usage': analysis_result.token_usage,
                    'cost': analysis_result.cost
                },
                rag_results=rag_results,
                processing_times=processing_times,
                total_cost=total_cost,
                pipeline_steps=pipeline_steps
            )
            
        except Exception as e:
            self.processing_stats['failed_queries'] += 1
            total_time = time.time() - start_time
            
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return PipelineResult(
                success=False,
                final_answer="ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                error_message=str(e),
                processing_times={'total': total_time},
                pipeline_steps=pipeline_steps
            )
    
    def _build_context(
        self, 
        analysis_result, 
        rag_results: List[SearchResult], 
        question: str
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        
        # OpenAI ë¶„ì„ ê²°ê³¼
        if analysis_result.extracted_text:
            context_parts.append(f"ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:\n{analysis_result.extracted_text}")
        
        if analysis_result.formulas:
            context_parts.append(f"ê°ì§€ëœ ìˆ˜ì‹:\n" + "\n".join(analysis_result.formulas))
        
        if analysis_result.key_concepts:
            context_parts.append(f"í•µì‹¬ ê°œë…:\n" + ", ".join(analysis_result.key_concepts))
        
        # RAG ê²€ìƒ‰ ê²°ê³¼
        if rag_results:
            rag_context = []
            for i, result in enumerate(rag_results[:5], 1):  # ìƒìœ„ 5ê°œë§Œ
                rag_context.append(f"ì°¸ê³ ìë£Œ {i}: {result.content[:500]}...")
            if rag_context:
                context_parts.append("ê´€ë ¨ ì „ë¬¸ ìë£Œ:\n" + "\n".join(rag_context))
        
        return "\n\n".join(context_parts)
    
    def _build_prompt(self, context: str, question: str, analysis_result) -> str:
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì „ë¬¸ì ì´ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

{context}

ë‹µë³€ ì§€ì¹¨:
1. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”
2. ì „ê¸°ê³µí•™ ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”
3. ìˆ˜ì‹ì´ ìˆìœ¼ë©´ LaTeX í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”
4. ë‹¨ê³„ë³„ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
5. ì‹¤ë¬´ì— ë„ì›€ì´ ë˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”

ë‹µë³€:"""
        
        return prompt
    
    def get_statistics(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        success_rate = 0.0
        if self.processing_stats['total_queries'] > 0:
            success_rate = self.processing_stats['successful_queries'] / self.processing_stats['total_queries']
        
        avg_cost = 0.0
        if self.processing_stats['successful_queries'] > 0:
            avg_cost = self.processing_stats['total_cost'] / self.processing_stats['successful_queries']
        
        return {
            **self.processing_stats,
            'success_rate': success_rate,
            'average_cost_per_query': avg_cost,
            'openai_call_efficiency': f"{self.processing_stats['openai_calls']}/{self.processing_stats['total_queries']} (1:1 ëª©í‘œ)"
        }
    
    def health_check(self) -> Dict[str, bool]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        status = {
            'openai_processor': False,
            'rag_system': False,
            'llm_client': False
        }
        
        # OpenAI í”„ë¡œì„¸ì„œ í™•ì¸
        try:
            stats = self.openai_processor.get_call_statistics()
            status['openai_processor'] = True
        except Exception as e:
            logger.error(f"OpenAI processor health check failed: {e}")
        
        # RAG ì‹œìŠ¤í…œ í™•ì¸
        if self.rag_system:
            try:
                # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                results = self.rag_system.search("í…ŒìŠ¤íŠ¸", k=1)
                status['rag_system'] = True
            except Exception as e:
                logger.error(f"RAG system health check failed: {e}")
        
        # LLM í´ë¼ì´ì–¸íŠ¸ í™•ì¸
        if self.llm_client:
            try:
                # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
                response = self.llm_client.generate_response("ì•ˆë…•í•˜ì„¸ìš”", max_tokens=10)
                status['llm_client'] = True
            except Exception as e:
                logger.error(f"LLM client health check failed: {e}")
        
        return status


def create_integrated_pipeline(config: Config) -> IntegratedPipeline:
    """í†µí•© íŒŒì´í”„ë¼ì¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return IntegratedPipeline(config)