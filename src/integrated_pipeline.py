#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© íŒŒì´í”„ë¼ì¸
OpenAI ë¶„ì„ (1íšŒ) â†’ RAG ê²€ìƒ‰ â†’ íŒŒì¸íŠœë‹ LLM â†’ ìµœì¢… ë‹µë³€
"""

import os
import logging
import time
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass
from PIL import Image

from config import Config
from unified_analysis_processor import UnifiedAnalysisProcessor
from rag_system import RAGSystem, SearchResult
from llm_client import LLMClient

logger = logging.getLogger(__name__)

@dataclass
class PipelineResult:
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼"""
    success: bool
    final_answer: str
    analysis_result: Optional[Dict] = None
    rag_results: Optional[List[SearchResult]] = None
    processing_times: Optional[Dict[str, float]] = None
    total_cost: Optional[float] = None
    error_message: Optional[str] = None
    pipeline_steps: Optional[List[str]] = None


class IntegratedPipeline:
    """í†µí•© ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: Config):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            config: ì „ì²´ ì„¤ì • ê°ì²´
        """
        self.config = config
        
        # 1. OpenAI í†µí•© ë¶„ì„ í”„ë¡œì„¸ì„œ (1íšŒ í˜¸ì¶œ ì œí•œ)
        openai_config = {
            'api_key': config.openai.api_key,
            'unified_model': config.openai.unified_model,
            'max_tokens': config.openai.max_tokens,
            'temperature': config.openai.temperature,
            'max_calls_per_query': config.openai.max_calls_per_query
        }
        self.openai_processor = UnifiedAnalysisProcessor(openai_config)
        
        # 2. RAG ì‹œìŠ¤í…œ ë° LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (SKIP_VLLM í™•ì¸)
        skip_vllm = os.getenv("SKIP_VLLM", "false").lower() == "true"
        logger.info(f"ğŸ”§ SKIP_VLLM í™˜ê²½ë³€ìˆ˜: {skip_vllm}")
        
        if skip_vllm:
            logger.info("âš ï¸ SKIP_VLLM=true - íŒŒì¸íŠœë‹ LLM ë° RAG ì‹œìŠ¤í…œ ë¹„í™œì„±í™”")
            self.llm_client = None
            self.rag_system = None
        else:
            try:
                # vLLM ê¸°ë°˜ íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©
                logger.info("ğŸ”§ vLLM ê¸°ë°˜ íŒŒì¸íŠœë‹ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
                self.llm_client = LLMClient(config.llm)
                self.rag_system = RAGSystem(
                    rag_config=config.rag,
                    dataset_config=config.dataset,
                    llm_client=self.llm_client
                )
                logger.info("âœ… RAG ì‹œìŠ¤í…œ ë° LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ RAG/LLM ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.llm_client = None
                self.rag_system = None
        
        # ì²˜ë¦¬ í†µê³„
        self.processing_stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'openai_calls': 0,
            'total_cost': 0.0
        }
        
        logger.info("IntegratedPipeline initialized")
    
    def process_query(
        self,
        question: str,
        image: Optional[Union[Image.Image, str, bytes]] = None,
        use_rag: bool = True,
        use_llm: bool = True
    ) -> PipelineResult:
        """
        ì§ˆì˜ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            image: ì„ íƒì  ì´ë¯¸ì§€
            use_rag: RAG ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
            use_llm: íŒŒì¸íŠœë‹ LLM ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            PipelineResult: ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        pipeline_steps = []
        processing_times = {}
        total_cost = 0.0
        
        self.processing_stats['total_queries'] += 1
        self.openai_processor.reset_call_count()  # ì§ˆì˜ë‹¹ í˜¸ì¶œ íšŸìˆ˜ ì´ˆê¸°í™”
        
        try:
            analysis_result = None
            rag_results = []
            
            # ========== ì´ë¯¸ì§€ ìœ ë¬´ì— ë”°ë¥¸ íŒŒì´í”„ë¼ì¸ ë¶„ê¸° ==========
            if image is not None:
                # ê²½ë¡œ 1: ì´ë¯¸ì§€ í¬í•¨ â†’ OpenAI â†’ RAG â†’ LLM
                logger.info("ğŸ–¼ï¸ ì´ë¯¸ì§€ í¬í•¨ ì§ˆì˜ - OpenAI â†’ RAG â†’ LLM íŒŒì´í”„ë¼ì¸ ì‹œì‘")
                
                # ë‹¨ê³„ 1: OpenAI í†µí•© ë¶„ì„ (ì´ë¯¸ì§€ ì²˜ë¦¬)
                step1_start = time.time()
                pipeline_steps.append("OpenAI_Analysis")
                
                logger.info("ğŸ” ë‹¨ê³„ 1: OpenAI GPT-5 ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ í†µí•© ë¶„ì„ ì‹œì‘")
                analysis_result = self.openai_processor.analyze_image_and_text(question, image)
                
                if not analysis_result.success:
                    return PipelineResult(
                        success=False,
                        final_answer="ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                        error_message=analysis_result.error_message,
                        pipeline_steps=pipeline_steps
                    )
                
                processing_times['openai_analysis'] = time.time() - step1_start
                total_cost += analysis_result.cost or 0.0
                self.processing_stats['openai_calls'] += 1
                
                logger.info(f"âœ… OpenAI ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ - ë¹„ìš©: ${analysis_result.cost:.4f}")
                
                # ë‹¨ê³„ 2: RAG ê²€ìƒ‰ (OpenAI ë¶„ì„ ê²°ê³¼ í™œìš©)
                if use_rag and self.rag_system:
                    step2_start = time.time()
                    pipeline_steps.append("RAG_Search")
                    
                    logger.info("ğŸ“š ë‹¨ê³„ 2: RAG ê²€ìƒ‰ ì‹œì‘ (OpenAI ë¶„ì„ ê²°ê³¼ í™œìš©)")
                    
                    # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (OpenAI ë¶„ì„ ê²°ê³¼ í™œìš©)
                    search_query = question
                    if analysis_result.key_concepts:
                        search_query += " " + " ".join(analysis_result.key_concepts[:3])
                    if analysis_result.extracted_text:
                        search_query += " " + analysis_result.extracted_text[:200]
                    
                    rag_results, max_score = self.rag_system.search(search_query)
                    processing_times['rag_search'] = time.time() - step2_start
                    
                    logger.info(f"ğŸ“š RAG ê²€ìƒ‰ ì™„ë£Œ - {len(rag_results)}ê°œ ë¬¸ì„œ ë°œê²¬, ìµœê³ ì ìˆ˜: {max_score:.3f}")
                else:
                    logger.warning("âš ï¸ RAG ì‹œìŠ¤í…œ ë¹„í™œì„±í™” ë˜ëŠ” ì´ˆê¸°í™” ì‹¤íŒ¨")
                    
            else:
                # ê²½ë¡œ 2: í…ìŠ¤íŠ¸ë§Œ â†’ RAG â†’ LLM (OpenAI ê±´ë„ˆë›°ê¸°)
                logger.info("ğŸ“ í…ìŠ¤íŠ¸ ì „ìš© ì§ˆì˜ - RAG â†’ LLM íŒŒì´í”„ë¼ì¸ ì‹œì‘")
                
                # ë‹¨ê³„ 1: RAG ê²€ìƒ‰ (í…ìŠ¤íŠ¸ ê¸°ë°˜)
                if use_rag and self.rag_system:
                    step1_start = time.time()
                    pipeline_steps.append("RAG_Search")
                    
                    logger.info("ğŸ” ë‹¨ê³„ 1: RAG ê²€ìƒ‰ ì‹œì‘ (í…ìŠ¤íŠ¸ ì „ìš©)")
                    
                    # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (ì§ˆë¬¸ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©)
                    search_query = question
                    
                    rag_results, max_score = self.rag_system.search(search_query)
                    processing_times['rag_search'] = time.time() - step1_start
                    
                    logger.info(f"ğŸ“š RAG ê²€ìƒ‰ ì™„ë£Œ - {len(rag_results)}ê°œ ë¬¸ì„œ ë°œê²¬, ìµœê³ ì ìˆ˜: {max_score:.3f}")
                else:
                    logger.warning("âš ï¸ RAG ì‹œìŠ¤í…œ ë¹„í™œì„±í™” ë˜ëŠ” ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # ========== ìµœì¢… ë‹¨ê³„: íŒŒì¸íŠœë‹ LLM ë‹µë³€ ìƒì„± ==========
            if use_llm and self.llm_client:
                llm_step_start = time.time()
                pipeline_steps.append("LLM_Response")
                
                # íŒŒì´í”„ë¼ì¸ ê²½ë¡œì— ë”°ë¥¸ ë¡œê¹…
                if image is not None:
                    logger.info("ğŸ¤– ë‹¨ê³„ 3: íŒŒì¸íŠœë‹ LLM ë‹µë³€ ìƒì„± ì‹œì‘ (ì´ë¯¸ì§€+í…ìŠ¤íŠ¸)")
                else:
                    logger.info("ğŸ¤– ë‹¨ê³„ 2: íŒŒì¸íŠœë‹ LLM ë‹µë³€ ìƒì„± ì‹œì‘ (í…ìŠ¤íŠ¸ ì „ìš©)")
                
                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                context = self._build_context(analysis_result, rag_results, question)
                
                # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                prompt = self._build_prompt(context, question, analysis_result)
                
                # LLM ë‹µë³€ ìƒì„± ì‹œë„
                final_answer = self.llm_client.generate_response(
                    question=question,
                    context=context
                )
                processing_times['llm_generation'] = time.time() - llm_step_start
                
                # LLM ì—°ê²° ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ (íŒŒì´í”„ë¼ì¸ ê²½ë¡œë³„ë¡œ ë‹¤ë¥¸ ë©”ì‹œì§€)
                if "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in final_answer:
                    logger.error("âŒ LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨ - íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€")
                    
                    if image is not None:
                        # ì´ë¯¸ì§€ í¬í•¨ ì§ˆì˜ ì‹¤íŒ¨
                        final_answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ì²˜ë¦¬ ì™„ë£Œëœ ë‹¨ê³„:**
- OpenAI ì´ë¯¸ì§€ ë¶„ì„: ì™„ë£Œ
- RAG ë¬¸ì„œ ê²€ìƒ‰: {len(rag_results)}ê°œ ê²°ê³¼
- íŒŒì¸íŠœë‹ LLM: ì—°ê²° ì‹¤íŒ¨"""
                    else:
                        # í…ìŠ¤íŠ¸ ì „ìš© ì§ˆì˜ ì‹¤íŒ¨
                        final_answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ì²˜ë¦¬ ì™„ë£Œëœ ë‹¨ê³„:**
- RAG ë¬¸ì„œ ê²€ìƒ‰: {len(rag_results)}ê°œ ê²°ê³¼  
- íŒŒì¸íŠœë‹ LLM: ì—°ê²° ì‹¤íŒ¨

ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì‹œê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."""
                
                logger.info("ğŸ¤– ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ")
                
            else:
                logger.info("âš ï¸ íŒŒì¸íŠœë‹ LLM ë‹¨ê³„ ê±´ë„ˆë›°ê¸° - ê¸°ë³¸ ë‹µë³€ ì œê³µ")
                
                # íŒŒì´í”„ë¼ì¸ ê²½ë¡œë³„ fallback ë‹µë³€ êµ¬ì„±
                if image is not None:
                    # ì´ë¯¸ì§€ í¬í•¨ ì§ˆì˜ - OpenAI ë¶„ì„ ê²°ê³¼ í™œìš©
                    if analysis_result and hasattr(analysis_result, 'extracted_text') and analysis_result.extracted_text:
                        final_answer = f"""**OpenAI GPT-5 ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:**

{analysis_result.extracted_text}

**ì°¸ê³ ì‚¬í•­:**
- í˜„ì¬ íŒŒì¸íŠœë‹ëœ ì „ë¬¸ ëª¨ë¸ì€ ë¹„í™œì„±í™” ìƒíƒœì…ë‹ˆë‹¤
- ìœ„ ë‹µë³€ì€ OpenAI GPT-5 ì˜ ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤
- ë” ì „ë¬¸ì ì¸ ë‹µë³€ì´ í•„ìš”í•˜ì‹œë©´ ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”"""
                    else:
                        final_answer = f"""**ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:**

ì§ˆë¬¸: {question[:100]}{'...' if len(question) > 100 else ''}

í˜„ì¬ íŒŒì¸íŠœë‹ëœ ì „ë¬¸ ëª¨ë¸ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ ìƒì„¸í•œ ì „ë¬¸ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ì²˜ë¦¬ ì™„ë£Œëœ ë‹¨ê³„:**
- OpenAI ì´ë¯¸ì§€ ë¶„ì„: ì™„ë£Œ
- RAG ê²€ìƒ‰: {len(rag_results)}ê°œ ë¬¸ì„œ
- íŒŒì¸íŠœë‹ LLM: ë¹„í™œì„±í™”ë¨"""
                else:
                    # í…ìŠ¤íŠ¸ ì „ìš© ì§ˆì˜ - RAG ê²°ê³¼ ê¸°ë°˜
                    final_answer = f"""**í…ìŠ¤íŠ¸ ì§ˆì˜ ì²˜ë¦¬ ê²°ê³¼:**

ì§ˆë¬¸: {question[:100]}{'...' if len(question) > 100 else ''}

í˜„ì¬ íŒŒì¸íŠœë‹ëœ ì „ë¬¸ ëª¨ë¸ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ ìƒì„¸í•œ ì „ë¬¸ ë¶„ì„ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ì²˜ë¦¬ ì™„ë£Œëœ ë‹¨ê³„:**
- RAG ê²€ìƒ‰: {len(rag_results)}ê°œ ë¬¸ì„œ
- íŒŒì¸íŠœë‹ LLM: ë¹„í™œì„±í™”ë¨

ë³´ë‹¤ ì „ë¬¸ì ì¸ ë‹µë³€ì´ í•„ìš”í•˜ì‹œë©´ ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”."""
            
            # ========== ê²°ê³¼ ì •ë¦¬ ==========
            total_time = time.time() - start_time
            processing_times['total'] = total_time
            
            self.processing_stats['successful_queries'] += 1
            self.processing_stats['total_cost'] += total_cost
            
            logger.info(f"âœ… í†µí•© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ - ì´ ì‹œê°„: {total_time:.2f}s, ë¹„ìš©: ${total_cost:.4f}")
            
            # ë¶„ì„ ê²°ê³¼ êµ¬ì„± (ì´ë¯¸ì§€ í¬í•¨ ì§ˆì˜ì¸ ê²½ìš°ì—ë§Œ)
            analysis_dict = None
            if analysis_result is not None:
                analysis_dict = {
                    'extracted_text': getattr(analysis_result, 'extracted_text', ''),
                    'formulas': getattr(analysis_result, 'formulas', []),
                    'key_concepts': getattr(analysis_result, 'key_concepts', []),
                    'question_intent': getattr(analysis_result, 'question_intent', ''),
                    'token_usage': getattr(analysis_result, 'token_usage', {}),
                    'cost': getattr(analysis_result, 'cost', 0.0)
                }
            
            return PipelineResult(
                success=True,
                final_answer=final_answer,
                analysis_result=analysis_dict,
                rag_results=rag_results,
                processing_times=processing_times,
                total_cost=total_cost,
                pipeline_steps=pipeline_steps
            )
            
        except Exception as e:
            self.processing_stats['failed_queries'] += 1
            total_time = time.time() - start_time
            
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return PipelineResult(
                success=False,
                final_answer="ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                error_message=str(e),
                processing_times={'total': total_time},
                pipeline_steps=pipeline_steps
            )
    
    def _build_context(
        self, 
        analysis_result, 
        rag_results: List[SearchResult], 
        question: str
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - í† í° ì œí•œ ê³ ë ¤"""
        context_parts = []
        max_total_length = 1500  # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´
        current_length = 0
        
        # OpenAI ë¶„ì„ ê²°ê³¼ (ì´ë¯¸ì§€ í¬í•¨ ì§ˆì˜ì¸ ê²½ìš°ì—ë§Œ ì¡´ì¬)
        if analysis_result is not None:
            if hasattr(analysis_result, 'extracted_text') and analysis_result.extracted_text:
                text = f"ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:\n{analysis_result.extracted_text[:300]}"
                context_parts.append(text)
                current_length += len(text)
            
            if hasattr(analysis_result, 'formulas') and analysis_result.formulas:
                formulas = analysis_result.formulas[:3]  # ìµœëŒ€ 3ê°œ ìˆ˜ì‹ë§Œ
                text = f"ê°ì§€ëœ ìˆ˜ì‹:\n" + "\n".join(formulas)
                if current_length + len(text) < max_total_length:
                    context_parts.append(text)
                    current_length += len(text)
            
            if hasattr(analysis_result, 'key_concepts') and analysis_result.key_concepts:
                concepts = analysis_result.key_concepts[:5]  # ìµœëŒ€ 5ê°œ ê°œë…ë§Œ
                text = f"í•µì‹¬ ê°œë…:\n" + ", ".join(concepts)
                if current_length + len(text) < max_total_length:
                    context_parts.append(text)
                    current_length += len(text)
        
        # RAG ê²€ìƒ‰ ê²°ê³¼
        if rag_results:
            rag_context = []
            remaining_length = max_total_length - current_length
            per_result_length = min(300, remaining_length // min(3, len(rag_results)))
            
            for i, result in enumerate(rag_results[:3], 1):  # ìƒìœ„ 3ê°œë§Œ
                # SearchResult í´ë˜ìŠ¤ì˜ ì˜¬ë°”ë¥¸ ì†ì„± ì‚¬ìš©: answer (contentê°€ ì•„ë‹˜)
                truncated_answer = result.answer[:per_result_length]
                if len(result.answer) > per_result_length:
                    truncated_answer += "..."
                rag_context.append(f"ì°¸ê³ ìë£Œ {i}: {truncated_answer}")
                
            if rag_context:
                context_parts.append("ê´€ë ¨ ì „ë¬¸ ìë£Œ:\n" + "\n".join(rag_context))
        
        final_context = "\n\n".join(context_parts)
        
        # ìµœì¢… ê¸¸ì´ í™•ì¸
        if len(final_context) > max_total_length:
            final_context = final_context[:max_total_length] + "..."
            logger.warning(f"ì»¨í…ìŠ¤íŠ¸ê°€ {max_total_length}ìë¡œ ì œí•œë¨")
        
        return final_context
    
    def _build_prompt(self, context: str, question: str, analysis_result) -> str:
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì „ë¬¸ì ì´ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

{context}

ë‹µë³€ ì§€ì¹¨:
1. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”
2. ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”
3. ìˆ˜ì‹ì´ ìˆìœ¼ë©´ LaTeX í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”
4. ë‹¨ê³„ë³„ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
5. ì‹¤ë¬´ì— ë„ì›€ì´ ë˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”

ë‹µë³€:"""
        
        return prompt
    
    def get_statistics(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        success_rate = 0.0
        if self.processing_stats['total_queries'] > 0:
            success_rate = self.processing_stats['successful_queries'] / self.processing_stats['total_queries']
        
        avg_cost = 0.0
        if self.processing_stats['successful_queries'] > 0:
            avg_cost = self.processing_stats['total_cost'] / self.processing_stats['successful_queries']
        
        return {
            **self.processing_stats,
            'success_rate': success_rate,
            'average_cost_per_query': avg_cost,
            'openai_call_efficiency': f"{self.processing_stats['openai_calls']}/{self.processing_stats['total_queries']} (1:1 ëª©í‘œ)"
        }
    
    def health_check(self) -> Dict[str, bool]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        status = {
            'openai_processor': False,
            'rag_system': False,
            'llm_client': False
        }
        
        # OpenAI í”„ë¡œì„¸ì„œ í™•ì¸
        try:
            stats = self.openai_processor.get_call_statistics()
            status['openai_processor'] = True
        except Exception as e:
            logger.error(f"OpenAI processor health check failed: {e}")
        
        # RAG ì‹œìŠ¤í…œ í™•ì¸
        if self.rag_system:
            try:
                # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                results = self.rag_system.search("í…ŒìŠ¤íŠ¸", k=1)
                status['rag_system'] = True
            except Exception as e:
                logger.error(f"RAG system health check failed: {e}")
        
        # LLM í´ë¼ì´ì–¸íŠ¸ í™•ì¸
        if self.llm_client:
            try:
                # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
                response = self.llm_client.generate_response(
                    question="ì•ˆë…•í•˜ì„¸ìš”", 
                    max_tokens=10
                )
                # ì‹¤ì œ ì‘ë‹µì´ ìˆê³  ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ì•„ë‹Œì§€ í™•ì¸
                if response and "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" not in response:
                    status['llm_client'] = True
                else:
                    status['llm_client'] = False
            except Exception as e:
                logger.error(f"LLM client health check failed: {e}")
                status['llm_client'] = False
        
        return status
    


def create_integrated_pipeline(config: Config) -> IntegratedPipeline:
    """Create integrated pipeline convenience function"""
    return IntegratedPipeline(config)