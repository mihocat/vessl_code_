#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© íŒŒì´í”„ë¼ì¸
OpenAI ë¶„ì„ (1íšŒ) â†’ RAG ê²€ìƒ‰ â†’ íŒŒì¸íŠœë‹ LLM â†’ ìµœì¢… ë‹µë³€
"""

import logging
import time
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass
from PIL import Image

from config import Config
from unified_analysis_processor import UnifiedAnalysisProcessor
from rag_system import RAGSystem, SearchResult
from llm_client import LLMClient

logger = logging.getLogger(__name__)

@dataclass
class PipelineResult:
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼"""
    success: bool
    final_answer: str
    analysis_result: Optional[Dict] = None
    rag_results: Optional[List[SearchResult]] = None
    processing_times: Optional[Dict[str, float]] = None
    total_cost: Optional[float] = None
    error_message: Optional[str] = None
    pipeline_steps: Optional[List[str]] = None


class IntegratedPipeline:
    """í†µí•© ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: Config):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            config: ì „ì²´ ì„¤ì • ê°ì²´
        """
        self.config = config
        
        # 1. OpenAI í†µí•© ë¶„ì„ í”„ë¡œì„¸ì„œ (1íšŒ í˜¸ì¶œ ì œí•œ)
        openai_config = {
            'api_key': config.openai.api_key,
            'unified_model': config.openai.unified_model,
            'max_tokens': config.openai.max_tokens,
            'temperature': config.openai.temperature,
            'max_calls_per_query': config.openai.max_calls_per_query
        }
        self.openai_processor = UnifiedAnalysisProcessor(openai_config)
        
        # 2. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (íŒŒì¸íŠœë‹ LLM í´ë¼ì´ì–¸íŠ¸ í•„ìš”)
        try:
            # vLLM ê¸°ë°˜ íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©
            self.llm_client = LLMClient(config.llm)
            self.rag_system = RAGSystem(
                rag_config=config.rag,
                dataset_config=config.dataset,
                llm_client=self.llm_client
            )
            logger.info("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ RAG/LLM ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.llm_client = None
            self.rag_system = None
        
        # ì²˜ë¦¬ í†µê³„
        self.processing_stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'openai_calls': 0,
            'total_cost': 0.0
        }
        
        logger.info("IntegratedPipeline initialized")
    
    def process_query(
        self,
        question: str,
        image: Optional[Union[Image.Image, str, bytes]] = None,
        use_rag: bool = True,
        use_llm: bool = True
    ) -> PipelineResult:
        """
        ì§ˆì˜ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            image: ì„ íƒì  ì´ë¯¸ì§€
            use_rag: RAG ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
            use_llm: íŒŒì¸íŠœë‹ LLM ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            PipelineResult: ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        pipeline_steps = []
        processing_times = {}
        total_cost = 0.0
        
        self.processing_stats['total_queries'] += 1
        self.openai_processor.reset_call_count()  # ì§ˆì˜ë‹¹ í˜¸ì¶œ íšŸìˆ˜ ì´ˆê¸°í™”
        
        try:
            # ========== ë‹¨ê³„ 1: OpenAI í†µí•© ë¶„ì„ (1íšŒ í˜¸ì¶œ) ==========
            step1_start = time.time()
            pipeline_steps.append("OpenAI_Analysis")
            
            logger.info("ğŸ” ë‹¨ê³„ 1: OpenAI GPT-4.1 í†µí•© ë¶„ì„ ì‹œì‘")
            analysis_result = self.openai_processor.analyze_image_and_text(question, image)
            
            if not analysis_result.success:
                return PipelineResult(
                    success=False,
                    final_answer="ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                    error_message=analysis_result.error_message,
                    pipeline_steps=pipeline_steps
                )
            
            processing_times['openai_analysis'] = time.time() - step1_start
            total_cost += analysis_result.cost or 0.0
            self.processing_stats['openai_calls'] += 1
            
            logger.info(f"âœ… OpenAI ë¶„ì„ ì™„ë£Œ - ë¹„ìš©: ${analysis_result.cost:.4f}")
            
            # ========== ë‹¨ê³„ 2: RAG ê²€ìƒ‰ ==========
            rag_results = []
            if use_rag and self.rag_system:
                step2_start = time.time()
                pipeline_steps.append("RAG_Search")
                
                logger.info("ğŸ“š ë‹¨ê³„ 2: RAG ê²€ìƒ‰ ì‹œì‘")
                
                # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (ë¶„ì„ ê²°ê³¼ í™œìš©)
                search_query = question
                if analysis_result.key_concepts:
                    search_query += " " + " ".join(analysis_result.key_concepts[:3])
                if analysis_result.extracted_text:
                    search_query += " " + analysis_result.extracted_text[:200]
                
                rag_results, max_score = self.rag_system.search(search_query)
                processing_times['rag_search'] = time.time() - step2_start
                
                logger.info(f"ğŸ“š RAG ê²€ìƒ‰ ì™„ë£Œ - {len(rag_results)}ê°œ ë¬¸ì„œ ë°œê²¬, ìµœê³ ì ìˆ˜: {max_score:.3f}")
            else:
                logger.warning("âš ï¸ RAG ì‹œìŠ¤í…œ ë¹„í™œì„±í™” ë˜ëŠ” ì´ˆê¸°í™” ì‹¤íŒ¨")
            
            # ========== ë‹¨ê³„ 3: íŒŒì¸íŠœë‹ LLM ìµœì¢… ë‹µë³€ ìƒì„± ==========
            if use_llm and self.llm_client:
                step3_start = time.time()
                pipeline_steps.append("LLM_Response")
                
                logger.info("ğŸ¤– ë‹¨ê³„ 3: íŒŒì¸íŠœë‹ LLM ë‹µë³€ ìƒì„± ì‹œì‘")
                
                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                context = self._build_context(analysis_result, rag_results, question)
                
                # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                prompt = self._build_prompt(context, question, analysis_result)
                
                # LLM ë‹µë³€ ìƒì„± ì‹œë„
                final_answer = self.llm_client.generate_response(prompt)
                processing_times['llm_generation'] = time.time() - step3_start
                
                # LLM ì—°ê²° ì‹¤íŒ¨ ì‹œ OpenAI ì‘ë‹µ í™œìš©í•˜ì—¬ í´ë°±
                if "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in final_answer or len(final_answer) < 50:
                    logger.warning("ğŸ”„ LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨ - OpenAI ì‘ë‹µìœ¼ë¡œ í´ë°± ì§„í–‰")
                    pipeline_steps.append("OpenAI_Fallback")
                    
                    fallback_answer = self._generate_openai_fallback_answer(analysis_result, rag_results, question)
                    if fallback_answer and len(fallback_answer) > 50:
                        final_answer = fallback_answer
                        logger.info("âœ… OpenAI í´ë°± ë‹µë³€ ìƒì„± ì™„ë£Œ")
                    else:
                        logger.warning("âš ï¸ OpenAI í´ë°±ë„ ì‹¤íŒ¨ - ê¸°ë³¸ ë‹µë³€ ì‚¬ìš©")
                
                logger.info("ğŸ¤– ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ")
                
            else:
                logger.warning("âŒ íŒŒì¸íŠœë‹ LLM ì‚¬ìš© ë¶ˆê°€ - OpenAI ì „ìš© ëª¨ë“œë¡œ ì „í™˜")
                pipeline_steps.append("OpenAI_Only")
                final_answer = self._generate_openai_fallback_answer(analysis_result, rag_results, question)
            
            # ========== ê²°ê³¼ ì •ë¦¬ ==========
            total_time = time.time() - start_time
            processing_times['total'] = total_time
            
            self.processing_stats['successful_queries'] += 1
            self.processing_stats['total_cost'] += total_cost
            
            logger.info(f"âœ… í†µí•© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ - ì´ ì‹œê°„: {total_time:.2f}s, ë¹„ìš©: ${total_cost:.4f}")
            
            return PipelineResult(
                success=True,
                final_answer=final_answer,
                analysis_result={
                    'extracted_text': analysis_result.extracted_text,
                    'formulas': analysis_result.formulas,
                    'key_concepts': analysis_result.key_concepts,
                    'question_intent': analysis_result.question_intent,
                    'token_usage': analysis_result.token_usage,
                    'cost': analysis_result.cost
                },
                rag_results=rag_results,
                processing_times=processing_times,
                total_cost=total_cost,
                pipeline_steps=pipeline_steps
            )
            
        except Exception as e:
            self.processing_stats['failed_queries'] += 1
            total_time = time.time() - start_time
            
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return PipelineResult(
                success=False,
                final_answer="ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                error_message=str(e),
                processing_times={'total': total_time},
                pipeline_steps=pipeline_steps
            )
    
    def _build_context(
        self, 
        analysis_result, 
        rag_results: List[SearchResult], 
        question: str
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        
        # OpenAI ë¶„ì„ ê²°ê³¼
        if analysis_result.extracted_text:
            context_parts.append(f"ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:\n{analysis_result.extracted_text}")
        
        if analysis_result.formulas:
            context_parts.append(f"ê°ì§€ëœ ìˆ˜ì‹:\n" + "\n".join(analysis_result.formulas))
        
        if analysis_result.key_concepts:
            context_parts.append(f"í•µì‹¬ ê°œë…:\n" + ", ".join(analysis_result.key_concepts))
        
        # RAG ê²€ìƒ‰ ê²°ê³¼
        if rag_results:
            rag_context = []
            for i, result in enumerate(rag_results[:5], 1):  # ìƒìœ„ 5ê°œë§Œ
                rag_context.append(f"ì°¸ê³ ìë£Œ {i}: {result.content[:500]}...")
            if rag_context:
                context_parts.append("ê´€ë ¨ ì „ë¬¸ ìë£Œ:\n" + "\n".join(rag_context))
        
        return "\n\n".join(context_parts)
    
    def _build_prompt(self, context: str, question: str, analysis_result) -> str:
        """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì „ë¬¸ì ì´ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

{context}

ë‹µë³€ ì§€ì¹¨:
1. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”
2. ì „ê¸°ê³µí•™ ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”
3. ìˆ˜ì‹ì´ ìˆìœ¼ë©´ LaTeX í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•˜ì„¸ìš”
4. ë‹¨ê³„ë³„ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
5. ì‹¤ë¬´ì— ë„ì›€ì´ ë˜ëŠ” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”

ë‹µë³€:"""
        
        return prompt
    
    def get_statistics(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        success_rate = 0.0
        if self.processing_stats['total_queries'] > 0:
            success_rate = self.processing_stats['successful_queries'] / self.processing_stats['total_queries']
        
        avg_cost = 0.0
        if self.processing_stats['successful_queries'] > 0:
            avg_cost = self.processing_stats['total_cost'] / self.processing_stats['successful_queries']
        
        return {
            **self.processing_stats,
            'success_rate': success_rate,
            'average_cost_per_query': avg_cost,
            'openai_call_efficiency': f"{self.processing_stats['openai_calls']}/{self.processing_stats['total_queries']} (1:1 ëª©í‘œ)"
        }
    
    def health_check(self) -> Dict[str, bool]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        status = {
            'openai_processor': False,
            'rag_system': False,
            'llm_client': False
        }
        
        # OpenAI í”„ë¡œì„¸ì„œ í™•ì¸
        try:
            stats = self.openai_processor.get_call_statistics()
            status['openai_processor'] = True
        except Exception as e:
            logger.error(f"OpenAI processor health check failed: {e}")
        
        # RAG ì‹œìŠ¤í…œ í™•ì¸
        if self.rag_system:
            try:
                # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                results = self.rag_system.search("í…ŒìŠ¤íŠ¸", k=1)
                status['rag_system'] = True
            except Exception as e:
                logger.error(f"RAG system health check failed: {e}")
        
        # LLM í´ë¼ì´ì–¸íŠ¸ í™•ì¸
        if self.llm_client:
            try:
                # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
                response = self.llm_client.generate_response("ì•ˆë…•í•˜ì„¸ìš”", max_tokens=10)
                status['llm_client'] = True
            except Exception as e:
                logger.error(f"LLM client health check failed: {e}")
        
        return status
    
    def _generate_openai_fallback_answer(
        self, 
        analysis_result, 
        rag_results: List[SearchResult], 
        question: str
    ) -> str:
        """OpenAI ì‘ë‹µ ê¸°ë°˜ í´ë°± ë‹µë³€ ìƒì„±"""
        try:
            logger.info("ğŸ”„ OpenAI í´ë°± ë‹µë³€ ìƒì„± ì‹œì‘")
            
            # OpenAI ë¶„ì„ ê²°ê³¼ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            extracted_text = analysis_result.extracted_text if analysis_result else ""
            formulas = analysis_result.formulas if analysis_result else []
            
            # ê¸°ë³¸ì ì¸ ë‹µë³€ êµ¬ì„±
            if extracted_text and len(extracted_text) > 100:
                # ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•œ ê²½ìš°
                answer_parts = [
                    "ğŸ“‹ **ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤:**\n",
                    f"**ë¬¸ì œ ë‚´ìš©:** {extracted_text[:300]}...\n" if len(extracted_text) > 300 else f"**ë¬¸ì œ ë‚´ìš©:** {extracted_text}\n"
                ]
                
                # ìˆ˜ì‹ì´ ìˆëŠ” ê²½ìš°
                if formulas:
                    answer_parts.append(f"**ìˆ˜ì‹ ë°œê²¬:** {len(formulas)}ê°œì˜ ìˆ˜í•™ í‘œí˜„ì‹ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n")
                
                # ì§ˆë¬¸ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì•ˆë‚´
                if "ë¯¸ë¶„" in question or "ë¯¸ë¶„" in extracted_text:
                    answer_parts.append("""
**ë¯¸ë¶„ ê´€ë ¨ ì„¤ëª…:**
- d/daëŠ” 'aì— ëŒ€í•œ ë¯¸ë¶„'ì„ ì˜ë¯¸í•©ë‹ˆë‹¤
- sëŠ” ë³€ìˆ˜, aëŠ” ìƒìˆ˜ë¡œ ì·¨ê¸‰ë©ë‹ˆë‹¤
- ìƒìˆ˜ë¥¼ ë¯¸ë¶„í•˜ë©´ 0, ë³€ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ë©´ 1ì´ ë©ë‹ˆë‹¤

ì¶”ê°€ì ì¸ ì„¸ë¶€ ì„¤ëª…ì´ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ì¸ ì‹ì´ë‚˜ ë¬¸ì œë¥¼ ë‹¤ì‹œ ì œì‹œí•´ ì£¼ì„¸ìš”.""")
                
                elif "ë²¡í„°" in question or "ê±°ë¦¬" in question:
                    answer_parts.append("""
**ë²¡í„° ê´€ë ¨ ì„¤ëª…:**
- ë‘ ì  ì‚¬ì´ì˜ ë²¡í„° ë°©í–¥ì€ ì‹œì ê³¼ ì¢…ì ì— ë”°ë¼ ê²°ì •ë©ë‹ˆë‹¤
- P-Qì™€ Q-PëŠ” ë°©í–¥ì´ ë°˜ëŒ€ì¸ ë²¡í„°ì…ë‹ˆë‹¤
- ë¬¼ë¦¬í•™ì—ì„œëŠ” 'ì˜í–¥ì„ ë°›ëŠ” ì 'ì´ ì¢…ì ì´ ë˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤

êµ¬ì²´ì ì¸ ë¬¸ì œ ìƒí™©ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…ì´ í•„ìš”í•˜ì‹œë©´ ë¬¸ì œë¥¼ ë‹¤ì‹œ ì œì‹œí•´ ì£¼ì„¸ìš”.""")
                
                else:
                    answer_parts.append("""
ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.
ë” ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ì„œëŠ” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.""")
                
                return "".join(answer_parts)
            
            else:
                # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•œ ê²½ìš° ì¼ë°˜ì ì¸ ë‹µë³€
                return f"""ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œ ì œì•½ìœ¼ë¡œ ì¸í•´ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.

**ì§ˆë¬¸:** {question}

**í˜„ì¬ ìƒí™©:**
- ì´ë¯¸ì§€ ë¶„ì„: {'ì™„ë£Œ' if extracted_text else 'ì œí•œì '}
- RAG ë¬¸ì„œ ê²€ìƒ‰: {len(rag_results)}ê°œ ë¬¸ì„œ ë°œê²¬
- LLM ì„œë²„: ì—°ê²° ëŒ€ê¸° ì¤‘

ë³´ë‹¤ ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê±°ë‚˜, 
ì§ˆë¬¸ì„ í…ìŠ¤íŠ¸ë¡œ ì…ë ¥í•´ ì£¼ì‹œë©´ ë” ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤."""
                
        except Exception as e:
            logger.error(f"OpenAI í´ë°± ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸: {question}"


def create_integrated_pipeline(config: Config) -> IntegratedPipeline:
    """í†µí•© íŒŒì´í”„ë¼ì¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    return IntegratedPipeline(config)