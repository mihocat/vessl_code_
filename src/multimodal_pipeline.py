#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì°¨ì„¸ëŒ€ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
Next-Generation Multimodal Processing Pipeline

ì§ˆì˜ ìš°ì„  ë¶„ì„ â†’ ì´ë¯¸ì§€ ì²˜ë¦¬ â†’ RAG + LLM í†µí•© ì‹œìŠ¤í…œ
Query-First Analysis â†’ Image Processing â†’ RAG + LLM Integration
"""

import os
import time
import asyncio
import logging
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum

# ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì„í¬íŠ¸
from query_intent_analyzer import QueryIntentAnalyzer, QueryType, ComplexityLevel, IntentAnalysisResult
from openai_vision_client import OpenAIVisionClient, VisionAnalysisResult, AnalysisType
from ncp_ocr_client import NCPOCRClient, OCRResult
from enhanced_rag_system import EnhancedRAGSystem, SearchStrategy, DomainType
from enhanced_llm_system import EnhancedLLMSystem, ModelType, ModelDomain, ModelResponse, ModelConfig

logger = logging.getLogger(__name__)


class ProcessingMode(Enum):
    """ì²˜ë¦¬ ëª¨ë“œ"""
    VISION_ONLY = "vision_only"          # ChatGPT Visionë§Œ ì‚¬ìš©
    OCR_ONLY = "ocr_only"                # NCP OCRë§Œ ì‚¬ìš©
    VISION_FIRST = "vision_first"        # ChatGPT Vision ìš°ì„ , ì‹¤íŒ¨ì‹œ OCR
    OCR_FIRST = "ocr_first"              # OCR ìš°ì„ , ì‹¤íŒ¨ì‹œ Vision
    PARALLEL = "parallel"                # ë³‘ë ¬ ì²˜ë¦¬ í›„ ê²°ê³¼ ë¹„êµ
    ADAPTIVE = "adaptive"                # ìƒí™©ì— ë”°ë¼ ìë™ ì„ íƒ


class ConfidenceLevel(Enum):
    """ì‹ ë¢°ë„ ë ˆë²¨"""
    VERY_HIGH = "very_high"    # 0.9+
    HIGH = "high"              # 0.7-0.9
    MEDIUM = "medium"          # 0.5-0.7
    LOW = "low"                # 0.3-0.5
    VERY_LOW = "very_low"      # 0.0-0.3


@dataclass
class ProcessingResult:
    """ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ê²°ê³¼"""
    success: bool
    query_analysis: Optional[IntentAnalysisResult]
    image_analysis: Optional[VisionAnalysisResult]
    ocr_fallback: Optional[OCRResult]
    rag_results: List[Dict[str, Any]]
    llm_response: Optional[ModelResponse]
    final_answer: str
    confidence_score: float
    processing_time: float
    processing_mode: ProcessingMode
    metadata: Dict[str, Any]
    error_message: Optional[str] = None


@dataclass
class PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    # ì¼ë°˜ ì„¤ì •
    processing_mode: ProcessingMode = ProcessingMode.ADAPTIVE
    use_rag: bool = True
    use_llm: bool = True
    enable_caching: bool = True
    max_processing_time: float = 60.0
    
    # ì‹ ë¢°ë„ ì„ê³„ê°’
    vision_confidence_threshold: float = 0.7
    ocr_confidence_threshold: float = 0.6
    rag_confidence_threshold: float = 0.5
    final_confidence_threshold: float = 0.6
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    enable_parallel_processing: bool = False
    parallel_timeout: float = 30.0
    
    # í’ˆì§ˆ í–¥ìƒ ì„¤ì •
    enable_result_validation: bool = True
    enable_multi_pass_refinement: bool = True
    max_refinement_iterations: int = 2


class MultimodalPipeline:
    """ì°¨ì„¸ëŒ€ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: Optional[PipelineConfig] = None):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            config: íŒŒì´í”„ë¼ì¸ ì„¤ì •
        """
        self.config = config or PipelineConfig()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.query_analyzer = None
        self.vision_client = None
        self.ocr_client = None
        self.rag_system = None
        self.llm_system = None
        
        # ì²˜ë¦¬ í†µê³„
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'vision_usage': 0,
            'ocr_usage': 0,
            'rag_usage': 0,
            'llm_usage': 0,
            'average_processing_time': 0.0,
            'average_confidence': 0.0,
            'mode_usage': {mode.value: 0 for mode in ProcessingMode}
        }
        
        # ì´ˆê¸°í™”
        self._initialize_components()
        
        logger.info("Multimodal Pipeline initialized successfully")
    
    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            # ì§ˆì˜ ë¶„ì„ê¸°
            self.query_analyzer = QueryIntentAnalyzer()
            logger.info("Query Intent Analyzer initialized")
            
            # ChatGPT Vision í´ë¼ì´ì–¸íŠ¸
            try:
                self.vision_client = OpenAIVisionClient()
                logger.info("OpenAI Vision Client initialized")
            except Exception as e:
                logger.warning(f"OpenAI Vision Client initialization failed: {e}")
            
            # NCP OCR í´ë¼ì´ì–¸íŠ¸
            try:
                self.ocr_client = NCPOCRClient()
                logger.info("NCP OCR Client initialized")
            except Exception as e:
                logger.warning(f"NCP OCR Client initialization failed: {e}")
            
            # í–¥ìƒëœ RAG ì‹œìŠ¤í…œ
            try:
                self.rag_system = EnhancedRAGSystem()
                logger.info("Enhanced RAG System initialized")
            except Exception as e:
                logger.warning(f"Enhanced RAG System initialization failed: {e}")
            
            # í–¥ìƒëœ LLM ì‹œìŠ¤í…œ
            try:
                kollama_config = ModelConfig(
                    name="LLM-Fine-Tuned",
                    model_type=ModelType.FINE_TUNED,
                    base_url="http://localhost:8000",
                    model_path="llm-fine-tuned",
                    domain_specialization=[ModelDomain.ELECTRICAL, ModelDomain.GENERAL_SCIENCE]
                )
                self.llm_system = EnhancedLLMSystem(kollama_config)
                logger.info("Enhanced LLM System initialized")
            except Exception as e:
                logger.warning(f"Enhanced LLM System initialization failed: {e}")
                
        except Exception as e:
            logger.error(f"Component initialization failed: {e}")
            raise
    
    async def process_multimodal_query(self, 
                                     query: str, 
                                     image_data: Optional[Union[str, bytes]] = None,
                                     custom_config: Optional[Dict] = None) -> ProcessingResult:
        """
        ë©€í‹°ëª¨ë‹¬ ì¿¼ë¦¬ ì²˜ë¦¬ (ë©”ì¸ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            image_data: ì´ë¯¸ì§€ ë°ì´í„° (ì„ íƒì )
            custom_config: ì‚¬ìš©ì ì •ì˜ ì„¤ì •
            
        Returns:
            ProcessingResult: ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        try:
            logger.info(f"Starting multimodal query processing: '{query[:50]}...'")
            
            # 1ë‹¨ê³„: ì§ˆì˜ ì˜ë„ ë¶„ì„ (í•µì‹¬)
            query_analysis = await self._analyze_query_intent(query)
            
            # 2ë‹¨ê³„: ì²˜ë¦¬ ëª¨ë“œ ê²°ì •
            processing_mode = self._determine_processing_mode(query_analysis, image_data, custom_config)
            self.stats['mode_usage'][processing_mode.value] += 1
            
            # 3ë‹¨ê³„: ì´ë¯¸ì§€ ì²˜ë¦¬ (ìˆëŠ” ê²½ìš°)
            image_analysis, ocr_fallback = await self._process_image(
                image_data, query_analysis, processing_mode
            )
            
            # 4ë‹¨ê³„: RAG ê²€ìƒ‰
            rag_results = await self._perform_rag_search(query, query_analysis, image_analysis)
            
            # 5ë‹¨ê³„: LLM ì‘ë‹µ ìƒì„±
            llm_response = await self._generate_llm_response(
                query, query_analysis, image_analysis, rag_results
            )
            
            # 6ë‹¨ê³„: ìµœì¢… ë‹µë³€ êµ¬ì„±
            final_answer, confidence_score = self._compose_final_answer(
                query_analysis, image_analysis, ocr_fallback, rag_results, llm_response
            )
            
            processing_time = time.time() - start_time
            
            # ê²°ê³¼ êµ¬ì„±
            result = ProcessingResult(
                success=True,
                query_analysis=query_analysis,
                image_analysis=image_analysis,
                ocr_fallback=ocr_fallback,
                rag_results=rag_results,
                llm_response=llm_response,
                final_answer=final_answer,
                confidence_score=confidence_score,
                processing_time=processing_time,
                processing_mode=processing_mode,
                metadata=self._create_metadata(processing_time, custom_config)
            )
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(result)
            
            logger.info(f"Multimodal processing completed successfully in {processing_time:.2f}s")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Multimodal processing failed: {e}")
            
            error_result = ProcessingResult(
                success=False,
                query_analysis=None,
                image_analysis=None,
                ocr_fallback=None,
                rag_results=[],
                llm_response=None,
                final_answer=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                confidence_score=0.0,
                processing_time=processing_time,
                processing_mode=ProcessingMode.ADAPTIVE,
                metadata={'error': True, 'processing_time': processing_time},
                error_message=str(e)
            )
            
            self.stats['failed_requests'] += 1
            return error_result
    
    async def _analyze_query_intent(self, query: str) -> IntentAnalysisResult:
        """ì§ˆì˜ ì˜ë„ ë¶„ì„"""
        try:
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ë˜í•‘
            loop = asyncio.get_event_loop()
            analysis = await loop.run_in_executor(
                None, self.query_analyzer.analyze_query, query
            )
            
            logger.info(f"Query analysis: type={analysis.query_type.value}, "
                       f"complexity={analysis.complexity.value}, confidence={analysis.confidence:.2f}")
            
            return analysis
            
        except Exception as e:
            logger.error(f"Query intent analysis failed: {e}")
            # ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
            return IntentAnalysisResult(
                query_type=QueryType.GENERAL_QUESTION,
                complexity=ComplexityLevel.MEDIUM,
                confidence=0.5,
                domain="general",
                suggested_processing="general",
                metadata={'error': str(e)}
            )
    
    def _determine_processing_mode(self, 
                                 query_analysis: IntentAnalysisResult, 
                                 image_data: Optional[Union[str, bytes]],
                                 custom_config: Optional[Dict]) -> ProcessingMode:
        """ì²˜ë¦¬ ëª¨ë“œ ê²°ì •"""
        # ì‚¬ìš©ì ì •ì˜ ì„¤ì • ìš°ì„ 
        if custom_config and 'processing_mode' in custom_config:
            return ProcessingMode(custom_config['processing_mode'])
        
        # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì „ìš©
        if not image_data:
            return ProcessingMode.VISION_ONLY  # RAG + LLMë§Œ ì‚¬ìš©
        
        # ì§ˆì˜ ìœ í˜•ì— ë”°ë¥¸ ìë™ ì„ íƒ
        if query_analysis.query_type in [QueryType.VISUAL_ANALYSIS, QueryType.DIAGRAM_INTERPRETATION]:
            return ProcessingMode.VISION_FIRST
        elif query_analysis.query_type == QueryType.TEXT_EXTRACTION:
            return ProcessingMode.OCR_FIRST
        elif query_analysis.query_type == QueryType.FORMULA_ANALYSIS:
            return ProcessingMode.VISION_FIRST  # ChatGPTê°€ ìˆ˜ì‹ì— ë” ê°•í•¨
        elif query_analysis.complexity == ComplexityLevel.VERY_HIGH:
            return ProcessingMode.PARALLEL  # ë³µì¡í•œ ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬
        else:
            return ProcessingMode.ADAPTIVE
    
    async def _process_image(self, 
                           image_data: Optional[Union[str, bytes]], 
                           query_analysis: IntentAnalysisResult,
                           processing_mode: ProcessingMode) -> Tuple[Optional[VisionAnalysisResult], Optional[OCRResult]]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬"""
        if not image_data:
            return None, None
        
        image_analysis = None
        ocr_fallback = None
        
        try:
            if processing_mode == ProcessingMode.VISION_ONLY:
                image_analysis = await self._analyze_with_vision(image_data, query_analysis)
                
            elif processing_mode == ProcessingMode.OCR_ONLY:
                ocr_fallback = await self._analyze_with_ocr(image_data, query_analysis)
                
            elif processing_mode == ProcessingMode.VISION_FIRST:
                image_analysis = await self._analyze_with_vision(image_data, query_analysis)
                
                # Vision ì‹¤íŒ¨í•˜ê±°ë‚˜ ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ OCR ì‹œë„
                if (not image_analysis or 
                    image_analysis.confidence_score < self.config.vision_confidence_threshold):
                    logger.info("Vision analysis failed or low confidence, trying OCR fallback")
                    ocr_fallback = await self._analyze_with_ocr(image_data, query_analysis)
                    
            elif processing_mode == ProcessingMode.OCR_FIRST:
                ocr_fallback = await self._analyze_with_ocr(image_data, query_analysis)
                
                # OCR ì‹¤íŒ¨í•˜ê±°ë‚˜ ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ Vision ì‹œë„
                if (not ocr_fallback or 
                    ocr_fallback.confidence_score < self.config.ocr_confidence_threshold):
                    logger.info("OCR analysis failed or low confidence, trying Vision")
                    image_analysis = await self._analyze_with_vision(image_data, query_analysis)
                    
            elif processing_mode == ProcessingMode.PARALLEL:
                # ë³‘ë ¬ ì²˜ë¦¬
                tasks = []
                if self.vision_client:
                    tasks.append(self._analyze_with_vision(image_data, query_analysis))
                if self.ocr_client:
                    tasks.append(self._analyze_with_ocr(image_data, query_analysis))
                
                if tasks:
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    for result in results:
                        if isinstance(result, VisionAnalysisResult):
                            image_analysis = result
                        elif isinstance(result, OCRResult):
                            ocr_fallback = result
            
            return image_analysis, ocr_fallback
            
        except Exception as e:
            logger.error(f"Image processing failed: {e}")
            return None, None
    
    async def _analyze_with_vision(self, 
                                 image_data: Union[str, bytes],
                                 query_analysis: IntentAnalysisResult) -> Optional[VisionAnalysisResult]:
        """ChatGPT Visionìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ì„"""
        if not self.vision_client:
            return None
        
        try:
            # ì§ˆì˜ ìœ í˜•ì— ë”°ë¥¸ ë¶„ì„ íƒ€ì… ê²°ì •
            analysis_type_map = {
                QueryType.VISUAL_ANALYSIS: AnalysisType.VISUAL_ANALYSIS,
                QueryType.TEXT_EXTRACTION: AnalysisType.TEXT_EXTRACTION,
                QueryType.FORMULA_ANALYSIS: AnalysisType.FORMULA_ANALYSIS,
                QueryType.PROBLEM_SOLVING: AnalysisType.PROBLEM_SOLVING,
                QueryType.DIAGRAM_INTERPRETATION: AnalysisType.DIAGRAM_INTERPRETATION
            }
            
            analysis_type = analysis_type_map.get(
                query_analysis.query_type, AnalysisType.GENERAL_ANALYSIS
            )
            
            # ë¹„ë™ê¸° ë˜í•‘
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                self.vision_client.analyze_image,
                image_data,
                analysis_type,
                None,  # custom_prompt
                asdict(query_analysis)  # query_context
            )
            
            if result.success:
                self.stats['vision_usage'] += 1
                logger.info(f"Vision analysis successful: confidence={result.confidence_score:.2f}")
            
            return result
            
        except Exception as e:
            logger.error(f"Vision analysis failed: {e}")
            return None
    
    async def _analyze_with_ocr(self, 
                              image_data: Union[str, bytes],
                              query_analysis: IntentAnalysisResult) -> Optional[OCRResult]:
        """NCP OCRë¡œ ì´ë¯¸ì§€ ë¶„ì„"""
        if not self.ocr_client:
            return None
        
        try:
            # ì§ˆì˜ ìœ í˜•ì— ë”°ë¥¸ ë¶„ì„ íƒ€ì… ê²°ì •
            analysis_type_map = {
                QueryType.TEXT_EXTRACTION: "text_extraction",
                QueryType.FORMULA_ANALYSIS: "formula_analysis",
                QueryType.PROBLEM_SOLVING: "problem_solving"
            }
            
            analysis_type = analysis_type_map.get(
                query_analysis.query_type, "text_extraction"
            )
            
            # ë¹„ë™ê¸° ë˜í•‘
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                self.ocr_client.analyze_image,
                image_data,
                analysis_type,
                None,  # custom_prompt
                asdict(query_analysis)  # query_context
            )
            
            if result.success:
                self.stats['ocr_usage'] += 1
                logger.info(f"OCR analysis successful: confidence={result.confidence_score:.2f}")
            
            return result
            
        except Exception as e:
            logger.error(f"OCR analysis failed: {e}")
            return None
    
    async def _perform_rag_search(self, 
                                query: str,
                                query_analysis: IntentAnalysisResult,
                                image_analysis: Optional[VisionAnalysisResult]) -> List[Dict[str, Any]]:
        """RAG ê²€ìƒ‰ ìˆ˜í–‰"""
        if not self.rag_system or not self.config.use_rag:
            return []
        
        try:
            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            search_queries = [query]
            
            # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©
            if image_analysis and image_analysis.success:
                if image_analysis.extracted_text:
                    search_queries.append(image_analysis.extracted_text)
                if image_analysis.analysis_result:
                    search_queries.append(image_analysis.analysis_result[:200])  # ì²˜ìŒ 200ìë§Œ
            
            # ë„ë©”ì¸ ê²°ì •
            domain_map = {
                'electrical': DomainType.ELECTRICAL,
                'mathematics': DomainType.MATHEMATICS,
                'physics': DomainType.PHYSICS,
                'chemistry': DomainType.CHEMISTRY
            }
            domain = domain_map.get(query_analysis.domain, DomainType.GENERAL)
            
            # ê²€ìƒ‰ ì „ëµ ê²°ì •
            strategy = SearchStrategy.ADAPTIVE
            if query_analysis.complexity == ComplexityLevel.VERY_HIGH:
                strategy = SearchStrategy.CHAIN_OF_THOUGHT
            elif query_analysis.query_type == QueryType.FORMULA_ANALYSIS:
                strategy = SearchStrategy.SEMANTIC
            
            # ë¹„ë™ê¸° ë˜í•‘í•˜ì—¬ RAG ê²€ìƒ‰ ìˆ˜í–‰
            loop = asyncio.get_event_loop()
            rag_results = []
            
            for search_query in search_queries:
                results = await loop.run_in_executor(
                    None,
                    self.rag_system.search,
                    search_query,
                    domain,
                    strategy
                )
                
                if results:
                    rag_results.extend(results[:3])  # ê° ì¿¼ë¦¬ë‹¹ ìµœëŒ€ 3ê°œ ê²°ê³¼
            
            # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ì •ë ¬
            unique_results = []
            seen_content = set()
            
            for result in rag_results:
                content_key = result.get('content', '')[:100]  # ì²˜ìŒ 100ìë¡œ ì¤‘ë³µ ì²´í¬
                if content_key not in seen_content:
                    unique_results.append(result)
                    seen_content.add(content_key)
            
            unique_results.sort(key=lambda x: x.get('score', 0), reverse=True)
            final_results = unique_results[:5]  # ìµœëŒ€ 5ê°œ ê²°ê³¼
            
            if final_results:
                self.stats['rag_usage'] += 1
                logger.info(f"RAG search successful: {len(final_results)} results found")
            
            return final_results
            
        except Exception as e:
            logger.error(f"RAG search failed: {e}")
            return []
    
    async def _generate_llm_response(self, 
                                   query: str,
                                   query_analysis: IntentAnalysisResult,
                                   image_analysis: Optional[VisionAnalysisResult],
                                   rag_results: List[Dict[str, Any]]) -> Optional[ModelResponse]:
        """LLM ì‘ë‹µ ìƒì„±"""
        if not self.llm_system or not self.config.use_llm:
            return None
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            
            # RAG ê²°ê³¼ ì¶”ê°€
            if rag_results:
                context_parts.append("=== ì°¸ê³  ìë£Œ ===")
                for i, result in enumerate(rag_results, 1):
                    content = result.get('content', '')
                    score = result.get('score', 0)
                    context_parts.append(f"{i}. {content} (ê´€ë ¨ë„: {score:.2f})")
                context_parts.append("")
            
            # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            if image_analysis and image_analysis.success:
                context_parts.append("=== ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ===")
                if image_analysis.extracted_text:
                    context_parts.append(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {image_analysis.extracted_text}")
                if image_analysis.analysis_result:
                    context_parts.append(f"ë¶„ì„ ê²°ê³¼: {image_analysis.analysis_result}")
                if image_analysis.formulas:
                    context_parts.append("ê°ì§€ëœ ìˆ˜ì‹:")
                    for formula in image_analysis.formulas[:3]:  # ìµœëŒ€ 3ê°œ
                        context_parts.append(f"- {formula.get('latex', '')}")
                context_parts.append("")
            
            context = "\n".join(context_parts)
            
            # ë„ë©”ì¸ ê²°ì •
            domain_map = {
                'electrical': ModelDomain.ELECTRICAL,
                'mathematics': ModelDomain.MATHEMATICS,
                'physics': ModelDomain.PHYSICS
            }
            domain = domain_map.get(query_analysis.domain, ModelDomain.ELECTRICAL)
            
            # ë¹„ë™ê¸° ë˜í•‘í•˜ì—¬ LLM ì‘ë‹µ ìƒì„±
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                self.llm_system.generate,
                query,
                context,
                domain
            )
            
            if response and not response.error_message:
                self.stats['llm_usage'] += 1
                logger.info(f"LLM response generated: confidence={response.confidence_score:.2f}")
            
            return response
            
        except Exception as e:
            logger.error(f"LLM response generation failed: {e}")
            return None
    
    def _compose_final_answer(self, 
                            query_analysis: IntentAnalysisResult,
                            image_analysis: Optional[VisionAnalysisResult],
                            ocr_fallback: Optional[OCRResult],
                            rag_results: List[Dict[str, Any]],
                            llm_response: Optional[ModelResponse]) -> Tuple[str, float]:
        """ìµœì¢… ë‹µë³€ êµ¬ì„±"""
        try:
            answer_parts = []
            confidence_scores = []
            
            # 1. LLM ì‘ë‹µì´ ìˆê³  í’ˆì§ˆì´ ì¢‹ìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            if llm_response and not llm_response.error_message and llm_response.confidence_score >= 0.7:
                answer_parts.append(llm_response.content)
                confidence_scores.append(llm_response.confidence_score)
                
                # ì¶”ê°€ ì •ë³´ ì œê³µ
                if image_analysis and image_analysis.success:
                    if image_analysis.extracted_text and len(image_analysis.extracted_text) > 20:
                        answer_parts.append(f"\n\nğŸ“· ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:\n{image_analysis.extracted_text}")
                
            # 2. LLM ì‘ë‹µì´ ì—†ê±°ë‚˜ í’ˆì§ˆì´ ë‚®ìœ¼ë©´ ë‹¤ë¥¸ ì†ŒìŠ¤ í™œìš©
            else:
                # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ìš°ì„ 
                if image_analysis and image_analysis.success:
                    if image_analysis.analysis_result:
                        answer_parts.append(image_analysis.analysis_result)
                        confidence_scores.append(image_analysis.confidence_score)
                    
                    if image_analysis.extracted_text:
                        answer_parts.append(f"\n\nğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:\n{image_analysis.extracted_text}")
                
                # OCR í´ë°± ê²°ê³¼
                elif ocr_fallback and ocr_fallback.success:
                    if ocr_fallback.extracted_text:
                        answer_parts.append(f"ğŸ“„ ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸:\n{ocr_fallback.extracted_text}")
                        confidence_scores.append(ocr_fallback.confidence_score)
                    
                    if ocr_fallback.formulas:
                        answer_parts.append("\nğŸ”¢ ê°ì§€ëœ ìˆ˜ì‹:")
                        for formula in ocr_fallback.formulas[:3]:
                            answer_parts.append(f"- {formula.get('latex', '')}")
                
                # RAG ê²°ê³¼ë§Œ ìˆëŠ” ê²½ìš°
                elif rag_results:
                    answer_parts.append("ğŸ“š ê´€ë ¨ ì •ë³´:")
                    for result in rag_results[:2]:
                        content = result.get('content', '')
                        answer_parts.append(f"â€¢ {content}")
                        confidence_scores.append(result.get('score', 0.5))
                
                # ì•„ë¬´ ê²°ê³¼ë„ ì—†ëŠ” ê²½ìš°
                else:
                    answer_parts.append("ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    confidence_scores.append(0.1)
            
            # ìµœì¢… ë‹µë³€ êµ¬ì„±
            final_answer = "\n".join(answer_parts)
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            if confidence_scores:
                final_confidence = sum(confidence_scores) / len(confidence_scores)
            else:
                final_confidence = 0.1
            
            # ë‹µë³€ í’ˆì§ˆ ê°œì„ 
            if len(final_answer) < 50:
                final_answer += "\n\nğŸ’¡ ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
            
            return final_answer, final_confidence
            
        except Exception as e:
            logger.error(f"Failed to compose final answer: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", 0.0
    
    def _create_metadata(self, processing_time: float, custom_config: Optional[Dict]) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        return {
            'pipeline_version': '1.0.0',
            'processing_time': processing_time,
            'timestamp': datetime.now().isoformat(),
            'components_used': {
                'query_analyzer': self.query_analyzer is not None,
                'vision_client': self.vision_client is not None,
                'ocr_client': self.ocr_client is not None,
                'rag_system': self.rag_system is not None,
                'llm_system': self.llm_system is not None
            },
            'custom_config': custom_config or {},
            'config': asdict(self.config)
        }
    
    def _update_stats(self, result: ProcessingResult):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        if result.success:
            self.stats['successful_requests'] += 1
        else:
            self.stats['failed_requests'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        n = self.stats['total_requests']
        prev_avg_time = self.stats['average_processing_time']
        self.stats['average_processing_time'] = (
            (prev_avg_time * (n - 1) + result.processing_time) / n
        )
        
        # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
        if result.success:
            prev_avg_conf = self.stats['average_confidence']
            success_count = self.stats['successful_requests']
            self.stats['average_confidence'] = (
                (prev_avg_conf * (success_count - 1) + result.confidence_score) / success_count
            )
    
    def get_pipeline_stats(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ í†µê³„ ë°˜í™˜"""
        success_rate = 0.0
        if self.stats['total_requests'] > 0:
            success_rate = self.stats['successful_requests'] / self.stats['total_requests']
        
        return {
            'pipeline_stats': self.stats,
            'success_rate': success_rate,
            'component_health': {
                'query_analyzer': self.query_analyzer is not None,
                'vision_client': self.vision_client is not None and self.vision_client.api_available,
                'ocr_client': self.ocr_client is not None and self.ocr_client.api_available,
                'rag_system': self.rag_system is not None,
                'llm_system': self.llm_system is not None
            }
        }
    
    # í¸ì˜ ë©”ì„œë“œë“¤
    def process_text_only(self, query: str) -> ProcessingResult:
        """í…ìŠ¤íŠ¸ ì „ìš© ì²˜ë¦¬"""
        return asyncio.run(self.process_multimodal_query(query, None))
    
    def process_with_image(self, query: str, image_path: str) -> ProcessingResult:
        """ì´ë¯¸ì§€ í¬í•¨ ì²˜ë¦¬"""
        return asyncio.run(self.process_multimodal_query(query, image_path))


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_default_pipeline() -> MultimodalPipeline:
    """ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    config = PipelineConfig(
        processing_mode=ProcessingMode.ADAPTIVE,
        use_rag=True,
        use_llm=True,
        enable_caching=True
    )
    return MultimodalPipeline(config)

async def process_query_async(query: str, 
                            image_data: Optional[Union[str, bytes]] = None,
                            config: Optional[PipelineConfig] = None) -> ProcessingResult:
    """ë¹„ë™ê¸° ì¿¼ë¦¬ ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜"""
    pipeline = MultimodalPipeline(config)
    return await pipeline.process_multimodal_query(query, image_data)

def process_query_sync(query: str, 
                      image_data: Optional[Union[str, bytes]] = None,
                      config: Optional[PipelineConfig] = None) -> ProcessingResult:
    """ë™ê¸° ì¿¼ë¦¬ ì²˜ë¦¬ í¸ì˜ í•¨ìˆ˜"""
    return asyncio.run(process_query_async(query, image_data, config))


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_pipeline():
    """íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    pipeline = create_default_pipeline()
    
    # í…ìŠ¤íŠ¸ ì „ìš© í…ŒìŠ¤íŠ¸
    result1 = await pipeline.process_multimodal_query("ì „ì••ê³¼ ì „ë¥˜ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
    print(f"Text-only test: success={result1.success}, confidence={result1.confidence_score:.2f}")
    
    # í†µê³„ ì¶œë ¥
    stats = pipeline.get_pipeline_stats()
    print(f"Pipeline stats: {stats}")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_pipeline())