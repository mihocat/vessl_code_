#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì§€ëŠ¥í˜• ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ
ì§ˆì˜ ì˜ë„ ë¶„ì„ â†’ ìµœì  ì²˜ë¦¬ ê²½ë¡œ ì„ íƒ â†’ í†µí•© ì‘ë‹µ ìƒì„±
"""

import logging
import time
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum

from query_intent_analyzer import QueryIntentAnalyzer, IntentAnalysisResult, QueryType, ProcessingMode
from optimized_openai_vision_analyzer import OptimizedOpenAIVisionAnalyzer as OpenAIVisionAnalyzer
from ncp_ocr_client import NCPOCRClient
from enhanced_multimodal_processor import EnhancedMultimodalProcessor
from rag_system import RAGSystem

logger = logging.getLogger(__name__)


class ProcessingStatus(Enum):
    """ì²˜ë¦¬ ìƒíƒœ"""
    ANALYZING_INTENT = "analyzing_intent"
    PROCESSING_IMAGE = "processing_image"
    SEARCHING_KNOWLEDGE = "searching_knowledge"
    REASONING = "reasoning"
    SYNTHESIZING = "synthesizing"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""
    success: bool
    response: str
    confidence: float
    processing_time: float
    
    # ì„¸ë¶€ ì •ë³´
    intent_analysis: Optional[IntentAnalysisResult]
    vision_result: Optional[Dict[str, Any]]
    rag_result: Optional[Dict[str, Any]]
    reasoning_steps: Optional[List[str]]
    
    # ë©”íƒ€ë°ì´í„°
    processing_path: List[str]
    fallback_used: bool
    tokens_used: int
    cost_estimate: float
    
    # í’ˆì§ˆ ë©”íŠ¸ë¦­
    response_quality_score: float
    user_satisfaction_prediction: float


class IntelligentMultimodalSystem:
    """ì§€ëŠ¥í˜• ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config):
        """
        ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        logger.info("Initializing Intelligent Multimodal System...")
        
        # ì˜ë„ ë¶„ì„ê¸°
        self.intent_analyzer = QueryIntentAnalyzer()
        
        # ì´ë¯¸ì§€ ë¶„ì„ ì‹œìŠ¤í…œ (OpenAI ìš°ì„ , NCP OCR fallback)
        self.openai_analyzer = OpenAIVisionAnalyzer(config)
        self.ncp_ocr = NCPOCRClient(config)
        self.multimodal_processor = EnhancedMultimodalProcessor(config)
        
        # RAG ì‹œìŠ¤í…œ
        self.rag_system = RAGSystem(config)
        
        # ì²˜ë¦¬ í†µê³„
        self.processing_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'openai_usage': 0,
            'ncp_usage': 0,
            'average_processing_time': 0.0,
            'average_confidence': 0.0
        }
        
        logger.info("Intelligent Multimodal System initialized successfully")
    
    def process_query(self, query: str, image_path: str = None, user_context: Dict = None) -> ProcessingResult:
        """
        í†µí•© ì§ˆì˜ ì²˜ë¦¬
        
        Args:
            query: ì‚¬ìš©ì ì§ˆì˜
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ (ì„ íƒì )
            user_context: ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = time.time()
        processing_path = []
        
        try:
            self.processing_stats['total_requests'] += 1
            logger.info(f"Processing query: '{query[:50]}...' (image: {bool(image_path)})")
            
            # 1ë‹¨ê³„: ì˜ë„ ë¶„ì„
            processing_path.append("intent_analysis")
            intent_result = self.intent_analyzer.analyze_intent(query, has_image=bool(image_path))
            
            logger.info(f"Intent analysis: {intent_result.query_type.value} "
                       f"(complexity: {intent_result.complexity.value}, "
                       f"mode: {intent_result.processing_mode.value})")
            
            # 2ë‹¨ê³„: ì²˜ë¦¬ ê²½ë¡œ ì„ íƒ ë° ì‹¤í–‰
            if intent_result.processing_mode == ProcessingMode.VISION_FIRST:
                result = self._process_vision_first(query, image_path, intent_result, processing_path)
            elif intent_result.processing_mode == ProcessingMode.RAG_FIRST:
                result = self._process_rag_first(query, image_path, intent_result, processing_path)
            elif intent_result.processing_mode == ProcessingMode.HYBRID:
                result = self._process_hybrid(query, image_path, intent_result, processing_path)
            elif intent_result.processing_mode == ProcessingMode.REASONING_CHAIN:
                result = self._process_reasoning_chain(query, image_path, intent_result, processing_path)
            else:
                result = self._process_direct_response(query, image_path, intent_result, processing_path)
            
            # 3ë‹¨ê³„: ê²°ê³¼ í›„ì²˜ë¦¬ ë° í’ˆì§ˆ í‰ê°€
            result = self._post_process_result(result, intent_result, processing_path)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = time.time() - start_time
            self._update_stats(result, processing_time)
            
            logger.info(f"Query processing completed in {processing_time:.2f}s "
                       f"(confidence: {result.confidence:.2f})")
            
            return result
            
        except Exception as e:
            logger.error(f"Query processing failed: {e}")
            
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ìƒì„±
            fallback_result = self._generate_fallback_response(
                query, image_path, str(e), processing_path, time.time() - start_time
            )
            
            return fallback_result
    
    def _process_vision_first(self, query: str, image_path: str, intent: IntentAnalysisResult, 
                             processing_path: List[str]) -> ProcessingResult:
        """ë¹„ì „ ìš°ì„  ì²˜ë¦¬"""
        processing_path.append("vision_analysis")
        
        if not image_path:
            raise ValueError("Vision-first processing requires an image")
        
        # OpenAI Vision API ì‹œë„
        vision_result = None
        fallback_used = False
        
        if self.openai_analyzer.api_available:
            try:
                logger.info("Using OpenAI Vision API for image analysis")
                vision_result = self.openai_analyzer.analyze_image(
                    image_path, 
                    question=query,
                    extract_text=intent.query_type in [QueryType.TEXT_EXTRACTION, QueryType.VISUAL_ANALYSIS],
                    detect_formulas=intent.query_type == QueryType.FORMULA_ANALYSIS
                )
                
                if vision_result['success']:
                    self.processing_stats['openai_usage'] += 1
                    logger.info("OpenAI Vision API analysis successful")
                else:
                    raise Exception(f"OpenAI Vision API failed: {vision_result.get('error')}")
                    
            except Exception as e:
                logger.warning(f"OpenAI Vision API failed: {e}, trying NCP OCR fallback")
                vision_result = None
        
        # NCP OCR fallback
        if not vision_result or not vision_result['success']:
            if self.ncp_ocr.api_available:
                try:
                    logger.info("Using NCP OCR as fallback")
                    vision_result = self.ncp_ocr.analyze_image(image_path, question=query)
                    fallback_used = True
                    self.processing_stats['ncp_usage'] += 1
                    logger.info("NCP OCR analysis successful")
                except Exception as e:
                    logger.error(f"NCP OCR also failed: {e}")
                    raise Exception("Both OpenAI Vision API and NCP OCR failed")
            else:
                raise Exception("No image analysis service available")
        
        # RAG ë³´ì™„ ê²€ìƒ‰ (í•„ìš”í•œ ê²½ìš°)
        rag_result = None
        if intent.requires_rag or intent.query_type in [QueryType.PROBLEM_SOLVING, QueryType.EXPLANATION]:
            processing_path.append("rag_search")
            try:
                # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ RAG ê²€ìƒ‰
                search_query = vision_result.get('extracted_text', query)
                rag_result = self.rag_system.search(search_query, max_results=5)
                logger.info(f"RAG search completed: {len(rag_result.get('results', []))} results")
            except Exception as e:
                logger.warning(f"RAG search failed: {e}")
        
        # ì‘ë‹µ ìƒì„±
        response = self._synthesize_vision_response(query, vision_result, rag_result, intent)
        
        return ProcessingResult(
            success=True,
            response=response,
            confidence=vision_result.get('confidence', 0.8),
            processing_time=0.0,  # ë‚˜ì¤‘ì— ì„¤ì •
            intent_analysis=intent,
            vision_result=vision_result,
            rag_result=rag_result,
            reasoning_steps=None,
            processing_path=processing_path.copy(),
            fallback_used=fallback_used,
            tokens_used=vision_result.get('usage', {}).get('total_tokens', 0),
            cost_estimate=self._estimate_cost(vision_result, rag_result),
            response_quality_score=0.8,  # ë‚˜ì¤‘ì— ê³„ì‚°
            user_satisfaction_prediction=0.8
        )
    
    def _process_rag_first(self, query: str, image_path: str, intent: IntentAnalysisResult,
                          processing_path: List[str]) -> ProcessingResult:
        """RAG ìš°ì„  ì²˜ë¦¬"""
        processing_path.append("rag_search")
        
        # RAG ê²€ìƒ‰
        rag_result = self.rag_system.search(query, max_results=10)
        logger.info(f"RAG search completed: {len(rag_result.get('results', []))} results")
        
        # ì´ë¯¸ì§€ ë¶„ì„ (ë³´ì™„ì )
        vision_result = None
        if image_path and intent.requires_image:
            processing_path.append("vision_analysis")
            try:
                vision_result = self.openai_analyzer.analyze_image(
                    image_path, question=query, extract_text=True
                )
                logger.info("Supplementary image analysis completed")
            except Exception as e:
                logger.warning(f"Supplementary image analysis failed: {e}")
        
        # ì‘ë‹µ ìƒì„±
        response = self._synthesize_rag_response(query, rag_result, vision_result, intent)
        
        return ProcessingResult(
            success=True,
            response=response,
            confidence=rag_result.get('confidence', 0.7),
            processing_time=0.0,
            intent_analysis=intent,
            vision_result=vision_result,
            rag_result=rag_result,
            reasoning_steps=None,
            processing_path=processing_path.copy(),
            fallback_used=False,
            tokens_used=(vision_result.get('usage', {}).get('total_tokens', 0) if vision_result else 0),
            cost_estimate=self._estimate_cost(vision_result, rag_result),
            response_quality_score=0.7,
            user_satisfaction_prediction=0.75
        )
    
    def _process_hybrid(self, query: str, image_path: str, intent: IntentAnalysisResult,
                       processing_path: List[str]) -> ProcessingResult:
        """í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ (ë³‘ë ¬)"""
        processing_path.extend(["vision_analysis", "rag_search"])
        
        vision_result = None
        rag_result = None
        fallback_used = False
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” asyncio ì‚¬ìš© ê°€ëŠ¥)
        if image_path:
            try:
                vision_result = self.openai_analyzer.analyze_image(
                    image_path, question=query, extract_text=True, detect_formulas=True
                )
                if not vision_result['success']:
                    vision_result = self.ncp_ocr.analyze_image(image_path, question=query)
                    fallback_used = True
            except Exception as e:
                logger.warning(f"Image analysis failed: {e}")
        
        try:
            # ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í•¨ê»˜ ê²€ìƒ‰
            search_query = query
            if vision_result and vision_result.get('extracted_text'):
                search_query = f"{query} {vision_result['extracted_text'][:200]}"
            
            rag_result = self.rag_system.search(search_query, max_results=8)
        except Exception as e:
            logger.warning(f"RAG search failed: {e}")
        
        # í†µí•© ì‘ë‹µ ìƒì„±
        response = self._synthesize_hybrid_response(query, vision_result, rag_result, intent)
        
        return ProcessingResult(
            success=True,
            response=response,
            confidence=max(
                vision_result.get('confidence', 0.5) if vision_result else 0.5,
                rag_result.get('confidence', 0.5) if rag_result else 0.5
            ),
            processing_time=0.0,
            intent_analysis=intent,
            vision_result=vision_result,
            rag_result=rag_result,
            reasoning_steps=None,
            processing_path=processing_path.copy(),
            fallback_used=fallback_used,
            tokens_used=(vision_result.get('usage', {}).get('total_tokens', 0) if vision_result else 0),
            cost_estimate=self._estimate_cost(vision_result, rag_result),
            response_quality_score=0.8,
            user_satisfaction_prediction=0.8
        )
    
    def _process_reasoning_chain(self, query: str, image_path: str, intent: IntentAnalysisResult,
                                processing_path: List[str]) -> ProcessingResult:
        """ì¶”ë¡  ì²´ì¸ ì²˜ë¦¬"""
        processing_path.append("reasoning_chain")
        
        reasoning_steps = []
        
        # 1ë‹¨ê³„: ë¬¸ì œ ë¶„ì„
        reasoning_steps.append("1. ë¬¸ì œ ë¶„ì„ ë° ë¶„í•´")
        
        # 2ë‹¨ê³„: ì •ë³´ ìˆ˜ì§‘
        vision_result = None
        rag_result = None
        
        if image_path:
            reasoning_steps.append("2. ì´ë¯¸ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ")
            vision_result = self.openai_analyzer.analyze_image(
                image_path, question=query, extract_text=True, detect_formulas=True
            )
        
        reasoning_steps.append("3. ê´€ë ¨ ì§€ì‹ ê²€ìƒ‰")
        search_query = query
        if vision_result and vision_result.get('extracted_text'):
            search_query = f"{query} {vision_result['extracted_text']}"
        
        rag_result = self.rag_system.search(search_query, max_results=5)
        
        # 3ë‹¨ê³„: ë‹¨ê³„ë³„ í•´ê²°
        reasoning_steps.append("4. ë‹¨ê³„ë³„ ë¬¸ì œ í•´ê²°")
        
        # ì‘ë‹µ ìƒì„± (ì¶”ë¡  ê³¼ì • í¬í•¨)
        response = self._synthesize_reasoning_response(
            query, vision_result, rag_result, intent, reasoning_steps
        )
        
        return ProcessingResult(
            success=True,
            response=response,
            confidence=0.85,  # ì¶”ë¡  ì²´ì¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ì‹ ë¢°ë„
            processing_time=0.0,
            intent_analysis=intent,
            vision_result=vision_result,
            rag_result=rag_result,
            reasoning_steps=reasoning_steps,
            processing_path=processing_path.copy(),
            fallback_used=False,
            tokens_used=(vision_result.get('usage', {}).get('total_tokens', 0) if vision_result else 0),
            cost_estimate=self._estimate_cost(vision_result, rag_result),
            response_quality_score=0.85,
            user_satisfaction_prediction=0.9
        )
    
    def _process_direct_response(self, query: str, image_path: str, intent: IntentAnalysisResult,
                               processing_path: List[str]) -> ProcessingResult:
        """ì§ì ‘ ì‘ë‹µ ì²˜ë¦¬"""
        processing_path.append("direct_response")
        
        # ê°„ë‹¨í•œ ì§ˆì˜ëŠ” ì§ì ‘ ì²˜ë¦¬
        response = f"ì§ˆë¬¸: {query}\n\n"
        
        if intent.query_type == QueryType.GENERAL_CHAT:
            response += "ì•ˆë…•í•˜ì„¸ìš”! ì´ë¯¸ì§€ ë¶„ì„, ìˆ˜ì‹ í•´ì„, ê¸°ìˆ ì  ì§ˆë¬¸ ë“± ë‹¤ì–‘í•œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ì´ë¯¸ì§€ë¥¼ ì œê³µí•´ ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:
            response += "ì£„ì†¡í•©ë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì •ë³´ë‚˜ ì´ë¯¸ì§€ê°€ í•„ìš”í•œ ì§ˆë¬¸ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì„ ì œê³µí•´ ì£¼ì‹œê±°ë‚˜ ê´€ë ¨ ì´ë¯¸ì§€ë¥¼ ì²¨ë¶€í•´ ì£¼ì‹œë©´ ë” ë‚˜ì€ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        return ProcessingResult(
            success=True,
            response=response,
            confidence=0.6,
            processing_time=0.0,
            intent_analysis=intent,
            vision_result=None,
            rag_result=None,
            reasoning_steps=None,
            processing_path=processing_path.copy(),
            fallback_used=False,
            tokens_used=0,
            cost_estimate=0.0,
            response_quality_score=0.6,
            user_satisfaction_prediction=0.5
        )
    
    def _synthesize_vision_response(self, query: str, vision_result: Dict, rag_result: Dict,
                                  intent: IntentAnalysisResult) -> str:
        """ë¹„ì „ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì‘ë‹µ í•©ì„±"""
        if not vision_result or not vision_result.get('success'):
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
        
        response_parts = []
        
        # ì›ë³¸ ì‘ë‹µ
        if vision_result.get('raw_response'):
            response_parts.append(vision_result['raw_response'])
        
        # RAG ë³´ì™„ ì •ë³´
        if rag_result and rag_result.get('results'):
            response_parts.append("\n\nğŸ“š ê´€ë ¨ ì •ë³´:")
            for i, result in enumerate(rag_result['results'][:3], 1):
                response_parts.append(f"{i}. {result.get('content', '')[:200]}...")
        
        # ì‹ ë¢°ë„ ì •ë³´
        confidence = vision_result.get('confidence', 0.8)
        if confidence < 0.7:
            response_parts.append(f"\n\nâš ï¸ ë¶„ì„ ì‹ ë¢°ë„: {confidence:.1%} - ê²°ê³¼ë¥¼ ê²€í† í•´ ì£¼ì„¸ìš”.")
        
        return '\n'.join(response_parts)
    
    def _synthesize_rag_response(self, query: str, rag_result: Dict, vision_result: Dict,
                               intent: IntentAnalysisResult) -> str:
        """RAG ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ì‘ë‹µ í•©ì„±"""
        if not rag_result or not rag_result.get('results'):
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        response_parts = [f"ì§ˆë¬¸: {query}\n"]
        
        # ì£¼ìš” ë‹µë³€
        best_result = rag_result['results'][0]
        response_parts.append(f"ğŸ’¡ ë‹µë³€:\n{best_result.get('content', '')}")
        
        # ì¶”ê°€ ì •ë³´
        if len(rag_result['results']) > 1:
            response_parts.append("\nğŸ“š ì¶”ê°€ ê´€ë ¨ ì •ë³´:")
            for i, result in enumerate(rag_result['results'][1:4], 1):
                response_parts.append(f"{i}. {result.get('content', '')[:150]}...")
        
        # ì´ë¯¸ì§€ ë³´ì™„ ì •ë³´
        if vision_result and vision_result.get('extracted_text'):
            response_parts.append(f"\nğŸ” ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:\n{vision_result['extracted_text'][:300]}...")
        
        return '\n'.join(response_parts)
    
    def _synthesize_hybrid_response(self, query: str, vision_result: Dict, rag_result: Dict,
                                  intent: IntentAnalysisResult) -> str:
        """í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µ í•©ì„±"""
        response_parts = [f"ğŸ“‹ ì¢…í•© ë¶„ì„ ê²°ê³¼\nì§ˆë¬¸: {query}\n"]
        
        # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼
        if vision_result and vision_result.get('success'):
            response_parts.append("ğŸ” ì´ë¯¸ì§€ ë¶„ì„:")
            response_parts.append(vision_result.get('raw_response', 'ë¶„ì„ ê²°ê³¼ ì—†ìŒ'))
        
        # ì§€ì‹ ê²€ìƒ‰ ê²°ê³¼
        if rag_result and rag_result.get('results'):
            response_parts.append("\nğŸ“š ê´€ë ¨ ì§€ì‹:")
            for i, result in enumerate(rag_result['results'][:2], 1):
                response_parts.append(f"{i}. {result.get('content', '')[:200]}...")
        
        # ì¢…í•© ê²°ë¡ 
        response_parts.append("\nâœ… ê²°ë¡ :")
        response_parts.append("ìœ„ ì´ë¯¸ì§€ ë¶„ì„ê³¼ ê´€ë ¨ ì§€ì‹ì„ ì¢…í•©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤.")
        
        return '\n'.join(response_parts)
    
    def _synthesize_reasoning_response(self, query: str, vision_result: Dict, rag_result: Dict,
                                     intent: IntentAnalysisResult, reasoning_steps: List[str]) -> str:
        """ì¶”ë¡  ì²´ì¸ ì‘ë‹µ í•©ì„±"""
        response_parts = [f"ğŸ§  ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •\nì§ˆë¬¸: {query}\n"]
        
        # ì¶”ë¡  ë‹¨ê³„
        response_parts.append("ğŸ“ í•´ê²° ê³¼ì •:")
        for step in reasoning_steps:
            response_parts.append(step)
        
        # ìˆ˜ì§‘ëœ ì •ë³´
        if vision_result and vision_result.get('success'):
            response_parts.append(f"\nğŸ” ì´ë¯¸ì§€ì—ì„œ í™•ì¸ëœ ì •ë³´:\n{vision_result.get('extracted_text', 'ì—†ìŒ')}")
        
        if rag_result and rag_result.get('results'):
            response_parts.append(f"\nğŸ“š ê´€ë ¨ ì§€ì‹:\n{rag_result['results'][0].get('content', '')[:300]}...")
        
        # ìµœì¢… ë‹µë³€
        response_parts.append("\nğŸ¯ ìµœì¢… ë‹µë³€:")
        response_parts.append("ìœ„ì˜ ë‹¨ê³„ë³„ ë¶„ì„ì„ í†µí•´ ë¬¸ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤.")
        
        return '\n'.join(response_parts)
    
    def _post_process_result(self, result: ProcessingResult, intent: IntentAnalysisResult,
                           processing_path: List[str]) -> ProcessingResult:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_score = self._calculate_quality_score(result)
        result.response_quality_score = quality_score
        
        # ì‚¬ìš©ì ë§Œì¡±ë„ ì˜ˆì¸¡
        satisfaction = self._predict_user_satisfaction(result, intent)
        result.user_satisfaction_prediction = satisfaction
        
        return result
    
    def _calculate_quality_score(self, result: ProcessingResult) -> float:
        """ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ì„±ê³µì ì¸ ì²˜ë¦¬
        if result.success:
            score += 0.2
        
        # ì‹ ë¢°ë„
        score += result.confidence * 0.2
        
        # ì‘ë‹µ ê¸¸ì´ (ì ì ˆí•œ ê¸¸ì´)
        response_length = len(result.response)
        if 100 <= response_length <= 2000:
            score += 0.1
        
        return min(1.0, score)
    
    def _predict_user_satisfaction(self, result: ProcessingResult, intent: IntentAnalysisResult) -> float:
        """ì‚¬ìš©ì ë§Œì¡±ë„ ì˜ˆì¸¡"""
        satisfaction = result.confidence * 0.7
        
        # ì˜ë„ì™€ ì²˜ë¦¬ ë°©ì‹ ì¼ì¹˜ë„
        if intent.processing_mode.value in result.processing_path:
            satisfaction += 0.2
        
        # ë‹¤ì¤‘ ì •ë³´ì› ì‚¬ìš©
        if result.vision_result and result.rag_result:
            satisfaction += 0.1
        
        return min(1.0, satisfaction)
    
    def _estimate_cost(self, vision_result: Dict, rag_result: Dict) -> float:
        """ë¹„ìš© ì¶”ì •"""
        cost = 0.0
        
        if vision_result and vision_result.get('usage'):
            # gpt-4o-mini ê¸°ì¤€
            tokens = vision_result['usage'].get('total_tokens', 0)
            cost += tokens * 0.0006 / 1000  # ì¶”ì •ì¹˜
        
        return cost
    
    def _generate_fallback_response(self, query: str, image_path: str, error: str,
                                  processing_path: List[str], processing_time: float) -> ProcessingResult:
        """ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ì‘ë‹µ ìƒì„±"""
        fallback_response = f"""ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ì§ˆë¬¸: {query}
ì˜¤ë¥˜: {error}

ë‹¤ìŒì„ ì‹œë„í•´ ë³´ì„¸ìš”:
1. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”
2. ì´ë¯¸ì§€ê°€ ìˆë‹¤ë©´ ì„ ëª…í•œ ì´ë¯¸ì§€ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”
3. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”

ê¸°ìˆ  ì§€ì›ì´ í•„ìš”í•˜ì‹œë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”."""
        
        return ProcessingResult(
            success=False,
            response=fallback_response,
            confidence=0.3,
            processing_time=processing_time,
            intent_analysis=None,
            vision_result=None,
            rag_result=None,
            reasoning_steps=None,
            processing_path=processing_path,
            fallback_used=True,
            tokens_used=0,
            cost_estimate=0.0,
            response_quality_score=0.3,
            user_satisfaction_prediction=0.2
        )
    
    def _update_stats(self, result: ProcessingResult, processing_time: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        result.processing_time = processing_time
        
        if result.success:
            self.processing_stats['successful_requests'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total = self.processing_stats['total_requests']
        current_avg = self.processing_stats['average_processing_time']
        self.processing_stats['average_processing_time'] = (
            (current_avg * (total - 1) + processing_time) / total
        )
        
        # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
        current_conf = self.processing_stats['average_confidence']
        self.processing_stats['average_confidence'] = (
            (current_conf * (total - 1) + result.confidence) / total
        )
    
    def get_system_stats(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„ ë°˜í™˜"""
        return self.processing_stats.copy()
    
    def health_check(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        return {
            'status': 'healthy',
            'components': {
                'intent_analyzer': True,
                'openai_analyzer': self.openai_analyzer.api_available,
                'ncp_ocr': self.ncp_ocr.api_available,
                'rag_system': True  # ì‹¤ì œë¡œëŠ” RAG ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
            },
            'stats': self.get_system_stats()
        }