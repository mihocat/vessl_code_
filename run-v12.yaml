name: korean-electrical-rag-github
description: Korean Electrical Engineering RAG System - GitHub based deployment
tags:
  - rag
  - korean
  - electrical-engineering
  - github-deploy
import:
  /code/: git://github.com/mihocat/vessl_code_.git
  /dataset/: volume://vessl-storage/DS-EXPLAIN-30000
  /model/: vessl-model://miho/llama-3.2-korean-bllossom-electrical-expert/1
resources:
  cluster: vessl-oci-sanjose
  preset: gpu-a10-small
image: quay.io/vessl-ai/torch:2.3.1-cuda12.1-r5
run:
  - name: install-dependencies
    command: |
      echo "=== 시스템 의존성 설치 ==="
      apt update && apt install -y libgl1-mesa-glx
      echo "=== Python 의존성 설치 ==="
      cd /code
      ls -la
      pip install --upgrade pip
      # vLLM 및 가속 관련 패키지
      pip install vllm==0.6.2 flash-attn==2.6.3 "bitsandbytes>=0.44.0" --no-cache-dir
      # 애플리케이션 의존성
      pip install -r requirements.txt --no-cache-dir
  
  - name: start-vllm-server
    command: |
      echo "=== vLLM KoLlama 서버 시작 ==="
      cd /code
      python -m vllm.entrypoints.openai.api_server \
        --model "MLP-KTLim/llama-3-Korean-Bllossom-8B" \
        --enable-lora \
        --lora-modules kollama-electrical=/model \
        --served-model-name kollama-electrical \
        --max-num-seqs 16 \
        --max-model-len 4096 \
        --quantization bitsandbytes \
        --load-format bitsandbytes \
        --enforce-eager \
        --port 8000 \
        --host 0.0.0.0 \
        --gpu-memory-utilization 0.8 > /tmp/vllm.log 2>&1 &
      
      echo "vLLM 서버 시작 대기 (60초)..."
      sleep 60
      
      # 서버 상태 확인
      curl -X GET http://localhost:8000/health || {
        echo "vLLM 서버 시작 실패. 로그 확인:"
        cat /tmp/vllm.log
        exit 1
      }
  
  - name: run-application
    command: |
      echo "=== RAG 애플리케이션 실행 ==="
      cd /code
      # 환경 변수 설정
      export PYTHONPATH="/code:$PYTHONPATH"
      export DATASET_PATH="/dataset"
      
      echo "애플리케이션 시작..."
      python src/app.py
ports:
  - name: gradio
    type: http
    port: 7860
  - name: vllm
    type: http
    port: 8000
env:
  CUDA_VISIBLE_DEVICES: "0"
  HF_HOME: /tmp/huggingface
  PYTHONUNBUFFERED: "1"
  TRANSFORMERS_OFFLINE: "0"